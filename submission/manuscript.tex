\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1.0in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Best practices for applying machine learning to bacterial 16S rRNA gene sequencing data},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{\textbf{Best practices for applying machine learning to bacterial 16S
rRNA gene sequencing data}}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{caption}

\usepackage{helvet} % Helvetica font
\renewcommand*\familydefault{\sfdefault} % Use the sans serif version of the font
\usepackage[T1]{fontenc}
\usepackage[labelfont=bf]{caption}

\usepackage[none]{hyphenat}

\usepackage{setspace}
\doublespacing
\setlength{\parskip}{1em}

\usepackage{lineno}

\usepackage{pdfpages}
\floatplacement{figure}{H} % Keep the figure up top of the page

\begin{document}
\maketitle

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}

Begüm D. Topçuoğlu\({^1}\), Nicholas A. Lesniak\({^1}\), Jenna
Wiens\({^2}\), Mack Ruffin\({^3}\), Patrick D.
Schloss\textsuperscript{1\(\dagger\)}

\vspace{40mm}

\(\dagger\) To whom correspondence should be addressed:
\href{mailto:pschloss@umich.edu}{\nolinkurl{pschloss@umich.edu}}

1. Department of Microbiology and Immunology, University of Michigan,
Ann Arbor, MI 48109

2. Department of Computer Science and Engineering, University or
Michigan, Ann Arbor, MI 49109

3. Department of Family Medicine and Community Medicine, Penn State
Hershey Medical Center, Hershey, PA

\newpage

\linenumbers

\subsection{Abstract}\label{abstract}

Machine learning (ML) modeling of the human microbiome has the potential
to identify the microbial biomarkers and aid in diagnosis of many
chronic diseases such as inflammatory bowel disease, diabetes and
colorectal cancer. Progress has been made towards developing ML models
that predict health outcomes from bacterial abundances, but rigourous ML
models are scarce due to the flawed methods that call the validity of
developed models into question. Furthermore, the use of black box ML
models has hindered the validation of microbial biomarkers. To overcome
these challenges, we benchmarked seven different ML models that use
fecal 16S rRNA sequences to predict colorectal cancer (CRC) lesions
(n=490 patients, 261 controls and 229 cases). To show the effect of
model selection, we assessed the predictive performance,
interpretability, and computational efficiency of the following models:
L2-regularized logistic regression, L1 and L2-regularized support vector
machines (SVM) with linear and radial basis function kernels, a decision
tree, random forest, and extreme gradient boosting (XGBoost). The random
forest model was best at detecting CRC lesions with an AUROC of 0.695
but it was slow to train (83.2 h) and hard to interpret. Despite its
simplicity, L2-regularized logistic regression followed random forest in
predictive performance with an AUROC of 0.680, and it trained much
faster (12 min). In this study, we established standards for the
development of modeling pipelines for microbiome-associated ML models.
Additionally, we showed that ML models should be chosen based on
expectations of predictive performance, interpretability and available
computational resources.

\newpage

\subsection{Importance (needs work)}\label{importance-needs-work}

Prediction of health outcomes using ML is rapidly being adopted by human
microbiome studies. However, the developed ML models so far are
overoptimistic in terms of validity and predictive performance. Without
rigorous ML pipelines, we cannot trust ML models. Before we can speed up
progress, we need to slow down, define and implement good ML practices.
\newpage

\subsection{Background}\label{background}

As the number of people represented in human microbiome datasets grow,
there is an increasing desire to use microbiome data to diagnose
disease. However, the structure of the human microbiome is remarkably
variable between individuals to the point where it is often difficult to
identify the bacterial populations that are associated with diseases
using traditional statistical models. This variation is likely due to
the ability of many bacterial populations to fill the same niche such
that different populations cause the same disease in different
individuals. Furthermore, a growing number of studies have shown that it
is rare for a single bacterial species to be associated with a disease.
Instead, subsets of the microbiome account for differences in health.
Traditional statistical approaches do not adequately account for the
variation in the human microbiome and typically consider the protective
or risk effects of each bacterial population individually. Recently,
machine learning models have grown in popularity among microbiome
researchers because of the large amount of data that can now be
generated and because the models are effective at accounting for the
interpersonal microbiome variation and the ecology of the disease.

ML models are useful for understanding the variation in the structure of
existing data and to apply that knowledge to make predictions about new
data. Researchers have used ML models to diagnose and understand the
ecological basis of diseases such as liver cirrhosis, colorectal cancer,
inflammatory bowel diseases (IBD), obesity, and type 2 diabetes (1--11).
The task of diagnosing an individual with high confidence relies on a ML
model that is built with rigorous methods. However, ML methodology in
the microbiome field has flaws that have not been addressed previously.
There is a lack of transparency in which methods are used and how these
methods are implemented (12, 13); models are being developed and
evaluated without a seperate held-out test data (3, 4, 14--16); there is
large variation between the cross-validation outcomes (16, 17) and
between cross-validation and testing performances (18) . These practices
limit reproducibility, cause overoptimism for model performance and
prevent the development of generalizable models, where the model makes
accurate predictions with newly acquired data as well as it does with
the training data. Nevertheless, the microbiome field is making progress
to avoid potential pitfalls of ML. More and more studies are now
validating their models on independent datasets (7, 18, 19) and they are
introducing frameworks to accurately use ML tools (20--23).

Among microbiome researchers, there has been a trend towards using more
complex ML models such as random forest and neural networks (2, 11,
24--26) over simpler models such as logistic regression or other linear
models (18, 22, 27). There is an implicit assumption that the more
complex models are better because they are more complex. Although these
models may be better at incorporating non-linear relationships or yield
better predictions, they are also called black box ML models because
they are not inherently interpretable. These models require post hoc
explanations to quantify the importance of each feature in making a
prediction. Depending on the application of the model researchers may
chose to use different modeling approaches. For example, a researcher
trying to identify the populations causing a disease would likely want a
more interpretable model whereas a clinician may emphasize performance.
Although one may feel that they are sacrificing interpretability for
performance, that tradeoff may be minimal (28, 29).

The lack of transparency in model selection and using flawed modelling
and interpretation methods negatively impacts model validity. To
showcase a rigorous ML pipeline and to shed light on how ML model
selection can affect modeling results, we performed an empirical
analysis comparing 7 modeling approaches using the same dataset and
pipeline. We established modeling pipelines for three linear models with
different forms of regularization: L2-regularized logistic regression
and L1 and L2-regularized support vector machines (SVM) with a linear
kernel. We also developed four non-linear models: SVM with radial basis
function kernel, a decision tree, random forest and XGBoost. We compared
the predictive performance, interpretability and computational
efficiency of the seven models to highlight the importance of model
selection. To demonstrate the performance of these modelling approaches
and our pipeline, we used data from a previously published study that
sought to classifiy individuals as having normal colons or colonic
lesions based on the 16S rRNA gene sequences collected from fecal
samples (3). This dataset was selected because it is a relatively large
collection of samples (N=490) connected to clinically significant
disease where there is ample evidence that it is driven by variation in
the microbiome (1, 3, 4, 30). With this dataset, we established
standards for ML pipeline construction, evaluated predictive
performance, and demonstrated model interpretation for models. This
framework can be easily applied to other host-associated and
environmental microbiome problems.

\subsection{Results}\label{results}

\textbf{Model selection and pipeline construction} We established a ML
pipeline where we train and validate each of the seven models {[}Figure
1{]}. We randomly split the data into training and test sets so that the
training set consisted of 80\% of the full dataset while the test set
was composed of the remaining data {[}Figure 1{]}. Since the cases are
not uniformly represented in the data, the data-split was stratified to
maintain the overall label distribution in both the training and test
sets. For example, our example dataset represented data from 490
individuals, 261 had normal colons and 229 had a screen relevant
neoplasia (SRN). After the data-split, the training set consisted of 393
patients (209 SRN), while the test set was composed of 97 patients (52
SRN). The training data was used for hyperparameter selection and the
test set was used for evaluating predictive performance.

We used ML models with different classification algorithms and
regularization methods. Regularization is a technique to discourage
overfitting by penalizing the model for learning the training data too
well. For regularized logistic regression and SVM with linear kernel, we
used L2 regularization to keep all potentially important features. For
comparison, we also trained an L1 regularized SVM model with linear
kernel. L1-regularization on microbiome data lead to a sparser solution
(i.e., force many coefficients to zero). Finally, to explore the
potential for non-linear relationships among features and the outcome of
interest, we trained tree based models, decision tree, random forest and
XGBoost, as well as an SVM with non-linear kernel.

Hyperparameters are the rules that are learned from the training data in
a classification algorithm. For example regularization term (C) that
decides how big the penalty for overfitting will be, is an
hyperparameter. The model becomes more generalizable when we tune for
the optimal C value. Similar to regularization term C, other
hyperparameters can be tuned over a full grid search and selected by
validation to build better models. We selected hyperparameter settings
by performing repeated five-fold cross-validation (CV) on the training
set {[}Figure 1{]}. Similar to the initial data-split, five-fold CV was
also stratified to maintain the overall label distribution. We chose the
best hyperparameter setting for each model based on its CV predictive
performance using the area under the receiver operating characteristic
curve (AUROC) metric {[}Figure S1 and S2{]}. The AUROC ranges from 1.0,
where the model perfectly distinguishes between cases and controls, to
0.50, where the model's predictions are no different from random chance.
The cross-validation of each hyperparameter setting was repeated over
100 randomizations to get a robust reading of predictive performance.

We then trained the full training dataset with the selected
hyperparameters. We used the held-out test set to evaluate the testing
predictive performance of each ML model. The data-split, hyperparameter
selection, training and testing steps were repeated 100 times to get a
reliable and robust reading of model performance {[}Figure 1{]}.

\textbf{Predictive performance and generalizability of the seven
models.} We evaluated the predictive performances of seven binary
classification models when applied to held-out test data using the AUROC
metric {[}Figure 2{]}. Random forest had significantly higher test AUROC
values than the other models for detecting SRNs when AUROC values were
compared to the other six by Wilcoxon rank sum test (p \textless{}
0.01). The median AUROC of the random forest model was 0.695 (IQR
0.044). L2-regularized logistic regression, XGBoost, L2-regularized SVM
with linear and radial basis function kernel AUROC values were not
significantly different from one another. They had median AUROC values
of 0.68 (IQR 0.055), 0.679 (IQR 0.052), 0.678 (IQR 0.056) and 0.668 (IQR
0.056) respectively. L1-regularized SVM with linear kernel and decision
tree had significantly lower AUROC values than the other ML models with
median AUROC of 0.65 (IQR 0.066) and 0.601 (IQR 0.059), respectively
{[}Figure 2{]}. Random forest had the highest median AUROC for detecting
SRN. Despite its simplicity, the L2-regularized logistic regression was
second best in predictive performance.

To evaluate the generalizability of each model, we compared the median
cross-validation AUROC to the median testing AUROC. The difference
between the two should be low to suggest the model is not overfitting
despite the large number of features. The largest difference between the
two was 0.021 in L1-regularized SVM with linear kernel, followed by SVM
with radial basis function kernel and decision tree with a difference of
0.007 and 0.006, respectively {[}Figure 2{]}. We also reported the
testing AUROC values over 100 randomizations of the initial data-split.
The testing AUROC values within each model varied 0.23 on average across
the seven models. For instance, the lowest AUROC value of the random
forest model was 0.59 whereas the highest was 0.81. These results showed
that depending on the data-split, the testing AUROC values showed great
variability {[}Figure 2{]}.

\textbf{Interpretation of each ML model.} Interpretability is the degree
to which humans can understand the reasons behind a model prediction
(31). Because we often use ML models not just to predict a health
outcome but also to learn the ecology behind a disease, model
interpretation becomes crucial for microbiome studies. The ML models we
built using L2-regularized logistic regression, L1 and L2 support vector
machines (SVM) with linear and radial basis function kernels, a decision
tree, random forest and XGBoost decrease in interpretability as they
increase in complexity. In this study we highlighted two methods to
interpret models with varying complexity.

We interpreted linear models (L1 and L2-regularized SVM with linear
kernel and L2-regularized logistic regression) using the absolute
feature weights of the trained models. We ranked the absolute weights of
all the OTUs for each data-split {[}Figure 3{]}. We calculated the
median ranks of these features over the 100 data-splits. In the three
linear models, OTUs that had the largest median ranks and drove the
detection of SRNs belonged to families \emph{Lachnospiraceae}, and
\emph{Ruminococcaceae} (OTU01239, OTU00659, OTU00742, OTU00012,
OTU00015, OTU00768, OTU00822, OTU00609), genera \emph{Gamella}
(OTU00426) and genera \emph{Peptostreptococcus} (OTU00367) {[}Figure
3{]}. Some of the OTUs with the highest ranks were shared among the
linear models.

We explained the feature importances in non-linear models; SVM with
radial basis kernel, decision tree, random forest and XGBoost, using a
method called permutation importance on the held-out test data.
Permutation importance analysis is a posthoc explanation of the model
where we randomly permute non-correlated features individually and
groups of highly correlated features together. We then calculate how
much the predictive performance of the model (i.e AUROC values) decrease
when each OTU or group of OTUs is permuted randomly. We ranked the OTUs
based on how much they decreased the median testing AUROC; the OTU with
the largest decrease ranking highest. The top 5 OTUs with the largest
negative impact on testing AUROC overlapped in tree-based models
{[}Figure 4{]}. Specifically, permuting \emph{Peptostreptococcus}
(OTU00367) abundances randomly, dropped the predictive performances the
most in all tree-based methods {[}Figure 4{]}. Decision tree, random
forest and XGBoost models' predictive performance dropped from 0.6
median AUROC to 0.52, from 0.69 to 0.68 and from 0.68 to 0.65,
respectively {[}Figure 4{]}.

To highlight the differences between the two interpretation methods, we
used permutation importance to interpret linear models as well {[}Figure
S3{]}. L1-regularized SVM with linear kernel picked out some of the same
OTUs (OTU00822, OTU01239, OTU00609) as important in feature rankings
based on weights {[}Figure 3{]} and permutation importance {[}Figure
S3{]}. Similarly, L2-regularized SVM and L2-regularized logistic
regression picked out some of the same OTUs in both interpretation
methods, OTU00659 and OTU00012, respectively. However, for all the
linear models, the rankings of these features were different due to the
collinearity in microbial communities. Collinearity in a microbial
dataset is when one OTU is dependant on another OTU. The feature weights
of correlated OTUs are influenced by one another which makes it
difficult to interpret models using feature weights. Our interpretation
of L2-regularized logistic regression based on feature weight rankings,
showed that an OTU that belongs to \emph{Lachnospiraceae} (OTU00056) was
not among the most important OTUs in making a prediction {[}Figure
3C{]}. However, it was picked out as important when the same model was
interpreted using permutation importance {[}Figure S3C{]}. This is due
to collinearity in the dataset, where \emph{Lachnospiraceae} (OTU00056)
is significantly correlated with \emph{Feacalibacterium} (OTU00015)
which has the highest ranked feature weights {[}Figure 3C{]}. We need to
investigate the relationships among features to identify the true
underlying factors when making a prediction.

\textbf{The computational efficiency of each ML model.}

We compared the training times of the seven ML models. As the complexity
of a ML model and the number of tuned hyperparameter settings increased
{[}Figures S1-S2{]}, its training times increased as well {[}Figure
5{]}. Linear models trained faster than non-linear models. L1 and
L2-regularized SVM with linear kernel and L2-regularized logistic
regression had the shortest training times with 0.2 hours, (std ± 0.03),
0.2 hours, (std ± 0.02), and 0.2 hours, (std ± 0.02), respectively.
Whereas, a decision tree, SVM with radial basis function kernel, random
forest and XGBoost had training times of 4.4 hours, (std ± 0.3), 59.6
hours, (std ± 8.8), 83.2 hours, (std ± 11.3) and 155.1 hours, (std ± 1),
respectively {[}Figure 5{]}.

\subsection{Discussion}\label{discussion}

Microbiome studies use ML models, often with a classification task, to
predict a disease but also to learn which microbes are indicators of
that disease (2--11). Achieving either of these tasks have far-reaching
impact on human health, however ML as a tool in microbiome studies is
still at its infancy. A framework is needed to develop rigorous ML
models, to identify and overcome potential pitfalls. Previous studies
generated workflows to allow ML to be widely used by the microbiome
researchers (20--23). This study sets-up standards for developing and
evaluating rigorous ML models for microbiome data {[}Table 1{]}.

We benchmarked seven ML models with different classification algorithms
to show that a clearly defined ML problem that is based on the goal of
the microbiome study should inform our model selection. Our results
showed that if the goal of a study is to learn the ecology behind a
disease and to identify microbial biomarkers, we can create ML models
that are inherently interpretable and easily trained without losing
predictive power. In terms of predictive performance, random forest
model had the best testing AUROC values compared to the other six
models. However, the second best model was L2-regularized logistic
regression with a median AUROC difference of only 0.015 compared to
random forest. While random forest took 83.2 hours to train,
L2-regularized logistic regression trained in 12 minutes. In terms of
interpretability, random forest was a more complex ML model and could
only be explained using methods such as permutation importance. On the
other hand, L2-regularized logistic regression was easier to interpret
by ranking absolute feature weights of the trained model.

Even with interpretable models such as L2-regularized logistic
regression, there are potential pitfalls when it comes to identifying
biomarkers of a disease. As domain experts, we know that
human-associated microbial communities have complex correlation
structures that creates collinearity in the dataset. Collinearity is a
severe problem and needs to be addressed to reliably interpret ML models
(32). In this study we used two different methods to interpret our
linear models; ranking each OTU by (1) their absolute weights in the
trained models and (2) their impact on the predictive performance based
on permutation importance. We observed differences in the OTU rankings
between the two interpretation methods due to collinearity in the
dataset. To avoid misinterpreting the models, once we identify the most
important microbes, we should check for their relationships with other
microbes as well. These relationships will help us generate new
hypotheses about the ecology of the disease. These hypotheses needs to
be tested with follow-up experiments to identify the true biomarkers of
a disease.

In this study, we also established a rigorous ML pipeline to use 16S
rRNA sequence counts to predict a binary health outcome. First of all,
we used a held-out test set to illustrate the difference between
cross-validation and testing AUROC values. When the difference between
cross-validation and test performance is low, this suggest the models
are not overfit and that they will perform similar with similar data. In
all the models, the difference between median cross-validation and
testing AUROC values did not exceed 0.021 which suggests that these
models are generalizable and can be used to test similar new data.
Furthermore, we performed the initial 80\%-20\% random datasplit 100
times in our ML pipeline. Depending on how the data is split, there is
the chance of being overoptimistic about the predictive performance of a
model. We showed that there was variability in AUROC values between
different random data-splits in each of the models we tested. Our
results showed that the testing AUROC values varied 0.23 on average
between different data-splits. The randomization and resampling of the
initial data-split to create a held-out test set is a crucial step in
the ML pipeline to develop robust ML models and to report reliable
performance metrics. Additionally, we performed a full grid search for
hyperparameter settings when training our ML models. Default
hyperparameter settings in previously developed ML packages in R,
Python, and Matlab programming languages are inadequate for effective
application of classification algorithms and need to be optimized for
each new dataset used to generate a model. In the example of
L1-regularized SVM with linear kernel {[}Figure S1{]}, the model showed
large variability between different regularization coefficients (C) and
was susceptible to performing poorly if the wrong regularization
coefficient was assigned to the model by default. And finally, we used
the AUROC metric in our study to evaluate the predictive performance of
the ML models. AUROC is always random at the value 0.5 and is a robust
metric when a dataset is imbalanced.

We used a balanced CRC dataset to develop ML models with a binary
classification task. We did not evaluate multicategory classification
methods or regression analyses to predict non-binary outcomes. However,
the principles highlighted throughout this study {[}Table 1{]} apply to
all ML modeling tasks with microbiome data. The models we built were
generalizable despite the high number of features microbiome datasets
usually have. The generalization performance of ML models depends on
sample size. The more complex the model, the more data it will need. Our
dataset had 490 samples, however microbiome studies that have smaller
sample sizes would benefit from using less complex models. Our analysis
was limited to shallow learning methods and did not explore deep
learning methods such as neural networks. Microbiome datasets often
suffer from having high dimensionality but low sample sizes which makes
deep learning models prone to overfitting. There are studies that
address overcoming these challenges in biomedical datasets (11, 33, 34),
however sudies that estalish frameworks with microbiome data are
lacking. This would be an interesting direction for future work in
microbiome studies.

This study highlighted the need to make educated choices at every step
of developing a ML model with microbiome data. Model selection should be
done with a solid understanding of model complexity and
interpretability, rigorous ML pipelines should be built with
cross-validation for hyperparameter tuning and with a held-out test set
for evaluating predictive performance and models should be interpreted
while considering collinearity in datasets. The right methods will help
us achieve the level of validity and accountability we want from models
built for patient health.

\subsection{Materials and Methods}\label{materials-and-methods}

\textbf{Data collection and study population.} The data used for this
analysis are stool bacterial abundances and clinical information of the
patients recruited by Great Lakes-New England Early Detection Research
Network study. These data were obtained from Sze et al (35). The stool
samples were provided by recruited adult participants who were
undergoing scheduled screening or surveillance colonoscopy.
Colonoscopies were performed and fecal samples were collected from
participants in four locations: Toronto (ON, Canada), Boston (MA, USA),
Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic health was
labeled by colonoscopy with adequate preparation and tissue
histopathology of all resected lesions. Patients with an adenoma greater
than 1 cm, more than three adenomas of any size, or an adenoma with
villous histology were classified as advanced adenoma. Study had 172
patients with normal colonoscopies, 198 with adenomas and 120 with
carcinomas. Of the 198 adenomas, 109 were identified as advanced
adenomas. Stool provided by the patients was used for 16S rRNA gene
sequencing to measure bacterial population abundances. The bacterial
abundance data was generated by Sze et al, by processing 16S rRNA
sequences in Mothur (v1.39.3) using the default quality filtering
methods, identifying and removing chimeric sequences using VSEARCH and
assigning to OTUs at 97\% similarity using the OptiClust algorithm
(36--38).

\textbf{Data definitions and pre-processing.}

The colorectal health of the patient was defined as two encompassing
classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class
includes patients with non-advanced adenomas or normal colons whereas
SRN class includes patients with advanced adenomas or carcinomas. The
study had 261 normal and 229 SRN samples. The bacterial abundances are
the features used to predict colorectal health of the patients. For each
patient, we had 6920 features (fecal bacterial abundances) and a
two-class label that defines their colorectal health (normal or SRN
colorectal lesions as defined by colonoscopies). We established modeling
pipelines for a binary prediction task Bacterial abundances are discrete
data in the form of Operational Taxonomic Unit (OTU) counts. OTU counts
were set to the size of our smallest sample and were subsampled at the
same distances. They were then transformed by scaling to a {[}0-1{]}
range.

\textbf{Model training and evaluation.}

Models were trained using the machine learning wrapper caret package
(v.6.0.81) in R (v.3.5.0). Within the caret package, we have made
modifications to L2-regularized SVM with linear kernel function
\textbf{svmLinear3} and developed a L1-regularized SVM with linear
kernel function \textbf{svmLinear4} to calculate decision values instead
of predicted probabilities. These changes are available at
\url{https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/}.

For L2-regularized logistic regression, L1 and L2 support vector
machines (SVM) with linear and radial basis function kernels we tuned
the \textbf{cost} hyperparameter which determines the regularization
strength where smaller values specify stronger regularization. For SVM
with radial basis function kernel we also tuned \textbf{sigma}
hyperparameter which determines the reach of a single training instance
where for a high value of sigma, the SVM decision boundary will be
dependent on the points that are closest to the decision boundary. For
the decision tree model, we tuned the \textbf{depth of the tree} where
deeper the tree, the more splits it has. For random forest, we tuned the
\textbf{number of features} to consider when looking for the best tree
split. For XGBoost, we tuned for \textbf{learning rate} and the
\textbf{fraction of samples} to be used for fitting the individual base
learners.For hyperparameter selection, we started with a granular grid
search. Then we narrowed and fine-tuned the range of each
hyperparameter. The range of the grid depends on the ML task and ML
model. A full grid search needs to be performed to avoid variability in
testing performance. We can use hyper-band to help us with our
hyperparameter selection (39).

The computational burden during model training due to model complexity
was reduced by parallelizing segments of the ML pipeline. In this study
we have parallelized each data-split which allowed 100 data-splits to be
processed through the ML pipeline at the same time for each model. We
can further parallelize the cross-validation step for each
hyperparameter setting.

\textbf{Permutation importance workflow.} We created a Spearman's
rank-order correlation matrix, corrected for multiple pairwise
comparisons. We then defined correlated OTUs as having perfect
correlation (correlation coef=1 and p\textless{}0.01). Non-correlated
OTUs were permuted individually whereas correlated ones were grouped
together and permuted at the same time.

\textbf{Statistical analysis workflow.} Data summaries, statistical
analysis, and data visualizations were performed using R (v.3.5.0) with
the tidyverse package (v.1.2.1). We compared the AUROC values of the
seven ML models by Wilcoxon rank sum tests to determine the best
predictive performance.

\textbf{Code availability.} The code for all sequence curation and
analysis steps including an Rmarkdown version of this manuscript is
available at
\url{https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/}.

\newpage

\includegraphics{Figure_1} \textbf{Figure 1. Machine learning pipeline
showing predictive model training and evaluation flowchart. } We split
the data 80\%/20\% stratified to maintain the overall label
distribution, performed five-fold cross-validation on the training data
to select the best hyperparameter setting and then using these
hyperparameters to train all of the training data. The model was
evaluated on a held-out set of data (not used in selecting the model).
Abbreviations: cvAUROC, cross-validation area under the receiver
operating characteristic curve \newpage
\includegraphics{Figure_2.png}

\textbf{Figure 2. Generalization and classification performance of ML
models using AUROC values of all cross validation and testing
performances. } The median AUROC for diagnosing individuals with SRN
using bacterial abundances was higher than chance (depicted by
horizontal line at 0.50) for all the ML models. Predictive performance
of random forest model was higher than other ML models. The boxplot
shows quartiles at the box ends and the statistical median as the
horizontal line in the box. The whiskers show the farthest points that
are not outliers. Outliers are data points that are not within 3/2 times
the interquartile ranges. Abbreviations: SRN, screen-relevant
neoplasias; AUROC, area under the receiver operating characteristic
curve; SVM, support vector machine; XGBoost, extreme gradient boosting
\newpage
\includegraphics[height=17cm, width=12cm]{Figure_3.png}

\textbf{Figure 3. Interpretation of the linear ML models.} The absolute
feature weights of (A) L2 logistic regression coefficients (B) L1 SVM
with linear kernel (C) L2 SVM with linear kernel were ranked from
highest rank 1 to 100 for each data-split. The feature ranks of the
highest ranked five OTUs based on their median ranks are shown here.
Similar OTUs had the largest impact on the predictive performance of L2
logistic regression and L2 SVM with linear kernel. Abbreviations: SVM,
support vector machine; OTU, Operational Taxonomic Unit. \newpage
\includegraphics{Figure_4.png}

\textbf{Figure 4. Interpretation of the non-linear ML models.} (A) SVM
with radial basis kernel (B) decision tree (C) random forest (D) XGBoost
feature importances were explained using permutation importance using
held-out test set. The gray rectangle and the dashed line show the IQR
range and median of the base testing AUROC without any permutation
performed. The colors of the box plots stand for the unique OTUs that
are shared among the different models; pink for OTU0008, salmon for
OTU0050, yellow for OTU00367, blue for OTU00110, green for OTU00361 and
red for OTU00882. For all the tree-based models, a
\emph{Peptostreptococcus} species (OTU00367) had the largest impact on
predictive performance of the model. Abbreviations: SVM, support vector
machine; OTU, Operational Taxonomic Unit; RBF, radial basis kernel; OTU,
Operational Taxonomic Unit. \newpage
\includegraphics{Figure_5.png}

\textbf{Figure 5. Computational efficiency of seven ML models.} The
training times for of each data-split showed the differences in
computational efficiency of the seven models. The median training time
in hours was the highest for XGBoost and shortest for L1-regularized SVM
with linear kernel. The boxplot shows quartiles at the box ends and the
statistical median as the horizontal line in the box. The whiskers show
the farthest points that are not outliers. Outliers are data points that
are not within 3/2 times the interquartile ranges. Abbreviations: AUROC,
area under the receiver operating characteristic curve; SVM, support
vector machine; XGBoost, extreme gradient boosting. \newpage
\includegraphics{Figure_S1.png}

\textbf{Figure S1. Hyperparameter setting performances for linear
models.} (A) L2 logistic regression (B) L1 SVM with linear kernel (C) L2
SVM with linear kernel mean cross-validation AUROC values when different
hyperparameters are used in training the model. The differences in AUROC
values when hyperparameters change show that hyperparameter tuning is a
crucial step in building a ML model.

\newpage

\includegraphics[height=30cm, width=15cm]{Figure_S2.png}

\textbf{Figure S2. Hyperparameter setting performances for non-linear
models.} (A) Decision tree (B) Random forest (C) SVM with radial basis
kernel (D) XGBoost mean cross-validation AUROC values when different
hyperparameters are used in training the model. The differences in AUROC
values when hyperparameters change show that hyperparameter tuning is a
crucial step in building a ML model. \newpage
\includegraphics[height=17.5cm, width=13cm]{Figure_S3.png}

\textbf{Figure S3. Interpretation of the linear ML models with
permutation importance.} (A) L1-regularized SVM with linear kernel (B)
L2-regularized SVM with linear kernel and (C) L2-regularized logistic
regression were interpreted using permutation importance using held-out
test set. The gray rectangle and the dashed line show the IQR range and
median of the base testing AUROC without any permutation performed.
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic
Unit; RBF, radial basis kernel; OTU, Operational Taxonomic Unit.

\newpage

\captionof{table}{Characteristics of the machine learning models in our comparative study.}
\small

\begin{tabular}{|l|l|l|l|l|}
\hline

\rowcolor{lightgray}
\textbf{Model} & \textbf{Description} & \textbf{Linearity} & \textbf{Interpretability} & \textbf{Refs.} \\ \hline

\makecell[l]{Logistic \\regression} & \makecell[l]{A predictive regression analysis when the dependent \\variable is binary.} & Linear & Interpretable & $^{36}$\\ \hline

\makecell[l]{SVM with \\linear kernel} & \makecell[l]{A classifier that is defined by an optimal linear \\separating hyperplane that discriminates between labels.} & Linear & Interpretable & $^{37}$\\ \hline

\makecell[l]{SVM with \\radial basis kernel} & \makecell[l]{A classifier that is defined by an optimal Gaussian \\separating hyperplane that discriminates between labels.} & Non-linear & Explainable$^*$ & $^{38}$\\ \hline
Decision tree & \makecell[l]{A classifier that sorts samples down from the
root to the \\leaf node where an attribute is tested to discriminate \\between labels} & Non-linear & Interpretable & $^{39}$\\ \hline

Random forest & \makecell[l]{A classifier that is a decision tree ensemble that \\grow randomly with subsampled data.} & Non-linear & Explainable$^*$ & $^{40-41}$  \\ \hline

XGBoost & \makecell[l]{A classifier that is a decision tree ensemble that \\grow with additive training.} & Non-linear & Explainable$^*$ & $^{42-43}$ \\ \hline

\end{tabular}\begin{tablenotes}\footnotesize
\item[1]{$^*$Explainable models are not inherently interpretable but can be explained with post-hoc analyses.}
\end{tablenotes}\newpage

\captionof{table}{An aspirational rubric for evaluating the rigor of ML practices.}
\small

\begin{tabular}{|l|l|l|l|}
\hline

\rowcolor{lightgray}
\textbf{Practice} & \textbf{Good} & \textbf{Better} & \textbf{Best} \\ \hline

\makecell[l]{Problem \\ definition} & \makecell[l]{Have we clearly stated \\ the ML task? \\ Do we have a priori hypotheses? \\Do we know the predictions \\ a domain expert would \\ make manually?} & \makecell[l]{Do we know the motivation \\ for solving the problem? \\ How much interpretability \\ does the problem need? } & \makecell[l]{Do we know our data? \\ Do we know the confounding \\ variables? } \\ \hline

\makecell[l]{Model \\ selection} & \makecell[l]{Do we know the \\ candidate algorithms \\ for the ML problem? \\} & \makecell[l]{Do we know our \\ computational resources \\ to fully train each model?} & \makecell[l]{How much interpretability \\ does the problem need? \\ How much each \\ candidate algorithm \\ can provide?} \\ \hline

\makecell[l]{ML pipeline \\ preparation} & \makecell[l]{Do we have an held-out \\ test dataset?} & \makecell[l]{Have we tested our model \\ on many different \\ held-out datasets?} & \makecell[l]{Have we tuned our model \\ hyperparameters in \\ cross-validation?} \\ \hline

\makecell[l]{Hyperparameter \\ selection} & \makecell[l]{Do we know the different \\ hyperparameters each model \\ can use and why?} & \makecell[l]{Did we use historically \\ effective hyperparameters? } & \makecell[l]{Did we search the \\ full grid space and \\optimized our model? } \\ \hline

\makecell[l]{Model \\ evaluation} & \makecell[l]{Have we chosen an appropriate \\ metric to evaluate \\ predictive performance?} & \makecell[l]{Have we reported \\ the predictive performance \\ on a held-out test data?} & \makecell[l]{Have we provided an average \\ predictive performance \\ of many model runs? } \\ \hline

\makecell[l]{Model \\ interpretation} & \makecell[l]{Do we know if \\ our model is \\ interpretable?} & \makecell[l]{If the model is not \\ interpretable, do we know \\how to  explain it? \\ Have we checked for \\ the effect of confounding \\ variables?} & \makecell[l]{Have we generated new \\ hypotheses based on \\ model interpretation \\ to test model results? } \\ \hline

\end{tabular}\newpage

\subsection{References}\label{references}

\hypertarget{refs}{}
\hypertarget{ref-zeller_potential_2014}{}
1. \textbf{Zeller G}, \textbf{Tap J}, \textbf{Voigt AY},
\textbf{Sunagawa S}, \textbf{Kultima JR}, \textbf{Costea PI},
\textbf{Amiot A}, \textbf{Böhm J}, \textbf{Brunetti F},
\textbf{Habermann N}, \textbf{Hercog R}, \textbf{Koch M},
\textbf{Luciani A}, \textbf{Mende DR}, \textbf{Schneider MA},
\textbf{Schrotz-King P}, \textbf{Tournigand C}, \textbf{Tran Van Nhieu
J}, \textbf{Yamada T}, \textbf{Zimmermann J}, \textbf{Benes V},
\textbf{Kloor M}, \textbf{Ulrich CM}, \textbf{Knebel Doeberitz M von},
\textbf{Sobhani I}, \textbf{Bork P}. 2014. Potential of fecal microbiota
for early-stage detection of colorectal cancer. Mol Syst Biol
\textbf{10}.
doi:\href{https://doi.org/10.15252/msb.20145645}{10.15252/msb.20145645}.

\hypertarget{ref-zackular_human_2014}{}
2. \textbf{Zackular JP}, \textbf{Rogers MAM}, \textbf{Ruffin MT},
\textbf{Schloss PD}. 2014. The human gut microbiome as a screening tool
for colorectal cancer. Cancer Prev Res \textbf{7}:1112--1121.
doi:\href{https://doi.org/10.1158/1940-6207.CAPR-14-0129}{10.1158/1940-6207.CAPR-14-0129}.

\hypertarget{ref-baxter_dna_2016}{}
3. \textbf{Baxter NT}, \textbf{Koumpouras CC}, \textbf{Rogers MAM},
\textbf{Ruffin MT}, \textbf{Schloss PD}. 2016. DNA from fecal
immunochemical test can replace stool for detection of colonic lesions
using a microbiota-based model. Microbiome \textbf{4}.
doi:\href{https://doi.org/10.1186/s40168-016-0205-y}{10.1186/s40168-016-0205-y}.

\hypertarget{ref-baxter_microbiota-based_2016}{}
4. \textbf{Baxter NT}, \textbf{Ruffin MT}, \textbf{Rogers MAM},
\textbf{Schloss PD}. 2016. Microbiota-based model improves the
sensitivity of fecal immunochemical test for detecting colonic lesions.
Genome Medicine \textbf{8}:37.
doi:\href{https://doi.org/10.1186/s13073-016-0290-3}{10.1186/s13073-016-0290-3}.

\hypertarget{ref-hale_shifts_2017}{}
5. \textbf{Hale VL}, \textbf{Chen J}, \textbf{Johnson S},
\textbf{Harrington SC}, \textbf{Yab TC}, \textbf{Smyrk TC},
\textbf{Nelson H}, \textbf{Boardman LA}, \textbf{Druliner BR},
\textbf{Levin TR}, \textbf{Rex DK}, \textbf{Ahnen DJ}, \textbf{Lance P},
\textbf{Ahlquist DA}, \textbf{Chia N}. 2017. Shifts in the fecal
microbiota associated with adenomatous polyps. Cancer Epidemiol
Biomarkers Prev \textbf{26}:85--94.
doi:\href{https://doi.org/10.1158/1055-9965.EPI-16-0337}{10.1158/1055-9965.EPI-16-0337}.

\hypertarget{ref-pasolli_machine_2016}{}
6. \textbf{Pasolli E}, \textbf{Truong DT}, \textbf{Malik F},
\textbf{Waldron L}, \textbf{Segata N}. 2016. Machine learning
meta-analysis of large metagenomic datasets: Tools and biological
insights. PLoS Comput Biol \textbf{12}.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004977}{10.1371/journal.pcbi.1004977}.

\hypertarget{ref-sze_looking_2016}{}
7. \textbf{Sze MA}, \textbf{Schloss PD}. 2016. Looking for a signal in
the noise: Revisiting obesity and the microbiome. mBio \textbf{7}.
doi:\href{https://doi.org/10.1128/mBio.01018-16}{10.1128/mBio.01018-16}.

\hypertarget{ref-walters_meta-analyses_2014}{}
8. \textbf{Walters WA}, \textbf{Xu Z}, \textbf{Knight R}. 2014.
Meta-analyses of human gut microbes associated with obesity and IBD.
FEBS Lett \textbf{588}:4223--4233.
doi:\href{https://doi.org/10.1016/j.febslet.2014.09.039}{10.1016/j.febslet.2014.09.039}.

\hypertarget{ref-vazquez-baeza_guiding_2018}{}
9. \textbf{Vázquez-Baeza Y}, \textbf{Gonzalez A}, \textbf{Xu ZZ},
\textbf{Washburne A}, \textbf{Herfarth HH}, \textbf{Sartor RB},
\textbf{Knight R}. 2018. Guiding longitudinal sampling in IBD cohorts.
Gut \textbf{67}:1743--1745.
doi:\href{https://doi.org/10.1136/gutjnl-2017-315352}{10.1136/gutjnl-2017-315352}.

\hypertarget{ref-qin_alterations_2014}{}
10. \textbf{Qin N}, \textbf{Yang F}, \textbf{Li A}, \textbf{Prifti E},
\textbf{Chen Y}, \textbf{Shao L}, \textbf{Guo J}, \textbf{Le Chatelier
E}, \textbf{Yao J}, \textbf{Wu L}, \textbf{Zhou J}, \textbf{Ni S},
\textbf{Liu L}, \textbf{Pons N}, \textbf{Batto JM}, \textbf{Kennedy SP},
\textbf{Leonard P}, \textbf{Yuan C}, \textbf{Ding W}, \textbf{Chen Y},
\textbf{Hu X}, \textbf{Zheng B}, \textbf{Qian G}, \textbf{Xu W},
\textbf{Ehrlich SD}, \textbf{Zheng S}, \textbf{Li L}. 2014. Alterations
of the human gut microbiome in liver cirrhosis. Nature
\textbf{513}:59--64.
doi:\href{https://doi.org/10.1038/nature13568}{10.1038/nature13568}.

\hypertarget{ref-geman_deep_2018}{}
11. \textbf{Geman O}, \textbf{Chiuchisan I}, \textbf{Covasa M},
\textbf{Doloc C}, \textbf{Milici M-R}, \textbf{Milici L-D}. 2018. Deep
learning tools for human microbiome big data, pp. 265--275. \emph{In}
Balas, VE, Jain, LC, Balas, MM (eds.), Soft computing applications.
Springer International Publishing.

\hypertarget{ref-thaiss_persistent_2016}{}
12. \textbf{Thaiss CA}, \textbf{Itav S}, \textbf{Rothschild D},
\textbf{Meijer MT}, \textbf{Levy M}, \textbf{Moresi C},
\textbf{Dohnalová L}, \textbf{Braverman S}, \textbf{Rozin S},
\textbf{Malitsky S}, \textbf{Dori-Bachash M}, \textbf{Kuperman Y},
\textbf{Biton I}, \textbf{Gertler A}, \textbf{Harmelin A},
\textbf{Shapiro H}, \textbf{Halpern Z}, \textbf{Aharoni A},
\textbf{Segal E}, \textbf{Elinav E}. 2016. Persistent microbiome
alterations modulate the rate of post-dieting weight regain. Nature
\textbf{540}:544--551.
doi:\href{https://doi.org/10.1038/nature20796}{10.1038/nature20796}.

\hypertarget{ref-dadkhah_gut_2019}{}
13. \textbf{Dadkhah E}, \textbf{Sikaroodi M}, \textbf{Korman L},
\textbf{Hardi R}, \textbf{Baybick J}, \textbf{Hanzel D}, \textbf{Kuehn
G}, \textbf{Kuehn T}, \textbf{Gillevet PM}. 2019. Gut microbiome
identifies risk for colorectal polyps. BMJ Open Gastroenterology
\textbf{6}:e000297.
doi:\href{https://doi.org/10.1136/bmjgast-2019-000297}{10.1136/bmjgast-2019-000297}.

\hypertarget{ref-flemer_oral_2018}{}
14. \textbf{Flemer B}, \textbf{Warren RD}, \textbf{Barrett MP},
\textbf{Cisek K}, \textbf{Das A}, \textbf{Jeffery IB}, \textbf{Hurley
E}, \textbf{O`Riordain M}, \textbf{Shanahan F}, \textbf{O`Toole PW}.
2018. The oral microbiota in colorectal cancer is distinctive and
predictive. Gut \textbf{67}:1454--1463.
doi:\href{https://doi.org/10.1136/gutjnl-2017-314814}{10.1136/gutjnl-2017-314814}.

\hypertarget{ref-montassier_pretreatment_2016}{}
15. \textbf{Montassier E}, \textbf{Al-Ghalith GA}, \textbf{Ward T},
\textbf{Corvec S}, \textbf{Gastinne T}, \textbf{Potel G}, \textbf{Moreau
P}, \textbf{Cochetiere MF de la}, \textbf{Batard E}, \textbf{Knights D}.
2016. Pretreatment gut microbiome predicts chemotherapy-related
bloodstream infection. Genome Medicine \textbf{8}:49.
doi:\href{https://doi.org/10.1186/s13073-016-0301-4}{10.1186/s13073-016-0301-4}.

\hypertarget{ref-ai_systematic_2017}{}
16. \textbf{Ai L}, \textbf{Tian H}, \textbf{Chen Z}, \textbf{Chen H},
\textbf{Xu J}, \textbf{Fang J-Y}. 2017. Systematic evaluation of
supervised classifiers for fecal microbiota-based prediction of
colorectal cancer. Oncotarget \textbf{8}:9546--9556.
doi:\href{https://doi.org/10.18632/oncotarget.14488}{10.18632/oncotarget.14488}.

\hypertarget{ref-dai_multi-cohort_2018}{}
17. \textbf{Dai Z}, \textbf{Coker OO}, \textbf{Nakatsu G}, \textbf{Wu
WKK}, \textbf{Zhao L}, \textbf{Chen Z}, \textbf{Chan FKL},
\textbf{Kristiansen K}, \textbf{Sung JJY}, \textbf{Wong SH}, \textbf{Yu
J}. 2018. Multi-cohort analysis of colorectal cancer metagenome
identified altered bacteria across populations and universal bacterial
markers. Microbiome \textbf{6}:70.
doi:\href{https://doi.org/10.1186/s40168-018-0451-2}{10.1186/s40168-018-0451-2}.

\hypertarget{ref-mossotto_classification_2017}{}
18. \textbf{Mossotto E}, \textbf{Ashton JJ}, \textbf{Coelho T},
\textbf{Beattie RM}, \textbf{MacArthur BD}, \textbf{Ennis S}. 2017.
Classification of paediatric inflammatory bowel disease using machine
learning. Scientific Reports \textbf{7}.
doi:\href{https://doi.org/10.1038/s41598-017-02606-2}{10.1038/s41598-017-02606-2}.

\hypertarget{ref-wong_quantitation_2017}{}
19. \textbf{Wong SH}, \textbf{Kwong TNY}, \textbf{Chow T-C}, \textbf{Luk
AKC}, \textbf{Dai RZW}, \textbf{Nakatsu G}, \textbf{Lam TYT},
\textbf{Zhang L}, \textbf{Wu JCY}, \textbf{Chan FKL}, \textbf{Ng SSM},
\textbf{Wong MCS}, \textbf{Ng SC}, \textbf{Wu WKK}, \textbf{Yu J},
\textbf{Sung JJY}. 2017. Quantitation of faecal fusobacterium improves
faecal immunochemical test in detecting advanced colorectal neoplasia.
Gut \textbf{66}:1441--1448.
doi:\href{https://doi.org/10.1136/gutjnl-2016-312766}{10.1136/gutjnl-2016-312766}.

\hypertarget{ref-statnikov_comprehensive_2013}{}
20. \textbf{Statnikov A}, \textbf{Henaff M}, \textbf{Narendra V},
\textbf{Konganti K}, \textbf{Li Z}, \textbf{Yang L}, \textbf{Pei Z},
\textbf{Blaser MJ}, \textbf{Aliferis CF}, \textbf{Alekseyenko AV}. 2013.
A comprehensive evaluation of multicategory classification methods for
microbiomic data. Microbiome \textbf{1}:11.
doi:\href{https://doi.org/10.1186/2049-2618-1-11}{10.1186/2049-2618-1-11}.

\hypertarget{ref-knights_supervised_2011}{}
21. \textbf{Knights D}, \textbf{Costello EK}, \textbf{Knight R}. 2011.
Supervised classification of human microbiota. FEMS Microbiology Reviews
\textbf{35}:343--359.
doi:\href{https://doi.org/10.1111/j.1574-6976.2010.00251.x}{10.1111/j.1574-6976.2010.00251.x}.

\hypertarget{ref-wirbel_meta-analysis_2019}{}
22. \textbf{Wirbel J}, \textbf{Pyl PT}, \textbf{Kartal E}, \textbf{Zych
K}, \textbf{Kashani A}, \textbf{Milanese A}, \textbf{Fleck JS},
\textbf{Voigt AY}, \textbf{Palleja A}, \textbf{Ponnudurai R},
\textbf{Sunagawa S}, \textbf{Coelho LP}, \textbf{Schrotz-King P},
\textbf{Vogtmann E}, \textbf{Habermann N}, \textbf{Niméus E},
\textbf{Thomas AM}, \textbf{Manghi P}, \textbf{Gandini S},
\textbf{Serrano D}, \textbf{Mizutani S}, \textbf{Shiroma H},
\textbf{Shiba S}, \textbf{Shibata T}, \textbf{Yachida S}, \textbf{Yamada
T}, \textbf{Waldron L}, \textbf{Naccarati A}, \textbf{Segata N},
\textbf{Sinha R}, \textbf{Ulrich CM}, \textbf{Brenner H},
\textbf{Arumugam M}, \textbf{Bork P}, \textbf{Zeller G}. 2019.
Meta-analysis of fecal metagenomes reveals global microbial signatures
that are specific for colorectal cancer. Nature Medicine
\textbf{25}:679.
doi:\href{https://doi.org/10.1038/s41591-019-0406-6}{10.1038/s41591-019-0406-6}.

\hypertarget{ref-vangay_microbiome_2019}{}
23. \textbf{Vangay P}, \textbf{Hillmann BM}, \textbf{Knights D}. 2019.
Microbiome learning repo (ML repo): A public repository of microbiome
regression and classification tasks. Gigascience \textbf{8}.
doi:\href{https://doi.org/10.1093/gigascience/giz042}{10.1093/gigascience/giz042}.

\hypertarget{ref-galkin_human_2018}{}
24. \textbf{Galkin F}, \textbf{Aliper A}, \textbf{Putin E},
\textbf{Kuznetsov I}, \textbf{Gladyshev VN}, \textbf{Zhavoronkov A}.
2018. Human microbiome aging clocks based on deep learning and tandem of
permutation feature importance and accumulated local effects. bioRxiv.
doi:\href{https://doi.org/10.1101/507780}{10.1101/507780}.

\hypertarget{ref-reiman_using_2017}{}
25. \textbf{Reiman D}, \textbf{Metwally A}, \textbf{Dai Y}. 2017. Using
convolutional neural networks to explore the microbiome, pp. 4269--4272.
\emph{In} 2017 39th annual international conference of the IEEE
engineering in medicine and biology society (EMBC).

\hypertarget{ref-fioravanti_phylogenetic_2017}{}
26. \textbf{Fioravanti D}, \textbf{Giarratano Y}, \textbf{Maggio V},
\textbf{Agostinelli C}, \textbf{Chierici M}, \textbf{Jurman G},
\textbf{Furlanello C}. 2017. Phylogenetic convolutional neural networks
in metagenomics. arXiv:170902268 {[}cs, q-bio{]}.

\hypertarget{ref-thomas_metagenomic_2019}{}
27. \textbf{Thomas AM}, \textbf{Manghi P}, \textbf{Asnicar F},
\textbf{Pasolli E}, \textbf{Armanini F}, \textbf{Zolfo M},
\textbf{Beghini F}, \textbf{Manara S}, \textbf{Karcher N}, \textbf{Pozzi
C}, \textbf{Gandini S}, \textbf{Serrano D}, \textbf{Tarallo S},
\textbf{Francavilla A}, \textbf{Gallo G}, \textbf{Trompetto M},
\textbf{Ferrero G}, \textbf{Mizutani S}, \textbf{Shiroma H},
\textbf{Shiba S}, \textbf{Shibata T}, \textbf{Yachida S}, \textbf{Yamada
T}, \textbf{Wirbel J}, \textbf{Schrotz-King P}, \textbf{Ulrich CM},
\textbf{Brenner H}, \textbf{Arumugam M}, \textbf{Bork P}, \textbf{Zeller
G}, \textbf{Cordero F}, \textbf{Dias-Neto E}, \textbf{Setubal JC},
\textbf{Tett A}, \textbf{Pardini B}, \textbf{Rescigno M},
\textbf{Waldron L}, \textbf{Naccarati A}, \textbf{Segata N}. 2019.
Metagenomic analysis of colorectal cancer datasets identifies
cross-cohort microbial diagnostic signatures and a link with choline
degradation. Nature Medicine \textbf{25}:667.
doi:\href{https://doi.org/10.1038/s41591-019-0405-7}{10.1038/s41591-019-0405-7}.

\hypertarget{ref-rudin_please_2018}{}
28. \textbf{Rudin C}. 2018. Please stop explaining black box models for
high stakes decisions. arXiv:181110154 {[}cs, stat{]}.

\hypertarget{ref-rudin_optimized_2018}{}
29. \textbf{Rudin C}, \textbf{Ustun B}. 2018. Optimized scoring systems:
Toward trust in machine learning for healthcare and criminal justice.
Interfaces \textbf{48}:449--466.
doi:\href{https://doi.org/10.1287/inte.2018.0957}{10.1287/inte.2018.0957}.

\hypertarget{ref-knights_human-associated_2011}{}
30. \textbf{Knights D}, \textbf{Parfrey LW}, \textbf{Zaneveld J},
\textbf{Lozupone C}, \textbf{Knight R}. 2011. Human-associated microbial
signatures: Examining their predictive value. Cell Host Microbe
\textbf{10}:292--296.
doi:\href{https://doi.org/10.1016/j.chom.2011.09.003}{10.1016/j.chom.2011.09.003}.

\hypertarget{ref-miller_explanation_2017}{}
31. \textbf{Miller T}. 2017. Explanation in artificial intelligence:
Insights from the social sciences. arXiv:170607269 {[}cs{]}.

\hypertarget{ref-dormann_collinearity:_2013}{}
32. \textbf{Dormann CF}, \textbf{Elith J}, \textbf{Bacher S},
\textbf{Buchmann C}, \textbf{Carl G}, \textbf{Carré G}, \textbf{Marquéz
JRG}, \textbf{Gruber B}, \textbf{Lafourcade B}, \textbf{Leitão PJ},
\textbf{Münkemüller T}, \textbf{McClean C}, \textbf{Osborne PE},
\textbf{Reineking B}, \textbf{Schröder B}, \textbf{Skidmore AK},
\textbf{Zurell D}, \textbf{Lautenbach S}. 2013. Collinearity: A review
of methods to deal with it and a simulation study evaluating their
performance. Ecography \textbf{36}:27--46.
doi:\href{https://doi.org/10.1111/j.1600-0587.2012.07348.x}{10.1111/j.1600-0587.2012.07348.x}.

\hypertarget{ref-kocheturov_massive_2019}{}
33. \textbf{Kocheturov A}, \textbf{Pardalos PM}, \textbf{Karakitsiou A}.
2019. Massive datasets and machine learning for computational
biomedicine: Trends and challenges. Ann Oper Res \textbf{276}:5--34.
doi:\href{https://doi.org/10.1007/s10479-018-2891-2}{10.1007/s10479-018-2891-2}.

\hypertarget{ref-kim_improved_2018}{}
34. \textbf{Kim M}, \textbf{Oh I}, \textbf{Ahn J}. 2018. An improved
method for prediction of cancer prognosis by network learning. Genes
\textbf{9}:478.
doi:\href{https://doi.org/10.3390/genes9100478}{10.3390/genes9100478}.

\hypertarget{ref-sze_leveraging_2018}{}
35. \textbf{Sze MA}, \textbf{Schloss PD}. 2018. Leveraging existing 16S
rRNA gene surveys to identify reproducible biomarkers in individuals
with colorectal tumors. mBio \textbf{9}:e00630--18.
doi:\href{https://doi.org/10.1128/mBio.00630-18}{10.1128/mBio.00630-18}.

\hypertarget{ref-schloss_introducing_2009}{}
36. \textbf{Schloss PD}, \textbf{Westcott SL}, \textbf{Ryabin T},
\textbf{Hall JR}, \textbf{Hartmann M}, \textbf{Hollister EB},
\textbf{Lesniewski RA}, \textbf{Oakley BB}, \textbf{Parks DH},
\textbf{Robinson CJ}, \textbf{Sahl JW}, \textbf{Stres B},
\textbf{Thallinger GG}, \textbf{Van Horn DJ}, \textbf{Weber CF}. 2009.
Introducing mothur: Open-Source, Platform-Independent,
Community-Supported Software for Describing and Comparing Microbial
Communities. ApplEnvironMicrobiol \textbf{75}:7537--7541.

\hypertarget{ref-westcott_opticlust_2017}{}
37. \textbf{Westcott SL}, \textbf{Schloss PD}. 2017. OptiClust, an
Improved Method for Assigning Amplicon-Based Sequence Data to
Operational Taxonomic Units. mSphere \textbf{2}.
doi:\href{https://doi.org/10.1128/mSphereDirect.00073-17}{10.1128/mSphereDirect.00073-17}.

\hypertarget{ref-rognes_vsearch_2016}{}
38. \textbf{Rognes T}, \textbf{Flouri T}, \textbf{Nichols B},
\textbf{Quince C}, \textbf{Mahé F}. 2016. VSEARCH: A versatile open
source tool for metagenomics. PeerJ \textbf{4}:e2584.
doi:\href{https://doi.org/10.7717/peerj.2584}{10.7717/peerj.2584}.

\hypertarget{ref-li_hyperband:_2016}{}
39. \textbf{Li L}, \textbf{Jamieson K}, \textbf{DeSalvo G},
\textbf{Rostamizadeh A}, \textbf{Talwalkar A}. 2016. Hyperband: A novel
bandit-based approach to hyperparameter optimization. arXiv:160306560
{[}cs, stat{]}.


\end{document}
