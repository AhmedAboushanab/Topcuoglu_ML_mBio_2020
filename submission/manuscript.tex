\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1.0in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Evaluation of machine learning methods for 16S rRNA gene data},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{\textbf{Evaluation of machine learning methods for 16S rRNA gene data}}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}

\usepackage{helvet} % Helvetica font
\renewcommand*\familydefault{\sfdefault} % Use the sans serif version of the font
\usepackage[T1]{fontenc}

\usepackage[none]{hyphenat}

\usepackage{setspace}
\doublespacing
\setlength{\parskip}{1em}

\usepackage{lineno}

\usepackage{pdfpages}
\floatplacement{figure}{H} % Keep the figure up top of the page

\begin{document}
\maketitle

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}

Begüm D. Topçuoğlu\({^1}\), Jenna Wiens\({^2}\), Patrick D.
Schloss\textsuperscript{1\(\dagger\)}

\vspace{40mm}

\(\dagger\) To whom correspondence should be addressed:
\href{mailto:pschloss@umich.edu}{\nolinkurl{pschloss@umich.edu}}

1. Department of Microbiology and Immunology, University of Michigan,
Ann Arbor, MI 48109

2. Department of Computer Science and Engineering, University or
Michigan, Ann Arbor, MI 49109

\newpage

\linenumbers

\subsection{Abstract}\label{abstract}

\newpage

\subsection{Introduction}\label{introduction}

Advances in sequencing technology and decreasing costs of generating 16S
rRNA gene sequences have allowed rapid exploration of human associated
microbiome and its health implications. Currently, the human microbiome
field is growing at an unprecedented rate and as a result, there is an
increasing demand for methods that identify associations between members
of the microbiome and human health. However, this is an undertaking as
human associated microbial communities are remarkably complex and
uneven. It is unlikely that a single species can explain a disease.
Instead, subsets of those communities, in relation to one another and to
their host, account for the differences in the health outcomes.
Therefore, researchers have started to explore the utility of machine
learning (ML) models that use microbiota associated biomarkers to
predict human health and to understand the microbial ecology of diseases
such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases
(IBD), obesity, and type 2 diabetes (1--11). ML methods are effective at
recognizing and highlighting patterns in complex microbial datasets.
However, currently the field's use of ML lacks clarity and consistency
on which methods are used and how these methods are implemented.
Moreover, there is a lack of deliberation on why a particular ML model
is utilized. Recently, there is a trend towards using more complex ML
models such as random forest, extreme gradient boosting and neural
networks without a discussion on if and how much model interpretibility
is necessary for the study (11--14). The lack of transparency on
methodology and modeling decisions can have negative consequences on
model reproducibility and reliability which we need avoid by (1)
implementing consistent, accessible and transparent machine learning
practices; (2) selecting ML models that reflect the goal of the study as
it will inform the expectations of model accuracy, complexity,
interpretibility and computational efficiency.

To showcase transparent ML methodologies and to shed light on how much
ML model selection can affect modeling results, we performed an
empirical analysis comparing several different ML models using the same
dataset. We used a previously published colorectal cancer (CRC) study
(3) which had fecal 16S rRNA gene sequences and human hemoglobin
concentrations from 490 patients. We built ML models using fecal 16S
rRNA gene sequences and human hemoglobin concentrations to predict
patients with normal or screen relevant neoplasias (SRN) disease status.
The study had 261 normal and 229 SRN samples. We established modeling
pipelines for L2-regularized logistic regression, L1 and L2 support
vector machines (SVM) with linear and radial basis function kernels, a
decision tree, random forest and XGBoost which increase in complexity
and decrease in interpretability respectively. Our ML pipeline performed
100 data-splits and utilized held-out test data to evaluate
generalization and predicton performance of each ML model. The mean
AUROC varied from 0.5 (std ± 0.01) to 0.82 (std ± 0.04). Random Forest
and XGBoost had the highest mean AUROC for detecting SRN. Despite the
simplicity, the L1-regularized linear kernel SVM followed Random Forest
and XGBoost in performance. In terms of computational efficiency, L2 SVM
with linear kernel trained the fastest (0.123 hours, std ± 0.014), while
XGBoost took the longest (106.568 hours, std ± 6.839). We found that
mean cross-validation and testing AUROC could vary by as much as 0.238,
which highlights the importance of a seperate held-out test set for
evaluation. Aside from evaluating generalization and classification
performances for each of these models, this study established standards
for modeling pipelines of microbiome-associated machine learning models.

\subsection{Results}\label{results}

\textbf{Model selection and construction}

We established modeling pipelines for binary classification with
L2-regularized logistic regression, L1 and L2 support vector machines
(SVM) with linear and radial basis function kernels, a decision tree,
random forest and XGBoost to emphasize the differences in model
accuracy, complexity, interpretibility, computational efficiency and
scalability due to model selection. We randomly split the data into
stratified training and test sets 100 times with a 80/20 split {[}Figure
1{]}. The training set consisted of 393 patients (209 SRN), while the
test set was composed of 97 patients (52 SRN). The training data was
used for training purposes and validation of parameter selection, and
the test set was used for evaluation purposes. Validation of parameter
selection was repeated 100 times to get a robust parameter selection for
each model {[}Figure 1{]}.

\textbf{The prediction and generalization performance of classifiers
during cross-validation and when applied to the held-out test data.}

We evaluated the prediction performance of seven binary classification
models when applied to held-out test data over 100 data-splits using
AUROC as classification performance metric. Random Forest and XGBoost
had the highest mean AUROC for detecting SRN, 0.816 (std ± 0.039) and
0.814 (std ± 0.04) respectively {[}Figure 2{]}. L1 linear SVM and
decision tree had significantly lower AUROC values, 0.503 (std ± 0.027)
and 0.741 (std ± 0.038) {[}Figure 2{]}. However, they had significanly
higher performances than L2 linear SVM, RBF SVM and L2 logistic
regression which had mean AUROC values of 0.501 (std ± 0.013), 0.68 (std
± 0.05) and 0.672 (std ± 0.05) respectively {[}Figure 2{]}. We also
evaluated the generalization performance of each classifier by comparing
their mean cross-validation AUROC and mean testing AUROC. We found a
statistically significant difference for classifiers L2 support vector
machines (SVM) with linear and radial basis function kernels and L2
logistic regression (p-values = 2.4e-37, 1.3e-14 and 2.6e-21
respectively). These differences were 0.222, 0.05 and 0.064,
respectively {[}Figure 2{]} .

\textbf{The complexity and interpretibility of each classifier.}

We interpreted the feature weights of L1 and L2 SVM with linear kernel
and regression coefficients of L2 logistic regression using the training
data. To get a sense of the importance of each feature we randomly
subsampled 80\% of the data 100 times and trained 100 models; this
resulted in 100 different sets of feature weights. We reported the mean
weights of the 10 most important features {[}Figure 4{]}.

\textbf{The computational efficiency of each classifier.}

Linear models trained faster than non-linear models. L2 logistic
Rregression and L1 and L2 SVM with linear kernel had training times of
7.2 min, (std ± 0.6), 10.2 min, (std ± 0.6) and 13.2 min, (std ± 1.8)
respectively. Whereas, SVM with radial basis function kernel, decision
tree, random forest and xgboost had training times of 2.01 hrs, (std ±
0.28), 4.2 hrs, (std ± 0.49), 106.57 hrs, (std ± 6.84) and 116.83 hrs,
(std ± 17.24), respectively.

\subsection{Conclusions}\label{conclusions}

\subsection{Materials and Methods}\label{materials-and-methods}

\paragraph{Data collection}\label{data-collection}

The data used for this analysis are stool bacterial abundances, stool
hemoglobin levels and clinical information of the patients recruited by
Great Lakes-New England Early Detection Research Network study. These
data were obtained from Sze et al (15). The stool samples were provided
by recruited adult participants who were undergoing scheduled screening
or surveillance colonoscopy. Colonoscopies were performed and fecal
samples were collected from participants in four locations: Toronto (ON,
Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA).
Patients' colonic disease status was defined by colonoscopy with
adequate preparation and tissue histopathology of all resected lesions.
Patients with an adenoma greater than 1 cm, more than three adenomas of
any size, or an adenoma with villous histology were classified as
advanced adenoma. Study had 172 patients with normal colonoscopies, 198
with adenomas and 120 with carcinomas. Of the 198 adenomas, 109 were
identified as advanced adenomas. Stool provided by the patients was used
for Fecal Immunological Tests (FIT) which measure human hemoglobin
concentrations and for 16S rRNA gene sequencing to measure bacterial
population abundances. The bacterial abundance data was generated by Sze
et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the
default quality filtering methods, identifying and removing chimeric
sequences using VSEARCH and assigning to OTUs at 97\% similarity using
the OptiClust algorithm (16--18).

\paragraph{Data definitions and
pre-processing}\label{data-definitions-and-pre-processing}

The colonic disease status is re-defined as two encompassing classes;
Normal or Screen Relevant Neoplasias (SRNs). Normal class includes
patients with non-advanced adenomas or normal colons whereas SRN class
includes patients with advanced adenomas or carcinomas. Colonic disease
status is the label predicted with each classifier. The bacterial
abundances and FIT results are the features used to predict colonic
disease status. Bacterial abundances are discrete data in the form of
Operational Taxonomic Unit (OTU) counts. There are 6920 OTUs for each
sample. FIT levels are continuous data present for each sample. Because
the data are in different scales, features are transformd by scaling
each feature to a {[}0-1{]} range (Table 1).

\paragraph{Learning the Classifier}\label{learning-the-classifier}

To train and validate our model, labeled data is randomly split 80/20
into a training set and testing set. Then, seven binary class
classifiers, L2 logistic regression, L1 and L2 linear suppor vector
machines (SVM), radial basis function SVM, decision tree, random forest
and XGBoost, are learned. The training set is used for training purposes
and validation of hyperparameter selection, and the test set is used for
evaluation purposes. Hyperparameters are selected using 5-fold
cross-validation with 100-repeats on the training set. Since the colonic
disease status are not uniformly represented in the data, 5-fold splits
are stratified to maintain the overall label distribution on the
training set.

\paragraph{Classifier Performance}\label{classifier-performance}

The classification performance of learned classifier is evaluated on the
labeled held-out testing set. The optimal classifier with optimal
hyperparameters selected in the cross-validation step is used to produce
a prediction for the testing set. The performance of this prediction is
measured in terms of the sensitivity and specificity, in addition to
Area Under the Curve (AUC) metrics. This process of splitting the data,
learning a classifier with cross-validation, and testing the classifier
is repeated on 100 different splits. In the end cross-validation AUC and
testing AUC averaged over the 100 different training/test splits are
reported. Hyperparameter budget and performance for each split is also
reported.

\newpage

\textbf{Figure 1. Generalization and classification performance of
modeling methods } AUC values of all cross validation and testing
performances. The boxplot shows quartiles at the box ends and the
statistical median as the horizontal line in the box. The whiskers show
the farthest points that are not outliers. Outliers are data points that
are not within 3/2 times the interquartile ranges.

\newpage

\subsection{References}\label{references}

\hypertarget{refs}{}
\hypertarget{ref-zeller_potential_2014}{}
1. \textbf{Zeller G}, \textbf{Tap J}, \textbf{Voigt AY},
\textbf{Sunagawa S}, \textbf{Kultima JR}, \textbf{Costea PI},
\textbf{Amiot A}, \textbf{Böhm J}, \textbf{Brunetti F},
\textbf{Habermann N}, \textbf{Hercog R}, \textbf{Koch M},
\textbf{Luciani A}, \textbf{Mende DR}, \textbf{Schneider MA},
\textbf{Schrotz-King P}, \textbf{Tournigand C}, \textbf{Tran Van Nhieu
J}, \textbf{Yamada T}, \textbf{Zimmermann J}, \textbf{Benes V},
\textbf{Kloor M}, \textbf{Ulrich CM}, \textbf{Knebel Doeberitz M von},
\textbf{Sobhani I}, \textbf{Bork P}. 2014. Potential of fecal microbiota
for early-stage detection of colorectal cancer. Mol Syst Biol
\textbf{10}.
doi:\href{https://doi.org/10.15252/msb.20145645}{10.15252/msb.20145645}.

\hypertarget{ref-zackular_human_2014}{}
2. \textbf{Zackular JP}, \textbf{Rogers MAM}, \textbf{Ruffin MT},
\textbf{Schloss PD}. 2014. The human gut microbiome as a screening tool
for colorectal cancer. Cancer Prev Res \textbf{7}:1112--1121.
doi:\href{https://doi.org/10.1158/1940-6207.CAPR-14-0129}{10.1158/1940-6207.CAPR-14-0129}.

\hypertarget{ref-baxter_dna_2016}{}
3. \textbf{Baxter NT}, \textbf{Koumpouras CC}, \textbf{Rogers MAM},
\textbf{Ruffin MT}, \textbf{Schloss PD}. 2016. DNA from fecal
immunochemical test can replace stool for detection of colonic lesions
using a microbiota-based model. Microbiome \textbf{4}.
doi:\href{https://doi.org/10.1186/s40168-016-0205-y}{10.1186/s40168-016-0205-y}.

\hypertarget{ref-baxter_microbiota-based_2016}{}
4. \textbf{Baxter NT}, \textbf{Ruffin MT}, \textbf{Rogers MAM},
\textbf{Schloss PD}. 2016. Microbiota-based model improves the
sensitivity of fecal immunochemical test for detecting colonic lesions.
Genome Medicine \textbf{8}:37.
doi:\href{https://doi.org/10.1186/s13073-016-0290-3}{10.1186/s13073-016-0290-3}.

\hypertarget{ref-hale_shifts_2017}{}
5. \textbf{Hale VL}, \textbf{Chen J}, \textbf{Johnson S},
\textbf{Harrington SC}, \textbf{Yab TC}, \textbf{Smyrk TC},
\textbf{Nelson H}, \textbf{Boardman LA}, \textbf{Druliner BR},
\textbf{Levin TR}, \textbf{Rex DK}, \textbf{Ahnen DJ}, \textbf{Lance P},
\textbf{Ahlquist DA}, \textbf{Chia N}. 2017. Shifts in the fecal
microbiota associated with adenomatous polyps. Cancer Epidemiol
Biomarkers Prev \textbf{26}:85--94.
doi:\href{https://doi.org/10.1158/1055-9965.EPI-16-0337}{10.1158/1055-9965.EPI-16-0337}.

\hypertarget{ref-pasolli_machine_2016}{}
6. \textbf{Pasolli E}, \textbf{Truong DT}, \textbf{Malik F},
\textbf{Waldron L}, \textbf{Segata N}. 2016. Machine learning
meta-analysis of large metagenomic datasets: Tools and biological
insights. PLoS Comput Biol \textbf{12}.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004977}{10.1371/journal.pcbi.1004977}.

\hypertarget{ref-sze_looking_2016}{}
7. \textbf{Sze MA}, \textbf{Schloss PD}. 2016. Looking for a signal in
the noise: Revisiting obesity and the microbiome. mBio \textbf{7}.
doi:\href{https://doi.org/10.1128/mBio.01018-16}{10.1128/mBio.01018-16}.

\hypertarget{ref-walters_meta-analyses_2014}{}
8. \textbf{Walters WA}, \textbf{Xu Z}, \textbf{Knight R}. 2014.
Meta-analyses of human gut microbes associated with obesity and IBD.
FEBS Lett \textbf{588}:4223--4233.
doi:\href{https://doi.org/10.1016/j.febslet.2014.09.039}{10.1016/j.febslet.2014.09.039}.

\hypertarget{ref-vazquez-baeza_guiding_2018}{}
9. \textbf{Vázquez-Baeza Y}, \textbf{Gonzalez A}, \textbf{Xu ZZ},
\textbf{Washburne A}, \textbf{Herfarth HH}, \textbf{Sartor RB},
\textbf{Knight R}. 2018. Guiding longitudinal sampling in IBD cohorts.
Gut \textbf{67}:1743--1745.
doi:\href{https://doi.org/10.1136/gutjnl-2017-315352}{10.1136/gutjnl-2017-315352}.

\hypertarget{ref-qin_alterations_2014}{}
10. \textbf{Qin N}, \textbf{Yang F}, \textbf{Li A}, \textbf{Prifti E},
\textbf{Chen Y}, \textbf{Shao L}, \textbf{Guo J}, \textbf{Le Chatelier
E}, \textbf{Yao J}, \textbf{Wu L}, \textbf{Zhou J}, \textbf{Ni S},
\textbf{Liu L}, \textbf{Pons N}, \textbf{Batto JM}, \textbf{Kennedy SP},
\textbf{Leonard P}, \textbf{Yuan C}, \textbf{Ding W}, \textbf{Chen Y},
\textbf{Hu X}, \textbf{Zheng B}, \textbf{Qian G}, \textbf{Xu W},
\textbf{Ehrlich SD}, \textbf{Zheng S}, \textbf{Li L}. 2014. Alterations
of the human gut microbiome in liver cirrhosis. Nature
\textbf{513}:59--64.
doi:\href{https://doi.org/10.1038/nature13568}{10.1038/nature13568}.

\hypertarget{ref-geman_deep_2018}{}
11. \textbf{Geman O}, \textbf{Chiuchisan I}, \textbf{Covasa M},
\textbf{Doloc C}, \textbf{Milici M-R}, \textbf{Milici L-D}. 2018. Deep
learning tools for human microbiome big data, pp. 265--275. \emph{In}
Balas, VE, Jain, LC, Balas, MM (eds.), Soft computing applications.
Springer International Publishing.

\hypertarget{ref-galkin_human_2018}{}
12. \textbf{Galkin F}, \textbf{Aliper A}, \textbf{Putin E},
\textbf{Kuznetsov I}, \textbf{Gladyshev VN}, \textbf{Zhavoronkov A}.
2018. Human microbiome aging clocks based on deep learning and tandem of
permutation feature importance and accumulated local effects. bioRxiv.
doi:\href{https://doi.org/10.1101/507780}{10.1101/507780}.

\hypertarget{ref-reiman_using_2017}{}
13. \textbf{Reiman D}, \textbf{Metwally A}, \textbf{Dai Y}. 2017. Using
convolutional neural networks to explore the microbiome, pp. 4269--4272.
\emph{In} 2017 39th annual international conference of the IEEE
engineering in medicine and biology society (EMBC).

\hypertarget{ref-fioravanti_phylogenetic_2017}{}
14. \textbf{Fioravanti D}, \textbf{Giarratano Y}, \textbf{Maggio V},
\textbf{Agostinelli C}, \textbf{Chierici M}, \textbf{Jurman G},
\textbf{Furlanello C}. 2017. Phylogenetic convolutional neural networks
in metagenomics. arXiv:170902268 {[}cs, q-bio{]}.

\hypertarget{ref-sze_leveraging_2018}{}
15. \textbf{Sze MA}, \textbf{Schloss PD}. 2018. Leveraging existing 16S
rRNA gene surveys to identify reproducible biomarkers in individuals
with colorectal tumors. mBio \textbf{9}:e00630--18.
doi:\href{https://doi.org/10.1128/mBio.00630-18}{10.1128/mBio.00630-18}.

\hypertarget{ref-schloss_introducing_2009}{}
16. \textbf{Schloss PD}, \textbf{Westcott SL}, \textbf{Ryabin T},
\textbf{Hall JR}, \textbf{Hartmann M}, \textbf{Hollister EB},
\textbf{Lesniewski RA}, \textbf{Oakley BB}, \textbf{Parks DH},
\textbf{Robinson CJ}, \textbf{Sahl JW}, \textbf{Stres B},
\textbf{Thallinger GG}, \textbf{Van Horn DJ}, \textbf{Weber CF}. 2009.
Introducing mothur: Open-Source, Platform-Independent,
Community-Supported Software for Describing and Comparing Microbial
Communities. ApplEnvironMicrobiol \textbf{75}:7537--7541.

\hypertarget{ref-westcott_opticlust_2017}{}
17. \textbf{Westcott SL}, \textbf{Schloss PD}. 2017. OptiClust, an
Improved Method for Assigning Amplicon-Based Sequence Data to
Operational Taxonomic Units. mSphere \textbf{2}.
doi:\href{https://doi.org/10.1128/mSphereDirect.00073-17}{10.1128/mSphereDirect.00073-17}.

\hypertarget{ref-rognes_vsearch_2016}{}
18. \textbf{Rognes T}, \textbf{Flouri T}, \textbf{Nichols B},
\textbf{Quince C}, \textbf{Mahé F}. 2016. VSEARCH: A versatile open
source tool for metagenomics. PeerJ \textbf{4}:e2584.
doi:\href{https://doi.org/10.7717/peerj.2584}{10.7717/peerj.2584}.


\end{document}
