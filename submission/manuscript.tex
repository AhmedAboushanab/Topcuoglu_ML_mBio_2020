\documentclass[11pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Effective application of machine learning to bacterial 16S rRNA gene sequencing data},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{\textbf{Effective application of machine learning to bacterial 16S rRNA
gene sequencing data}}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{}
    \preauthor{}\postauthor{}
    \date{}
    \predate{}\postdate{}
  
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage[table]{xcolor}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{caption}
\usepackage{hyperref}

\usepackage{helvet} % Helvetica font
\renewcommand*\familydefault{\sfdefault} % Use the sans serif version of the font
\usepackage[T1]{fontenc}
\usepackage[labelfont=bf]{caption}

\usepackage[none]{hyphenat}

\usepackage{setspace}
\doublespacing
\setlength{\parskip}{1em}

\usepackage{lineno}

\usepackage{pdfpages}
\floatplacement{figure}{H} % Keep the figure up top of the page

\begin{document}
\maketitle

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}

Begüm D. Topçuoğlu\({^1}\), Nicholas A. Lesniak\({^1}\), Mack
Ruffin\({^3}\), Jenna Wiens\({^2}\), Patrick D.
Schloss\textsuperscript{1\(\dagger\)}

\vspace{40mm}

\(\dagger\) To whom correspondence should be addressed:
\href{mailto:pschloss@umich.edu}{\nolinkurl{pschloss@umich.edu}}

1. Department of Microbiology and Immunology, University of Michigan,
Ann Arbor, MI 48109

2. Department of Computer Science and Engineering, University or
Michigan, Ann Arbor, MI 49109

3. Department of Family Medicine and Community Medicine, Penn State
Hershey Medical Center, Hershey, PA

\newpage

\linenumbers

\subsection{Abstract}\label{abstract}

Machine learning (ML) modeling of the human microbiome has the potential
to identify the microbial biomarkers and aid in diagnosis of many
chronic diseases such as inflammatory bowel disease, diabetes, and
colorectal cancer. Progress has been made towards developing ML models
that predict health outcomes from bacterial abundances, but inconsistent
adoption of methods call the validity of these models into question.
Furthermore, there appears to be a preference by many researchers to
favor increased model complexity over interpretability. To overcome
these challenges, we trained seven models that used fecal 16S rRNA
sequences to predict the presence of colonic screen relevant neoplasias
(SRNs; n=490 patients, 261 controls and 229 cases). We developed a
generalizable pipeline to train, validate and interpret the models. To
show the effect of model selection, we assessed the predictive
performance, interpretability, and computational efficiency of the
following models: L2-regularized logistic regression, L1 and
L2-regularized support vector machines (SVM) with linear and radial
basis function kernels, a decision tree, random forest, and extreme
gradient boosting (XGBoost). The random forest model was best at
detecting SRNs with an AUROC of 0.695 but it was slow to train (83.2 h)
and hard to interpret. Despite its simplicity, L2-regularized logistic
regression followed random forest in predictive performance with an
AUROC of 0.680, it trained much faster (12 min) and was more
interpretable. Our analysis showed that ML models should be chosen based
on the goal of the study as it will inform our expectations of
predictive performance and interpretability.

\newpage

\subsection{Importance}\label{importance}

Prediction of health outcomes using machine learning (ML) is rapidly
being adopted in human microbiome studies. However, these ML models are
likely overoptimistic in terms of quantifying predictive performance.
Moreover, there is a trend towards using black box models such as random
forest and neural networks without a discussion of the difficulty of
interpreting such models when trying to identify microbial biomarkers of
disease. This work represents a step towards developing better ML
practices in microbiome research by implementing a rigorous pipeline and
emphasizing the importance of selecting ML models that reflect the goal
of the study. These concepts are not particular to study of health
outcomes but can also be applied to environmental microbiology studies
as well.

\newpage

\subsection{Background}\label{background}

As the number of people represented in human microbiome datasets grow,
there is an increasing desire to use microbiome data to diagnose
diseases. However, the structure of the human microbiome is remarkably
variable between individuals to the point where it is often difficult to
identify the bacterial populations that are associated with diseases
using traditional statistical models. This variation is likely due to
the ability of many bacterial populations to fill the same niche such
that different populations cause the same disease in different
individuals. Furthermore, a growing number of studies have shown that it
is rare for a single bacterial species to be associated with a disease.
Instead, subsets of the microbiome account for differences in health.
Traditional statistical approaches do not adequately account for the
variation in the human microbiome and typically consider the protective
or risk effects of each bacterial population individually. Recently,
machine learning models have grown in popularity among microbiome
researchers because of the large amount of data that can now be
generated and because the models are effective at accounting for the
interpersonal microbiome variation and the ecology of the disease.

ML models are useful for understanding the variation in the structure of
existing data and to apply that knowledge to make predictions about new
data. Researchers have used ML models to diagnose and understand the
ecological basis of diseases such as liver cirrhosis, colorectal cancer,
inflammatory bowel diseases (IBD), obesity, and type 2 diabetes (1--16,
16--18). The task of diagnosing an individual with high confidence
relies on a model that is built with rigorous methods. However, there
are common methodological problems across many of these studies that
need to be addressed as the field progresses. These include a lack of
transparency in which methods are used and how these methods are
implemented; developing and evaluating models without a separate
held-out test data; large variation between the predictive performance
on different folds of cross-validation; and large variation between
cross-validation and testing performances. Nevertheless, the microbiome
field is making progress to avoid some of these pitfalls including
validating their models on independent datasets (7, 18, 19) and
introducing analysis frameworks to better use ML tools (20--23). More
work is needed to further improve reproducibility and minimize
over-optimism for model performance.

Among microbiome researchers, the lack of transparency in justifying a
modelling approach has been due to an implicit assumption that more
complex models are better because they are more complex. This has
resulted in a trend towards using models such as random forest and
neural networks (2, 11, 24--26) over simpler models such as logistic
regression or other linear models (18, 22, 27). Although the more
complex models may be better at incorporating non-linear relationships
or yield better predictions, they are considered to be black box models
because they are not inherently interpretable. These models require post
hoc explanations to quantify the importance of each feature in making a
prediction and they do not show the structure of how the features are
used. Depending on the application of the model, researchers may choose
to use different modeling approaches. For example, researchers trying to
identify the populations causing a disease would likely want a more
interpretable model whereas clinicians may emphasize predictive
performance. Although one may feel that they are sacrificing
interpretability for performance, that tradeoff may be minimal (28, 29).
Regardless, it is important for researchers to articulate why they have
selected a specific modelling approach or even compare multiple
approaches in the same study.

To showcase a rigorous ML pipeline and to shed light on how ML model
selection can affect modeling results, we performed an empirical
analysis comparing 7 modeling approaches with the same dataset and
pipeline. We built three linear models with different forms of
regularization: L2-regularized logistic regression and L1 and
L2-regularized support vector machines (SVM) with a linear kernel. We
also built four non-linear models: SVM with radial basis function
kernel, a decision tree, random forest and XGBoost. We compared their
predictive performance, interpretability, and computational efficiency.
To demonstrate the performance of these modeling approaches and our
pipeline, we used data from a previously published study that sought to
classifiy individuals as having normal colons or colonic lesions based
on the 16S rRNA gene sequences collected from fecal samples (3). This
dataset was selected because it is a relatively large collection of
individuals (N=490) connected to a clinically significant disease where
there is ample evidence that the disease is driven by variation in the
microbiome (1, 3, 4, 30). With this dataset we developed a framework
that implements a ML pipeline that can be used for any modeling
approach, evaluates predictive performance, and demonstrates how to
interpret these models. This framework can be easily applied to other
host-associated and environmental microbiome datasets.

\subsection{Results}\label{results}

\textbf{Model selection and pipeline construction} We established a ML
pipeline where we trained and validated each of the seven models using a
common approach that is based on standard methods within the ML
community (REFS by Jenna?){[}Figure 1{]}.

First, we randomly split the data into training and test sets so that
the training set consisted of 80\% of the full dataset while the test
set was composed of the remaining 20\% of the data {[}Figure 1{]}. To
maintain the distribution of controls and cases that was found with the
full dataset, we performed stratified splits. For example, our full
dataset included 490 individuals. Of these, 261 had normal colons (53\%)
and 229 had a screen relevant neoplasia (SRN; 46.7\%). A training set
included 393 individuals, of which 209 had an SRN (53\%), while the test
set was composed of 97 individuals of which 52 had an SRN (54\%). The
training data was used to build the models and the test set was used for
evaluating predictive performance.

Second, we trained seven different models using the training data
{[}Table 1{]}. We selected models with different classification
algorithms and regularization methods. Regularization is a technique
that discourages overfitting by penalizing the model for learning the
training data too well. For regularized logistic regression and SVM with
linear kernel, we used L2 regularization to keep all potentially
important features. For comparison, we also trained an L1-regularized
SVM model with linear kernel. L1-regularization on microbiome data led
to a sparser solution (i.e., force many coefficients to zero). To
explore the potential for non-linear relationships among features to
improve classification, we trained tree-based models including decision
tree, random forest, and XGBoost and we trained an SVM model with
non-linear kernel.

Third, fitting of these models require selecting appropriate
hyperparameters. Hyperparameters are the rules that are learned from the
training set in a classification algorithm. For example, in the linear
models the regularization term (C) is a hyperparameter that indicates
the penalty for overfitting. Similar to regularization term C, all
hyperparameters are tuned to find the best model. We selected
hyperparameters by performing 100 five-fold cross-validation (CV)
repeats on the training set {[}Figure 1{]}. The five-fold CV was also
stratified to maintain the overall case and control distribution. We
chose the best hyperparameter values for each model based on its CV
predictive performance using the area under the receiver operating
characteristic curve (AUROC) metric {[}Figure S1 and S2{]}. The AUROC
ranges from 1.0, where the model perfectly distinguishes between cases
and controls, to 0.50, where the model's predictions are no different
from random chance. To select the best performing hyperparameter, we
performed a full grid search for hyperparameter settings when training
our models. Default hyperparameter settings in previously developed ML
packages in R, Python, and Matlab programming languages are inadequate
for effective application of classification algorithms and need to be
optimized for each new ML task. In the example of L1-regularized SVM
with linear kernel {[}Figure S1{]}, the model showed large variability
between different regularization coefficients (C) and was susceptible to
performing poorly if the wrong regularization coefficient was assigned
to the model by default.

Finally, we trained the full training dataset with the selected
hyperparameter values and applied the model to the held-out data to
evaluate the testing predictive performance of each model. The
data-split, hyperparameter selection, training and testing steps were
repeated 100 times to get a reliable and robust reading of model
performance {[}Figure 1{]}.

\textbf{Predictive performance and generalizability of the seven
models.} We evaluated the predictive performance of seven models to
classify individuals as having normal colons or SRNs {[}Figure 2{]}. The
random forest model had significantly higher test AUROC values than the
other models for detecting SRNs (Wilcoxon rank sum test, p \textless{}
0.01). The median AUROC of the random forest model was 0.695 (IQR
0.044). L2-regularized logistic regression, XGBoost, L2-regularized SVM
with linear and radial basis function kernel AUROC values were not
significantly different from one another and had median AUROC values of
0.68 (IQR 0.055), 0.679 (IQR 0.052), 0.678 (IQR 0.056) and 0.668 (IQR
0.056), respectively. L1-regularized SVM with linear kernel and decision
tree had significantly lower AUROC values than the other ML models with
median AUROC of 0.65 (IQR 0.066) and 0.601 (IQR 0.059), respectively
{[}Figure 2{]}. Interestingly, these results demonstrate that the most
complex model (XGBoost) did not have the best performance and that the
most interpretable models (L2-regularized logistic regression and SVM
with linear kernel) performed nearly as well as random forest.

To evaluate the generalizability of each model, we compared the median
cross-validation AUROC to the median testing AUROC. If the difference
between the cross-validation and testing AUROCs was large, then that
would indicate that the models were overfit. The difference in median
AUROCs was 0.021 in L1-regularized SVM with linear kernel, followed by
SVM with radial basis function kernel and decision tree with a
difference of 0.007 and 0.006, respectively {[}Figure 2{]}; however,
these differences are relatively small and would not indicate a problem
with overfitting.

To evaluate the risk for over-optimism of each model, we calculated the
range of AUROC values for each model using 100 data-splits. The range
among the testing AUROC values within each model varied by 0.23 on
average across the seven models. If we had only done a single split,
then there is a risk that we could gotten lucky or unlucky with the
performance of the model. For instance, the lowest AUROC value of the
random forest model was 0.593 whereas the highest was 0.81. These
results showed that depending on the data-split, the testing AUROC
values showed great variability {[}Figure 2{]}. Therefore, it is
important to employ the hierarchical data splits that were included in
our pipeline to minimize the risk of over-optimism.

To show the effect of sample size on model generalizability, we compared
cross-validation AUROC values of L2-regularized logistic regression and
random forest models when we subsetted our original study design with
490 subjects to 15, 30, 60, 120, and 245 subjects {[}Figure S3{]}. The
range among the cross-validation AUROC values within both models at
lower sample sizes were much larger than when the full collection of
samples was used to train and validate the models. These results showed
that because the microbiome data had many features (6920 OTUs), it was
important to train the models using appropriate sample sizes to avoid
problems with generalizability and over-optimism. Furthermore, it was
encouraging that even for a small number of samples, the interquartile
range included the median AUROC values for the larger subsetted
datasets.

\textbf{Interpretation of each ML model.} Interpretability is the degree
to which humans can understand the reasons behind a model prediction
(31). Because we often use ML models not just to predict a health
outcome but also to learn the ecology behind a disease, model
interpretation becomes crucial for microbiome studies. ML models
decrease in interpretability as they increase in complexity. In this
study we used two methods to help interpret our models.

First, we interpreted the feature importance of the linear models (L1
and L2-regularized SVM with linear kernel and L2-regularized logistic
regression) using the median rank of absolute feature weights for each
OTU {[}Figure 3{]}. We also reviewed the signs of feature weights to
determine whether an OTU is associated with classifying a subject as
being healthy or having an SRN - negative sign indicated being healthy
and positive sign indicated having an SRN. It was encouraging that many
of the highest ranked OTUs were shared across these three models,
(e.g.~OTU 50, 426, 609, 822, 1239). The benefit of this approach was
that the results of the analysis were based on the trained model
parameters and provided information regarding the sign and magnitude of
the impact of each OTU. However, this approach was only possible with
linear models.

Second, to analyze non-linear models we interpreted the feature
importance using permutation importance. Whereas the absolute feature
weights were determined from the trained models, here we measured
importance using the held-out test data. Permutation importance analysis
is a posthoc explanation of the model where we randomly permuted
non-correlated features individually and groups of perfectly correlated
features across the two groups in the held-out test data. We then
calculated how much the predictive performance of the model (i.e testing
AUROC values) decreased when each OTU or group of OTUs was randomly
permuted. We ranked the OTUs based on how much the median testing AUROC
decreased when it was permuted; the OTU with the largest decrease ranked
highest {[}Figure 4{]}. Among the twenty OTUs with the largest impact,
there was only one OTU (OTU 822) that was shared among all of the
models; however, we found three OTUs (OTU 58, 110, 367) that were
important in each of the tree-based models. Similarly, the random forest
and XGBoost models, shared four of the most important OTUs (OTU 2, 12,
361, 477). Permutation analysis results also revealed that with the
exception of the decision tree model, removal of any individual OTU had
a minimal impact on model performance. For example, if OTU 367 was
permuted across the samples in the decision tree model, the median AUROC
dropped from 0.601 to 0.525. In contrast, if the same OTU was permuted
in the random forest model, the AUROC only dropped from 0.695 to 0.68.
Effectively, the complexity of the communities was more fully
represented in the better performing models (22, 32). At least in this
case, it was not possible to distinguish between health and disease
using a single OTU. While permutation analysis allowed us to gauge the
importance of an OTU, the analysis was post-hoc (i.e.~done using the
test data) and these results did not allow us to directly interrogate
the models to know whether an OTU is associated with classifying a
subject as being healthy or having an SRN.

To further highlight the differences between the two interpretation
methods, we used permutation importance to interpret the linear models
{[}Figure S4{]}. When we analyzed the L1-regularized SVM with linear
kernel model using feature rankings based on weights {[}Figure 3{]} and
permutation importance {[}Figure S4{]}, 17 out of the 20 top OTUs
(e.g.~OTU 609, 822, 1239) were deemed important by both interpretation
methods. Similarly, for the L2-regularized SVM and L2-regularized
logistic regression, 9 and 12 OTUs, respectively, were shared among the
two interpretation methods. These results indicate that both methods are
consistent in selecting the most important OTUs.

\textbf{The computational efficiency of each ML model.} We compared the
training times of the seven ML models. As expected, the training times
increased with the complexity of the model and the number of tuned
hyperparameter settings. Also, the linear models trained faster than
non-linear models {[}Figures S1-S2; Figure 5{]}. When we subsetted the
size of the training dataset, we observed a linear relationship between
the size of the dataset and the training times for L2-regularized
logistic regression and random forest models {[}Figure S5{]}.

\subsection{Discussion}\label{discussion}

There is a growing awareness that many human diseases and environmental
processes are not driven by a single organism but are the product of
multiple bacterial populations. Traditional statistical approaches are
useful for identifying those cases where a single organism is associated
with a process. In contrast, ML methods offer the ability to incorporate
the structure of the microbial communities as a whole to classify them
into different categories such as coming from a patient who is healthy
or has SRNs. If it is possible to classify communities reliably, then ML
methods also offer the ability to identify those microbial populations
within the communities that are responsible for the classification.
However, the application of ML in microbiome studies is still in its
infancy and the field still needs to develop a better understanding of
different ML methods, their strengths and weaknesses, and how to
implement them.

To address these needs, we developed a framework to train rigorous,
transparent, and reproducible models. We benchmarked seven ML models and
showed that we can create models that are inherently interpretable and
easily trained without losing predictive performance. In terms of
predictive performance, the random forest model had the best testing
AUROC values compared to the other six models. However, the second-best
model was L2-regularized logistic regression with a median AUROC
difference of only 0.015 compared to random forest. While random forest
took 83.2 hours to train, L2-regularized logistic regression trained in
12 minutes. In terms of interpretability, random forest was a complex ML
model and could only be explained using post-hoc methods such as
permutation importance. On the other hand, L2-regularized logistic
regression was easier to interpret by ranking the OTUs based on their
feature weights and reviewing the signs of these weights. Comparing many
different models showed us that the most complex model is not
necessarily the best model for our ML task.

As we set out to select the best model, we established a pipeline that
can be generalized to any modeling method that uses 16S rRNA sequence
counts to predict a binary health outcome. We performed a random
datasplit to create a training set (80\% of the data) and a held-out
test set (20\% of the data), which we used to evaluate predictive
performance. We repeated this datasplit 100 times to measure predictive
performance. During the training, we tuned the model hyperparameters
with a repeated five-fold cross-validation. Despite the high number of
features microbiome datasets typically have, the models we built with
this pipeline were generalizable as shown by the similar AUROC values
from the cross-validation and testing.

We highlighted the importance of model interpretation to gain greater
biological insights into microbiota-associated diseases. In this study
we showcased two different interpretation methods: ranking each OTU by
(i) their absolute weights in the trained models and (ii) their impact
on the predictive performance based on permutation importance.
Human-associated microbial communities have complex correlation
structures which create collinearity in the datasets we work with. This
can hinder our ability to reliably interpret models because the feature
weights of correlated OTUs are influenced by one another (33). For
example if one OTU is highly correlated with another OTU, only one of
these OTUs would have a large feature weight thus hiding the importance
of its correlated OTU. To capture all important features, once we
identify highly ranked OTUs, we should review their relationships with
other OTUs. These relationships will help us generate new hypotheses
about the ecology of the disease and test them with follow-up
experiments. When we used permutation importance, we took collinearity
into consideration by grouping correlated OTUs to determine their impact
as a group. We grouped OTUs that had a perfect correlation with each
other however, we can reduce the correlation threshold to further
investigate the relationships among features. It is important to know
the correlation strutures of the data to avoid missinterpreting the
models. This is likely to be a particular problem with shotgun
metagenomic datasets where collinearity will be more pronunced due to
many genes being correlated with one another because they come from the
same chromosome. To identify the true underlying microbial factors of a
disease, it is crucial to do correlation analyses and further
experimentation for biological validation.

In this study, we did not consider all possible modeling approaches.
However, the principles highlighted throughout this study apply to all
ML modeling tasks with microbiome data. For example, we did not evaluate
multicategory classification methods to predict non-binary outcomes. For
example, we could have trained models to differentiate between people
with normal colons and those with adenomas or carcinomas (k=3
categories). We did not perform this analysis because the clinically
relevant diagnosis grouping was between patients with normal colons and
those with SRNs. Furthermore, as number of categories to classify
increase, more samples are required for each category to train a model.
We also did not use regression-based analyses to predict a
non-categorical outcome. We have previously used such an approach to
train random forest models to predict fecal short-chain fatty acid
concentrations based on microbiome data (34). Our analysis was also
limited to shallow learning methods and did not explore deep learning
methods such as neural networks. Deep learning methods hold great
promise (11, 35, 36) but microbiome datasets often suffer from having
many features and small sample sizes, which makes the deep learning
models prone to overfitting. These methods are even more complex than
random forest and XGBoost and are considered uninterpretable. There is
great potential for applying ML approaches to microbiome data and we can
choose which ML approach is best for the study.

Our framework gives structure to investigators wanting to train,
evaluate, and interpret their own ML models to identify OTUs that might
be biologically relevant. However, deploying microbiome-based models to
make clinical diagnoses or predictions is a significantly harder and
distinct undertaking. For example, we currently lack standardized
methods to collect patient samples, generate sequence data, and report
clinical data. We are also challenged by the practical constraints of
OTU-based approaches. The de novo algorithms commonly in use are slow,
require considerable memory, and result in different OTU assignments as
new data are added. Finally, we also need independent validation cohorts
to test the performance of a diagnostic model. To realize the potential
for using ML approaches with microbiome data, it is necessary that we
direct our efforts to overcome these challenges.

This study highlighted the need to make educated choices at every step
of developing a ML model with microbiome data. We created an
aspirational rubric that researchers can use to identify potential
pitfalls when using ML in microbiome studies and ways to avoid them
{[}Table S1{]}. We have highlighted the tradeoffs between model
complexity and interpretability, the need for cross-validation to tune
hyperparameters, the utility of held-out test sets for evaluating
predictive performance, and the importance of considering correlation
structures in datasets for reliable interpretation. Furthermore, we
underscored the importance of proper experimental design and methods to
help us achieve the level of validity and accountability we want from
models built for patient health.

\subsection{Materials and Methods}\label{materials-and-methods}

\textbf{Data collection and study population.} The original stool
samples described in our analysis were obtained from patients recruited
by Great Lakes-New England Early Detection Research Network (Reference
from Mack?). Stool samples were provided by adults who were undergoing a
scheduled screening or surveillance colonoscopy. Participants were
recruited from four locations: Toronto (ON, Canada), Boston (MA, USA),
Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic health was
visually assessed by colonoscopy with bowel preparation and tissue
histopathology of all resected lesions. We assigned patients into two
classes: those with normal colons and those with screen relevant
neoplasias (SRNs). The normal class included patients with normal colons
or non-advanced adenomas whereas the SRN class included patients with
advanced adenomas or carcinomas (reference for SRN?). Patients with an
adenoma greater than 1 cm, more than three adenomas of any size, or an
adenoma with villous histology were classified as having advanced
adenomas. The study had 172 patients with normal colonoscopies, 198 with
adenomas, and 120 with carcinomas. Of the 198 adenomas, 109 were
identified as advanced adenomas. The study collected samples from 261
patients classified as normal and 229 patients classified as having a
SRN.

\textbf{16S rRNA gene sequencing data.} Stool samples provided by the
patients was used for 16S rRNA gene sequencing to measure bacterial
population abundances. The sequence data used in our analyses were
originally generated by Baxter et al. (available through NCBI Sequence
Read Archive {[}SRP062005{]}, 2015). The OTU abundance table was
generated by Sze et al (32), who processed the 16S rRNA sequences in
mothur (v1.39.3) using the default quality filtering methods,
identifying and removing chimeric sequences using VSEARCH, and assigning
to OTUs at 97\% similarity using the OptiClust algorithm (37--39);
(\url{https://github.com/SchlossLab/Sze_CRCMetaAnalysis_mBio_2018/blob/master/data/process/baxter/baxter.0.03.subsample.shared}).
These OTU abundances were the features we used to predict colorectal
health of the patients. There were 6920 OTUs. OTU abundances were
subsampled to the size of the smallest sample and normalized across
samples such that the highest abundance of each OTU would be 1 and
lowest would be 0.

\textbf{Model training and evaluation.} Models were trained using the
caret package (v.6.0.81) in R (v.3.5.0). We modified the caret code to
calculate decision values instead of predicted probabilities for models
generated using L2-regularized SVM with linear kernel and L1-regularized
SVM with linear kernel. These changes were necessary to calculate AUROC
values for SVMs. The code for these changes on L2-regularized SVM with
linear kernel and L1-regularized SVM with linear kernel are available at
\url{https://github.com/SchlossLab/Topcuoglu_ML_XXX_2019/blob/master/data/caret_models/svmLinear3.R}
and at
\url{https://github.com/SchlossLab/Topcuoglu_ML_XXX_2019/blob/master/data/caret_models/svmLinear4.R},
respectively.

For hyperparameter selection, we started with a granular grid search.
Then we narrowed and fine-tuned the range of each hyperparameter. The
range of the grid changes with different ML tasks and ML models. A full
grid search was needed to avoid large variation in prediction
performance. For L2-regularized logistic regression, L1 and
L2-regularized SVM with linear and radial basis function kernels, we
tuned the \textbf{cost} hyperparameter which determines the
regularization strength where smaller values specify stronger
regularization. For SVM with radial basis function kernel we also tuned
\textbf{sigma} hyperparameter which determines the reach of a single
training instance where for a high value of sigma, the SVM decision
boundary will be dependent on the points that are closest to the
decision boundary. For the decision tree model, we tuned the
\textbf{depth of the tree} where deeper the tree, the more splits it
has. For random forest, we tuned the \textbf{number of features} to
consider when looking for the best tree split. For XGBoost, we tuned for
\textbf{learning rate} and the \textbf{fraction of samples} to be used
for fitting the individual base learners. There are newly developed
tools such as Hyperband that help researcher with hyperparameter
selection (40) that can be incorporated to microbiome studies.

The computational burden during model training due to model complexity
was reduced by parallelizing segments of the ML pipeline. We
parallelized the training of each data-split. This allowed 100
data-splits to be processed through the ML pipeline at the same time for
each model. It is possible to further parallelize the cross-validation
step for each hyperparameter setting if limited by computational
resources.

\textbf{Permutation importance workflow.} We created a Spearman's
rank-order correlation matrix and defined correlated OTUs as having
perfect correlation (correlation coefficient = 1 and p \textless{}
0.01). Non-correlated OTUs were permuted individually whereas correlated
ones were grouped together and permuted at the same time.

\textbf{Statistical analysis workflow.} Data summaries, statistical
analysis, and data visualizations were performed using R (v.3.5.0) with
the tidyverse package (v.1.2.1). We compared the AUROC values of the
seven ML models by Wilcoxon rank sum tests to determine the best
predictive performance.

\textbf{Code availability.} The code for all sequence curation and
analysis steps including an Rmarkdown version of this manuscript is
available at \url{https://github.com/SchlossLab/Topcuoglu_ML_XXX_2019/}.

\newpage

\includegraphics{Figure_1} \textbf{Figure 1. Machine learning pipeline.
} We split the data to create a training (80\%) and held-out test set
(20\%). The splits were stratified to maintain the overall label
distribution. We performed five-fold cross-validation on the training
data to select the best hyperparameter setting and then used these
hyperparameters to train the models. The model was evaluated on the
held-out data set. Abbreviations: cvAUC, cross-validation area under the
receiver operating characteristic curve. \newpage
\includegraphics{Figure_2.png}

\textbf{Figure 2. Generalization and classification performance of ML
models using AUROC values of all cross validation and testing
performances. } The median AUROC for diagnosing individuals with SRN
using bacterial abundances was higher than chance (depicted by
horizontal line at 0.50) for all the ML models. Predictive performance
of random forest model was higher than other ML models. The boxplot
shows quartiles at the box ends and the median as the horizontal line in
the box. The whiskers show the farthest points that were not outliers.
Outliers were defined as those data points that are not within 1.5 times
the interquartile ranges. \newpage
\includegraphics[height=17cm, width=12cm]{Figure_3.png}

\textbf{Figure 3. Interpretation of the linear ML models.} The absolute
feature weights of (A) L2-regularized logistic regression, (B)
L1-regularized SVM with linear kernel, and (C) L2-regularized SVM with
linear kernel were ranked from highest rank, 1, to lowest rank, 100, for
each data-split. The feature ranks of the 20 highest ranked OTUs based
on their median ranks (median shown in black) are reported here. OTUs
that are associated with classifying a subject as being healthy or
having an SRN are in blue and red, respectively. Some of the same OTUs
were identified as important in all of the linear models. \newpage
\includegraphics{Figure_4.png}

\textbf{Figure 4. Interpretation of the non-linear ML models.} (A) SVM
with radial basis kernel, (B) decision tree, (C) random forest, and (D)
XGBoost feature importances were explained using permutation importance
on the held-out test data set. The gray rectangle and the dashed line
show the IQR range and median of the base testing AUROC without any
permutation. The colors of the box plots represent the OTUs that were
shared among the different models; yellow were OTUs that were shared
among all the non-linear models, salmon were OTUs that were shared among
the tree-based models, green were the OTUs shared among SVM with radial
basis kernel, decision tree and XGBoost, pink were the OTUs shared among
SVM with radial basis kernel and XGBoost only, red were the OTUs shared
among random forest and XGBoost only and blue were the OTUs shared among
decision tree and random forest only. For all of the tree-based models,
a Peptostreptococcus species (OTU00367) had the largest impact on
predictive performance. \newpage
\includegraphics{Figure_5.png}

\textbf{Figure 5. Computational efficiency of seven ML models.} The
median training time was the highest for XGBoost and shortest for
L2-regularized logistic regression. \newpage
\includegraphics{Figure_S1.png}

\textbf{Figure S1. Hyperparameter setting performances for linear
models.} (A) L2 logistic regression, (B) L1 SVM with linear kernel, and
(C) L2 SVM with linear kernel mean cross-validation AUROC values when
different hyperparameters were used in training the model. The
differences in AUROC values when hyperparameters change show that
hyperparameter tuning is a crucial step in building a ML model.

\newpage

\includegraphics[height=30cm, width=15cm]{Figure_S2.png}

\textbf{Figure S2. Hyperparameter setting performances for non-linear
models.} (A) Decision tree, (B) Random forest, (C) SVM with radial basis
kernel, and (D) XGBoost mean cross-validation AUROC values when
different hyperparameters were used in training the model. The
differences in AUROC values when hyperparameters change show that
hyperparameter tuning is a crucial step in building a ML model. \newpage
\includegraphics[height=17.5cm, width=13cm]{Figure_S3.png}

\textbf{Figure S3. Classification performance of ML models across cross
validation when dataset is subsetted .} (A) L2-regularized logistic
regression and (B) Random forest models were trained using the original
study design with 490 subjects and subsets of it with 15, 30, 60, 120,
and 245 subjects. The range among the cross-validation AUROC values
within both models at lower sample sizes were much larger than when the
full collection of samples was used to train and validate the models,
but included the ranges observed with the more complete datasets.

\newpage

\includegraphics[height=17.5cm, width=13cm]{Figure_S4.png}

\textbf{Figure S4. Interpretation of the linear ML models with
permutation importance.} (A) L1-regularized SVM with linear kernel, (B)
L2-regularized SVM with linear kernel, and (C) L2-regularized logistic
regression were interpreted using permutation importance using held-out
test set.

\newpage

\includegraphics[height=17.5cm, width=13cm]{Figure_S5.png}

\textbf{Figure S5. Training times of ML models when dataset is subsetted
.} (A) L2-regularized logistic regression and (B) Random forest models
were trained using the original study design with 490 subjects and
subsets of it with 15, 30, 60, 120, and 245 subjects. As the size of the
dataset increased, the training times for L2-regularized logistic
regression and random forest models increased linearly.

\newpage

\captionof{table}{Characteristics of the machine learning models in our comparative study.}
\small

\begin{tabular}{|l|l|l|l|l|}
\hline

\rowcolor{lightgray}
\textbf{Model} & \textbf{Description} & \textbf{Linearity} & \textbf{Interpretability} & \textbf{Refs.} \\ \hline

\makecell[l]{Logistic \\regression} & \makecell[l]{A predictive regression analysis when the dependent \\variable is binary.} & Linear & Interpretable & $^{36}$\\ \hline

\makecell[l]{SVM with \\linear kernel} & \makecell[l]{A classifier that is defined by an optimal linear \\separating hyperplane that discriminates between labels.} & Linear & Interpretable & $^{37}$\\ \hline

\makecell[l]{SVM with \\radial basis kernel} & \makecell[l]{A classifier that is defined by an optimal Gaussian \\separating hyperplane that discriminates between labels.} & Non-linear & Explainable$^*$ & $^{38}$\\ \hline
Decision tree & \makecell[l]{A classifier that sorts samples down from the
root to the \\leaf node where an attribute is tested to discriminate \\between labels} & Non-linear & Interpretable & $^{39}$\\ \hline

Random forest & \makecell[l]{A classifier that is a decision tree ensemble that \\grow randomly with subsampled data.} & Non-linear & Explainable$^*$ & $^{40-41}$  \\ \hline

XGBoost & \makecell[l]{A classifier that is a decision tree ensemble that \\grow with additive training.} & Non-linear & Explainable$^*$ & $^{42-43}$ \\ \hline

\end{tabular}\begin{tablenotes}\footnotesize
\item[1]{$^*$Explainable models are not inherently interpretable but can be explained with post-hoc analyses.}
\end{tablenotes}\newpage

\captionof{table}{Table S1. An aspirational rubric for evaluating the rigor of ML practices.}
\small

\begin{tabular}{|l|l|l|l|}
\hline

\rowcolor{lightgray}
\textbf{Practice} & \textbf{Good} & \textbf{Better} & \textbf{Best} \\ \hline

\makecell[l]{Problem \\ definition} & \makecell[l]{Have we clearly stated \\ the ML task? \\ Do we have a priori hypotheses? \\Do we know the predictions \\ a domain expert would \\ make manually?} & \makecell[l]{Do we know the motivation \\ for solving the problem? \\ How much interpretability \\ does the problem need? } & \makecell[l]{Do we know our data? \\ Do we know the correlated \\ variables? } \\ \hline

\makecell[l]{Model \\ selection} & \makecell[l]{Do we know the \\ candidate algorithms \\ for the ML problem? \\} & \makecell[l]{Do we know our \\ computational resources \\ to fully train each model?} & \makecell[l]{How much interpretability \\ does the problem need? \\ How much each \\ candidate algorithm \\ can provide?} \\ \hline

\makecell[l]{ML pipeline \\ preparation} & \makecell[l]{Did we do \\ cross-validation} & \makecell[l]{Do we have an held-out \\ test dataset? \\ Have we tuned our model \\ hyperparameters in \\ cross-validation?} & \makecell[l]{Have we tested our model \\ on many different \\  datasets?} \\ \hline

\makecell[l]{Hyperparameter \\ selection} & \makecell[l]{Do we know the different \\ hyperparameters each model \\ can use and why?} & \makecell[l]{Did we use historically \\ effective hyperparameters? } & \makecell[l]{Did we search the \\ full grid space and \\optimized our model? } \\ \hline

\makecell[l]{Model \\ evaluation} & \makecell[l]{Have we chosen an appropriate \\ metric to evaluate \\ predictive performance?} & \makecell[l]{Have we reported \\ the predictive performance \\ on a held-out test data?} & \makecell[l]{Have we provided an average \\ predictive performance \\ of many model runs? } \\ \hline

\makecell[l]{Model \\ interpretation} & \makecell[l]{Do we know if \\ our model is \\ interpretable?} & \makecell[l]{If the model is not \\ interpretable, do we know \\how to  explain it? \\ Have we checked for \\ the effect of correlated \\ variables?} & \makecell[l]{Have we generated new \\ hypotheses based on \\ model interpretation \\ to test model results? } \\ \hline

\end{tabular}\newpage

\subsection{References}\label{references}

\hypertarget{refs}{}
\hypertarget{ref-zeller_potential_2014}{}
1. \textbf{Zeller G}, \textbf{Tap J}, \textbf{Voigt AY},
\textbf{Sunagawa S}, \textbf{Kultima JR}, \textbf{Costea PI},
\textbf{Amiot A}, \textbf{Böhm J}, \textbf{Brunetti F},
\textbf{Habermann N}, \textbf{Hercog R}, \textbf{Koch M},
\textbf{Luciani A}, \textbf{Mende DR}, \textbf{Schneider MA},
\textbf{Schrotz-King P}, \textbf{Tournigand C}, \textbf{Tran Van Nhieu
J}, \textbf{Yamada T}, \textbf{Zimmermann J}, \textbf{Benes V},
\textbf{Kloor M}, \textbf{Ulrich CM}, \textbf{Knebel Doeberitz M von},
\textbf{Sobhani I}, \textbf{Bork P}. 2014. Potential of fecal microbiota
for early-stage detection of colorectal cancer. Mol Syst Biol
\textbf{10}.
doi:\href{https://doi.org/10.15252/msb.20145645}{10.15252/msb.20145645}.

\hypertarget{ref-zackular_human_2014}{}
2. \textbf{Zackular JP}, \textbf{Rogers MAM}, \textbf{Ruffin MT},
\textbf{Schloss PD}. 2014. The human gut microbiome as a screening tool
for colorectal cancer. Cancer Prev Res \textbf{7}:1112--1121.
doi:\href{https://doi.org/10.1158/1940-6207.CAPR-14-0129}{10.1158/1940-6207.CAPR-14-0129}.

\hypertarget{ref-baxter_dna_2016}{}
3. \textbf{Baxter NT}, \textbf{Koumpouras CC}, \textbf{Rogers MAM},
\textbf{Ruffin MT}, \textbf{Schloss PD}. 2016. DNA from fecal
immunochemical test can replace stool for detection of colonic lesions
using a microbiota-based model. Microbiome \textbf{4}.
doi:\href{https://doi.org/10.1186/s40168-016-0205-y}{10.1186/s40168-016-0205-y}.

\hypertarget{ref-baxter_microbiota-based_2016}{}
4. \textbf{Baxter NT}, \textbf{Ruffin MT}, \textbf{Rogers MAM},
\textbf{Schloss PD}. 2016. Microbiota-based model improves the
sensitivity of fecal immunochemical test for detecting colonic lesions.
Genome Medicine \textbf{8}:37.
doi:\href{https://doi.org/10.1186/s13073-016-0290-3}{10.1186/s13073-016-0290-3}.

\hypertarget{ref-hale_shifts_2017}{}
5. \textbf{Hale VL}, \textbf{Chen J}, \textbf{Johnson S},
\textbf{Harrington SC}, \textbf{Yab TC}, \textbf{Smyrk TC},
\textbf{Nelson H}, \textbf{Boardman LA}, \textbf{Druliner BR},
\textbf{Levin TR}, \textbf{Rex DK}, \textbf{Ahnen DJ}, \textbf{Lance P},
\textbf{Ahlquist DA}, \textbf{Chia N}. 2017. Shifts in the fecal
microbiota associated with adenomatous polyps. Cancer Epidemiol
Biomarkers Prev \textbf{26}:85--94.
doi:\href{https://doi.org/10.1158/1055-9965.EPI-16-0337}{10.1158/1055-9965.EPI-16-0337}.

\hypertarget{ref-pasolli_machine_2016}{}
6. \textbf{Pasolli E}, \textbf{Truong DT}, \textbf{Malik F},
\textbf{Waldron L}, \textbf{Segata N}. 2016. Machine learning
meta-analysis of large metagenomic datasets: Tools and biological
insights. PLoS Comput Biol \textbf{12}.
doi:\href{https://doi.org/10.1371/journal.pcbi.1004977}{10.1371/journal.pcbi.1004977}.

\hypertarget{ref-sze_looking_2016}{}
7. \textbf{Sze MA}, \textbf{Schloss PD}. 2016. Looking for a signal in
the noise: Revisiting obesity and the microbiome. mBio \textbf{7}.
doi:\href{https://doi.org/10.1128/mBio.01018-16}{10.1128/mBio.01018-16}.

\hypertarget{ref-walters_meta-analyses_2014}{}
8. \textbf{Walters WA}, \textbf{Xu Z}, \textbf{Knight R}. 2014.
Meta-analyses of human gut microbes associated with obesity and IBD.
FEBS Lett \textbf{588}:4223--4233.
doi:\href{https://doi.org/10.1016/j.febslet.2014.09.039}{10.1016/j.febslet.2014.09.039}.

\hypertarget{ref-vazquez-baeza_guiding_2018}{}
9. \textbf{Vázquez-Baeza Y}, \textbf{Gonzalez A}, \textbf{Xu ZZ},
\textbf{Washburne A}, \textbf{Herfarth HH}, \textbf{Sartor RB},
\textbf{Knight R}. 2018. Guiding longitudinal sampling in IBD cohorts.
Gut \textbf{67}:1743--1745.
doi:\href{https://doi.org/10.1136/gutjnl-2017-315352}{10.1136/gutjnl-2017-315352}.

\hypertarget{ref-qin_alterations_2014}{}
10. \textbf{Qin N}, \textbf{Yang F}, \textbf{Li A}, \textbf{Prifti E},
\textbf{Chen Y}, \textbf{Shao L}, \textbf{Guo J}, \textbf{Le Chatelier
E}, \textbf{Yao J}, \textbf{Wu L}, \textbf{Zhou J}, \textbf{Ni S},
\textbf{Liu L}, \textbf{Pons N}, \textbf{Batto JM}, \textbf{Kennedy SP},
\textbf{Leonard P}, \textbf{Yuan C}, \textbf{Ding W}, \textbf{Chen Y},
\textbf{Hu X}, \textbf{Zheng B}, \textbf{Qian G}, \textbf{Xu W},
\textbf{Ehrlich SD}, \textbf{Zheng S}, \textbf{Li L}. 2014. Alterations
of the human gut microbiome in liver cirrhosis. Nature
\textbf{513}:59--64.
doi:\href{https://doi.org/10.1038/nature13568}{10.1038/nature13568}.

\hypertarget{ref-geman_deep_2018}{}
11. \textbf{Geman O}, \textbf{Chiuchisan I}, \textbf{Covasa M},
\textbf{Doloc C}, \textbf{Milici M-R}, \textbf{Milici L-D}. 2018. Deep
learning tools for human microbiome big data, pp. 265--275. \emph{In}
Balas, VE, Jain, LC, Balas, MM (eds.), Soft computing applications.
Springer International Publishing.

\hypertarget{ref-thaiss_persistent_2016}{}
12. \textbf{Thaiss CA}, \textbf{Itav S}, \textbf{Rothschild D},
\textbf{Meijer MT}, \textbf{Levy M}, \textbf{Moresi C},
\textbf{Dohnalová L}, \textbf{Braverman S}, \textbf{Rozin S},
\textbf{Malitsky S}, \textbf{Dori-Bachash M}, \textbf{Kuperman Y},
\textbf{Biton I}, \textbf{Gertler A}, \textbf{Harmelin A},
\textbf{Shapiro H}, \textbf{Halpern Z}, \textbf{Aharoni A},
\textbf{Segal E}, \textbf{Elinav E}. 2016. Persistent microbiome
alterations modulate the rate of post-dieting weight regain. Nature
\textbf{540}:544--551.
doi:\href{https://doi.org/10.1038/nature20796}{10.1038/nature20796}.

\hypertarget{ref-dadkhah_gut_2019}{}
13. \textbf{Dadkhah E}, \textbf{Sikaroodi M}, \textbf{Korman L},
\textbf{Hardi R}, \textbf{Baybick J}, \textbf{Hanzel D}, \textbf{Kuehn
G}, \textbf{Kuehn T}, \textbf{Gillevet PM}. 2019. Gut microbiome
identifies risk for colorectal polyps. BMJ Open Gastroenterology
\textbf{6}:e000297.
doi:\href{https://doi.org/10.1136/bmjgast-2019-000297}{10.1136/bmjgast-2019-000297}.

\hypertarget{ref-flemer_oral_2018}{}
14. \textbf{Flemer B}, \textbf{Warren RD}, \textbf{Barrett MP},
\textbf{Cisek K}, \textbf{Das A}, \textbf{Jeffery IB}, \textbf{Hurley
E}, \textbf{O`Riordain M}, \textbf{Shanahan F}, \textbf{O`Toole PW}.
2018. The oral microbiota in colorectal cancer is distinctive and
predictive. Gut \textbf{67}:1454--1463.
doi:\href{https://doi.org/10.1136/gutjnl-2017-314814}{10.1136/gutjnl-2017-314814}.

\hypertarget{ref-montassier_pretreatment_2016}{}
15. \textbf{Montassier E}, \textbf{Al-Ghalith GA}, \textbf{Ward T},
\textbf{Corvec S}, \textbf{Gastinne T}, \textbf{Potel G}, \textbf{Moreau
P}, \textbf{Cochetiere MF de la}, \textbf{Batard E}, \textbf{Knights D}.
2016. Pretreatment gut microbiome predicts chemotherapy-related
bloodstream infection. Genome Medicine \textbf{8}:49.
doi:\href{https://doi.org/10.1186/s13073-016-0301-4}{10.1186/s13073-016-0301-4}.

\hypertarget{ref-ai_systematic_2017}{}
16. \textbf{Ai L}, \textbf{Tian H}, \textbf{Chen Z}, \textbf{Chen H},
\textbf{Xu J}, \textbf{Fang J-Y}. 2017. Systematic evaluation of
supervised classifiers for fecal microbiota-based prediction of
colorectal cancer. Oncotarget \textbf{8}:9546--9556.
doi:\href{https://doi.org/10.18632/oncotarget.14488}{10.18632/oncotarget.14488}.

\hypertarget{ref-dai_multi-cohort_2018}{}
17. \textbf{Dai Z}, \textbf{Coker OO}, \textbf{Nakatsu G}, \textbf{Wu
WKK}, \textbf{Zhao L}, \textbf{Chen Z}, \textbf{Chan FKL},
\textbf{Kristiansen K}, \textbf{Sung JJY}, \textbf{Wong SH}, \textbf{Yu
J}. 2018. Multi-cohort analysis of colorectal cancer metagenome
identified altered bacteria across populations and universal bacterial
markers. Microbiome \textbf{6}:70.
doi:\href{https://doi.org/10.1186/s40168-018-0451-2}{10.1186/s40168-018-0451-2}.

\hypertarget{ref-mossotto_classification_2017}{}
18. \textbf{Mossotto E}, \textbf{Ashton JJ}, \textbf{Coelho T},
\textbf{Beattie RM}, \textbf{MacArthur BD}, \textbf{Ennis S}. 2017.
Classification of paediatric inflammatory bowel disease using machine
learning. Scientific Reports \textbf{7}.
doi:\href{https://doi.org/10.1038/s41598-017-02606-2}{10.1038/s41598-017-02606-2}.

\hypertarget{ref-wong_quantitation_2017}{}
19. \textbf{Wong SH}, \textbf{Kwong TNY}, \textbf{Chow T-C}, \textbf{Luk
AKC}, \textbf{Dai RZW}, \textbf{Nakatsu G}, \textbf{Lam TYT},
\textbf{Zhang L}, \textbf{Wu JCY}, \textbf{Chan FKL}, \textbf{Ng SSM},
\textbf{Wong MCS}, \textbf{Ng SC}, \textbf{Wu WKK}, \textbf{Yu J},
\textbf{Sung JJY}. 2017. Quantitation of faecal fusobacterium improves
faecal immunochemical test in detecting advanced colorectal neoplasia.
Gut \textbf{66}:1441--1448.
doi:\href{https://doi.org/10.1136/gutjnl-2016-312766}{10.1136/gutjnl-2016-312766}.

\hypertarget{ref-statnikov_comprehensive_2013}{}
20. \textbf{Statnikov A}, \textbf{Henaff M}, \textbf{Narendra V},
\textbf{Konganti K}, \textbf{Li Z}, \textbf{Yang L}, \textbf{Pei Z},
\textbf{Blaser MJ}, \textbf{Aliferis CF}, \textbf{Alekseyenko AV}. 2013.
A comprehensive evaluation of multicategory classification methods for
microbiomic data. Microbiome \textbf{1}:11.
doi:\href{https://doi.org/10.1186/2049-2618-1-11}{10.1186/2049-2618-1-11}.

\hypertarget{ref-knights_supervised_2011}{}
21. \textbf{Knights D}, \textbf{Costello EK}, \textbf{Knight R}. 2011.
Supervised classification of human microbiota. FEMS Microbiology Reviews
\textbf{35}:343--359.
doi:\href{https://doi.org/10.1111/j.1574-6976.2010.00251.x}{10.1111/j.1574-6976.2010.00251.x}.

\hypertarget{ref-wirbel_meta-analysis_2019}{}
22. \textbf{Wirbel J}, \textbf{Pyl PT}, \textbf{Kartal E}, \textbf{Zych
K}, \textbf{Kashani A}, \textbf{Milanese A}, \textbf{Fleck JS},
\textbf{Voigt AY}, \textbf{Palleja A}, \textbf{Ponnudurai R},
\textbf{Sunagawa S}, \textbf{Coelho LP}, \textbf{Schrotz-King P},
\textbf{Vogtmann E}, \textbf{Habermann N}, \textbf{Niméus E},
\textbf{Thomas AM}, \textbf{Manghi P}, \textbf{Gandini S},
\textbf{Serrano D}, \textbf{Mizutani S}, \textbf{Shiroma H},
\textbf{Shiba S}, \textbf{Shibata T}, \textbf{Yachida S}, \textbf{Yamada
T}, \textbf{Waldron L}, \textbf{Naccarati A}, \textbf{Segata N},
\textbf{Sinha R}, \textbf{Ulrich CM}, \textbf{Brenner H},
\textbf{Arumugam M}, \textbf{Bork P}, \textbf{Zeller G}. 2019.
Meta-analysis of fecal metagenomes reveals global microbial signatures
that are specific for colorectal cancer. Nature Medicine
\textbf{25}:679.
doi:\href{https://doi.org/10.1038/s41591-019-0406-6}{10.1038/s41591-019-0406-6}.

\hypertarget{ref-vangay_microbiome_2019}{}
23. \textbf{Vangay P}, \textbf{Hillmann BM}, \textbf{Knights D}. 2019.
Microbiome learning repo (ML repo): A public repository of microbiome
regression and classification tasks. Gigascience \textbf{8}.
doi:\href{https://doi.org/10.1093/gigascience/giz042}{10.1093/gigascience/giz042}.

\hypertarget{ref-galkin_human_2018}{}
24. \textbf{Galkin F}, \textbf{Aliper A}, \textbf{Putin E},
\textbf{Kuznetsov I}, \textbf{Gladyshev VN}, \textbf{Zhavoronkov A}.
2018. Human microbiome aging clocks based on deep learning and tandem of
permutation feature importance and accumulated local effects. bioRxiv.
doi:\href{https://doi.org/10.1101/507780}{10.1101/507780}.

\hypertarget{ref-reiman_using_2017}{}
25. \textbf{Reiman D}, \textbf{Metwally A}, \textbf{Dai Y}. 2017. Using
convolutional neural networks to explore the microbiome, pp. 4269--4272.
\emph{In} 2017 39th annual international conference of the IEEE
engineering in medicine and biology society (EMBC).

\hypertarget{ref-fioravanti_phylogenetic_2017}{}
26. \textbf{Fioravanti D}, \textbf{Giarratano Y}, \textbf{Maggio V},
\textbf{Agostinelli C}, \textbf{Chierici M}, \textbf{Jurman G},
\textbf{Furlanello C}. 2017. Phylogenetic convolutional neural networks
in metagenomics. arXiv:170902268 {[}cs, q-bio{]}.

\hypertarget{ref-thomas_metagenomic_2019}{}
27. \textbf{Thomas AM}, \textbf{Manghi P}, \textbf{Asnicar F},
\textbf{Pasolli E}, \textbf{Armanini F}, \textbf{Zolfo M},
\textbf{Beghini F}, \textbf{Manara S}, \textbf{Karcher N}, \textbf{Pozzi
C}, \textbf{Gandini S}, \textbf{Serrano D}, \textbf{Tarallo S},
\textbf{Francavilla A}, \textbf{Gallo G}, \textbf{Trompetto M},
\textbf{Ferrero G}, \textbf{Mizutani S}, \textbf{Shiroma H},
\textbf{Shiba S}, \textbf{Shibata T}, \textbf{Yachida S}, \textbf{Yamada
T}, \textbf{Wirbel J}, \textbf{Schrotz-King P}, \textbf{Ulrich CM},
\textbf{Brenner H}, \textbf{Arumugam M}, \textbf{Bork P}, \textbf{Zeller
G}, \textbf{Cordero F}, \textbf{Dias-Neto E}, \textbf{Setubal JC},
\textbf{Tett A}, \textbf{Pardini B}, \textbf{Rescigno M},
\textbf{Waldron L}, \textbf{Naccarati A}, \textbf{Segata N}. 2019.
Metagenomic analysis of colorectal cancer datasets identifies
cross-cohort microbial diagnostic signatures and a link with choline
degradation. Nature Medicine \textbf{25}:667.
doi:\href{https://doi.org/10.1038/s41591-019-0405-7}{10.1038/s41591-019-0405-7}.

\hypertarget{ref-rudin_please_2018}{}
28. \textbf{Rudin C}. 2018. Please stop explaining black box models for
high stakes decisions. arXiv:181110154 {[}cs, stat{]}.

\hypertarget{ref-rudin_optimized_2018}{}
29. \textbf{Rudin C}, \textbf{Ustun B}. 2018. Optimized scoring systems:
Toward trust in machine learning for healthcare and criminal justice.
Interfaces \textbf{48}:449--466.
doi:\href{https://doi.org/10.1287/inte.2018.0957}{10.1287/inte.2018.0957}.

\hypertarget{ref-knights_human-associated_2011}{}
30. \textbf{Knights D}, \textbf{Parfrey LW}, \textbf{Zaneveld J},
\textbf{Lozupone C}, \textbf{Knight R}. 2011. Human-associated microbial
signatures: Examining their predictive value. Cell Host Microbe
\textbf{10}:292--296.
doi:\href{https://doi.org/10.1016/j.chom.2011.09.003}{10.1016/j.chom.2011.09.003}.

\hypertarget{ref-miller_explanation_2017}{}
31. \textbf{Miller T}. 2017. Explanation in artificial intelligence:
Insights from the social sciences. arXiv:170607269 {[}cs{]}.

\hypertarget{ref-sze_leveraging_2018}{}
32. \textbf{Sze MA}, \textbf{Schloss PD}. 2018. Leveraging existing 16S
rRNA gene surveys to identify reproducible biomarkers in individuals
with colorectal tumors. mBio \textbf{9}:e00630--18.
doi:\href{https://doi.org/10.1128/mBio.00630-18}{10.1128/mBio.00630-18}.

\hypertarget{ref-dormann_collinearity:_2013}{}
33. \textbf{Dormann CF}, \textbf{Elith J}, \textbf{Bacher S},
\textbf{Buchmann C}, \textbf{Carl G}, \textbf{Carré G}, \textbf{Marquéz
JRG}, \textbf{Gruber B}, \textbf{Lafourcade B}, \textbf{Leitão PJ},
\textbf{Münkemüller T}, \textbf{McClean C}, \textbf{Osborne PE},
\textbf{Reineking B}, \textbf{Schröder B}, \textbf{Skidmore AK},
\textbf{Zurell D}, \textbf{Lautenbach S}. 2013. Collinearity: A review
of methods to deal with it and a simulation study evaluating their
performance. Ecography \textbf{36}:27--46.
doi:\href{https://doi.org/10.1111/j.1600-0587.2012.07348.x}{10.1111/j.1600-0587.2012.07348.x}.

\hypertarget{ref-sze_fecal_2019}{}
34. \textbf{Sze MA}, \textbf{Topçuoğlu BD}, \textbf{Lesniak NA},
\textbf{Ruffin MT}, \textbf{Schloss PD}. 2019. Fecal short-chain fatty
acids are not predictive of colonic tumor status and cannot be predicted
based on bacterial community structure. mBio \textbf{10}:e01454--19.
doi:\href{https://doi.org/10.1128/mBio.01454-19}{10.1128/mBio.01454-19}.

\hypertarget{ref-kocheturov_massive_2019}{}
35. \textbf{Kocheturov A}, \textbf{Pardalos PM}, \textbf{Karakitsiou A}.
2019. Massive datasets and machine learning for computational
biomedicine: Trends and challenges. Ann Oper Res \textbf{276}:5--34.
doi:\href{https://doi.org/10.1007/s10479-018-2891-2}{10.1007/s10479-018-2891-2}.

\hypertarget{ref-kim_improved_2018}{}
36. \textbf{Kim M}, \textbf{Oh I}, \textbf{Ahn J}. 2018. An improved
method for prediction of cancer prognosis by network learning. Genes
\textbf{9}:478.
doi:\href{https://doi.org/10.3390/genes9100478}{10.3390/genes9100478}.

\hypertarget{ref-schloss_introducing_2009}{}
37. \textbf{Schloss PD}, \textbf{Westcott SL}, \textbf{Ryabin T},
\textbf{Hall JR}, \textbf{Hartmann M}, \textbf{Hollister EB},
\textbf{Lesniewski RA}, \textbf{Oakley BB}, \textbf{Parks DH},
\textbf{Robinson CJ}, \textbf{Sahl JW}, \textbf{Stres B},
\textbf{Thallinger GG}, \textbf{Van Horn DJ}, \textbf{Weber CF}. 2009.
Introducing mothur: Open-Source, Platform-Independent,
Community-Supported Software for Describing and Comparing Microbial
Communities. ApplEnvironMicrobiol \textbf{75}:7537--7541.

\hypertarget{ref-westcott_opticlust_2017}{}
38. \textbf{Westcott SL}, \textbf{Schloss PD}. 2017. OptiClust, an
Improved Method for Assigning Amplicon-Based Sequence Data to
Operational Taxonomic Units. mSphere \textbf{2}.
doi:\href{https://doi.org/10.1128/mSphereDirect.00073-17}{10.1128/mSphereDirect.00073-17}.

\hypertarget{ref-rognes_vsearch_2016}{}
39. \textbf{Rognes T}, \textbf{Flouri T}, \textbf{Nichols B},
\textbf{Quince C}, \textbf{Mahé F}. 2016. VSEARCH: A versatile open
source tool for metagenomics. PeerJ \textbf{4}:e2584.
doi:\href{https://doi.org/10.7717/peerj.2584}{10.7717/peerj.2584}.

\hypertarget{ref-li_hyperband:_2016}{}
40. \textbf{Li L}, \textbf{Jamieson K}, \textbf{DeSalvo G},
\textbf{Rostamizadeh A}, \textbf{Talwalkar A}. 2016. Hyperband: A novel
bandit-based approach to hyperparameter optimization. arXiv:160306560
{[}cs, stat{]}.


\end{document}
