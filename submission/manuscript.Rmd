---
title: '**Best practices for applying machine learning to bacterial 16S rRNA gene sequencing data**'
csl: mbio.csl
fontsize: 11pt
output:
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes
  word_document: default
geometry: margin=1.0in
bibliography: references.bib
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in important functions -----------------#
######################################################################
source("../code/learning/functions.R")

#####################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################
source("../code/learning/load_data.R")

######################################################################
#----------------- Load feature importance data -----------------#
# Median of base testing AUC for each model
# 100 datasplit permutation importances new testing AUC for top 5 features
######################################################################

source("../code/learning/load_imp_data.R")

######################################################################
#-------------------------- Load traintime data -----------------#
######################################################################
source("../code/learning/load_traintime.R")

######################################################################
#-------- Load AUROC performance data and Wilcoxon test ------------#

######################################################################
source("../code/learning/load_AUROC_data.R")
```

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Nicholas A. Lesniak${^1}$, Jenna Wiens${^2}$, Mack Ruffin${^3}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109

3\. Department of Family Medicine and Community Medicine, Penn State Hershey Medical Center, Hershey, PA




\newpage
\linenumbers


## Abstract

Machine learning (ML) modeling of the human microbiome has the potential to identify the microbial biomarkers and aid in diagnosis of many chronic diseases such as inflammatory bowel disease, diabetes and colorectal cancer. Progress has been made towards developing ML models that predict health outcomes from bacterial abundances, but rigourous ML models are scarce due to the flawed methods that call the validity of developed ML models into question. Furthermore, the use of black box ML models has hindered the validation of microbial biomarkers. To overcome these challenges, we benchmarked seven different ML models that use fecal 16S rRNA sequences to predict colorectal cancer (CRC) lesions (n=490 patients, 261 controls and 229 cases). To show the effect of model selection, we assessed the predictive performance, interpretability, and computational efficiency of the following models: L2-regularized logistic regression, L1 and L2-regularized support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest, and extreme gradient boosting (XGBoost). The random forest model was best at detecting CRC lesions with an AUROC of 0.695 but it was slow to train (83.2 h) and hard to interpret. Despite its simplicity, L2-regularized logistic regression followed random forest in predictive performance with an AUROC of 0.680, and it trained much faster (12 min). In this study, we established standards for the development of modeling pipelines for microbiome-associated ML models. Additionally, we showed that ML models should be chosen based on expectations of predictive performance, interpretability and available computational resources. 

\newpage
## Importance (needs work)

Prediction of health outcomes using ML is rapidly being adopted by human microbiome studies. However, the developed ML models so far are overoptimistic in terms of validity and predictive performance. Without rigorous ML pipelines, we cannot trust ML models. Before we can speed up progress, we need to slow down, define and implement good ML practices.
\newpage


## Background

Advances in sequencing technology and decreases in the costs of generating 16S rRNA gene sequences have allowed rapid exploration of the human microbiome and its health implications. Currently, the human microbiome field is growing at an unprecedented rate and there is an increasing demand for methods that identify associations between microbiome members and human health. However, this is difficult as human associated microbial communities are remarkably complex, high-dimensional and uneven within and between individuals with the same disease. It is unlikely that a single species can explain a disease. Instead, subsets of those communities, in relation to one another and to their host, account for differences in health outcomes. 

Machine learning (ML) methods are effective at recognizing and highlighting patterns in complex microbial datasets. They learn from existing data to predict the outcomes of new data and allow us to infer on the reasons underlying that prediction. Therefore, researchers have started to explore the utility of ML models that use microbiota associated biomarkers to predict human health and to understand the ecological basis of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases (IBD), obesity, and type 2 diabetes [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018]. The high-stake task of predicting the correct diagnosis with high confidence relies on a ML model that is built with rigorous methods. However, the field's use of ML lacks clarity and consistency in which methods are used and how these methods are implemented [@thaiss_persistent_2016; @dadkhah_gut_2019]. More notably, we commonly see flawed ML practices such as using ML pipelines where there is no seperate held-out test dataset to evaluate predictive performance or reporting few or only the best outcomes of cross-validation. Even when there are seperate testing sets to evaluate predictive performance, there are large differences between cross-validation and testing performances and large confidence intervals in testing performances [@flemer_oral_2018; @baxter_microbiota-based_2016; @dai_multi-cohort_2018; @montassier_pretreatment_2016; @papa_non-invasive_2012; @mossotto_classification_2017; @ai_systematic_2017; @wong_quantitation_2017]. These practices prevent the development of generalizable models, where the model makes accurate predictions with newly acquired data as well as it does with the training data. 

Moreover, there is a lack of discussion on why a particular ML model is utilized and a lack of emphasis on the strengths and weaknesses of it. Recently, there is a trend towards using more complex ML models such as random forest, extreme gradient boosting (XGBoost) and neural networks without a discussion on if and how much model interpretibility is necessary for the study [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017; @ @geman_deep_2018]. These models are also called black box ML models because they are not inherently interpretable and require posthoc explanations to determine the feature importances in making a prediction. These explanations can be misleading and at times unreliable when making high-stake decisions about patient health [@rudin_please_2018]. The models we develop for healthcare, both to predict disease and to understand underlying reasons behind that prediction, should be transparent and accountable [@rudin_optimized_2018].

The lack of transparency on model selection and interpretation as well as flawed modeling methods negatively impact model  validity and reproducibility. We need to strive toward better machine learning practices by (1) implementing rigorous machine learning pipelines and (2) selecting ML models that reflect the goal of the study as it will inform our expectations of predictive performance, complexity, interpretability and computational efficiency. To showcase a rigorous ML pipeline and to shed light on how ML model selection can affect modeling results, we performed an empirical analysis comparing several different ML models using the same dataset and the same ML pipeline. We used a previously published colorectal cancer (CRC) study [@baxter_dna_2016] which had fecal 16S rRNA gene sequences of CRC patients. The human microbiome is hypothesized to directly contribute to the development of CRC and fecal 16S rRNA gene sequences have been used to detect it [@baxter_microbiota-based_2016; @baxter_dna_2016; @zeller_potential_2014; @knights_human-associated_2011]. We built seven ML models using fecal 16S rRNA gene sequences to predict healthy patients versus patients with colorectal lesions that were identified by colonoscopy as screen relevant neoplasias (SRN). We established modeling pipelines for three linear models with different forms of regularization; L2-regularized logistic regression, L1 and L2-regularized support vector machines (SVM) with linear kernel. We also developed four non-linear models; SVM with radial basis function kernel, a decision tree, random forest and XGBoost. We compared the predictive performance, interpretability and computational efficiency of the seven models to highlight the importance of model selection. We established standards for ML pipeline construction, predictive performance evaluation and model interpretation for microbiome-associated ML models that have a binary prediction task. 

## Results 

__Model selection and pipeline construction__

We first determined the dataset and the ML models to use in our study. We used a previously published study on a CRC cohort of 490 patients with 261 cases of SRN. For each patient, we had `r paste(ncol(data)-1)` features (fecal bacterial abundances) and a two-class label that defines their colorectal health (normal or SRN colorectal lesions as defined by colonoscopies). We established modeling pipelines for a binary prediction task with L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and extreme gradient boosted decision tree (XGBoost).

We used ML models with different classification algorithms and regularization parameters. For regularized logistic regression and SVM with linear kernel, we used L2 regularization to keep all potentially important features. For comparison, we also trained an L1 regularized SVM model with linear kernel. L1-regularization on microbiome data lead to a sparser solution (i.e., force many coefficients to zero). Finally, to explore the potential for non-linear relationships among features and the outcome of interest, we trained tree based models, decision tree, random forest and XGBoost, as well as an SVM with non-linear kernel. 

We established a ML pipeline where we train and validate each of the seven models [Figure 1]. We randomly split the data into training/validation and test sets so that the training/validation set consisted of 80% of the full dataset while the test set was composed of the remaining data [Figure 1]. Since the cases are not uniformly represented in the data, the initial data-split was stratified to maintain the overall label distribution in both the training/validation and test sets. Training/validation set consisted of 393 patients (209 SRN), while the test set was composed of 97 patients (52 SRN). The training/validation data was used for training purposes and validation of hyperparameter selection, and the test set was used for evaluation purposes. 

Hyperparameters are the rules that are learned from the training data in a classification algorithm. When they are tuned over a full grid search and selected by validation, they make the ML model better are predicting. We selected hyperparameter settings using repeated five-fold cross-validation on the training/validation set [Figure 1]. We chose the best hyperparameter setting for each model based on its predictive performance on the validation set using the area under the receiver operating characteristic curve (AUROC) metric [Figure S1 and S2]. The AUROC ranges from 1.0, where the model perfectly distinguishes between cases and controls, to 0.50, where the model's predictions are no different from random chance. Similar to the initial data-split, five-fold cross-validation for hyperparameter selection was also stratified to maintain the overall label distribution on the training and validation sets. The cross-validation of each hyperparameter setting was repeated over 100 randomizations to get a robust reading of predictive performance. 

We then trained the full training/validation dataset with the selected hyperparameters. We used the held-out test set to evaluate the predictive performance of each ML model. The data-split, hyperparameter selection, training and testing steps were repeated 100 times to get a reliable and robust reading of model performance [Figure 1]. 

__Predictive performance and generalizability of the seven models.__

We evaluated the predictive performances of seven binary classification models when applied to held-out test data using the AUROC metric [Figure 2]. Random forest had significantly higher test AUROC values than the other models for detecting SRNs when AUROC values were compared to the other six by Wilcoxon rank sum test (p < 0.01). The median AUROC of the random forest model was `r paste(round(median((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Random_Forest"))$test_aucs), 3))`). L2-regularized logistic regression, XGBoost, L2-regularized SVM with linear and radial basis function kernel AUROC values were not significantly different from one another. They had median AUROC values of `r paste(round(median((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="XGBoost"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="XGBoost"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))`) and  `r paste(round(median((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))`) respectively. L1-regularized SVM with linear kernel and decision tree had significantly lower AUROC values than the other ML models with median AUROC of `r paste(round(median((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))`) and `r paste(round(median((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))`), respectively [Figure 2]. Random forest had the highest median AUROC for detecting SRN. Despite its simplicity, the L2-regularized logistic regression was second best in predictive performance. 

To evaluate the generalizability of each model, we compared the median cross-validation AUROC to the median testing AUROC. The difference between the two should be low to suggest the model is not overfitting despite the large number of features. The largest difference between the two was `r paste(round(abs(difference_model[difference_index[7],1]), 3))` in L1-regularized SVM with linear kernel, followed by SVM with radial basis function kernel and decision tree with a difference of `r paste(round(abs(difference_model[difference_index[6],1]), 3))` and `r paste(round(abs(difference_model[difference_index[5],1]), 3))`, respectively [Figure 2]. We also reported the testing AUROC values over 100 randomizations of the initial data-split. The testing AUROC values within each model varied `r paste(round(mean((all %>% group_by(model) %>% summarise(difference=max(test_aucs)-min(test_aucs)))$difference), 2))` on average across the seven models. For instance, the lowest AUROC value of the random forest model was `r paste(round(min((all %>% filter(model=="Random_Forest"))$test_aucs), 2))` whereas the highest was `r paste(round(max((all %>% filter(model=="Random_Forest"))$test_aucs), 2))`. These results showed that depending on the data-split, the testing AUROC values showed great variability [Figure 2]. 

__Interpretation of each ML model.__

Interpretability is the degree to which humans can understand the reasons behind a model prediction [@miller_explanation_2017]. Because we often use ML models not just to predict a health outcome but also to learn the ecology behind a disease, model interpretation becomes crucial for microbiome studies. The ML models we built using L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost decrease in interpretability as they increase in complexity.  In this study we highlighted two methods to interpret models with varying complexity. 

We interpreted linear models (L1 and L2-regularized SVM with linear kernel and L2-regularized logistic regression) using the absolute feature weights of the trained models. We ranked the absolute weights of all the OTUs for each data-split [Figure 3]. We calculated the median ranks of these features over the 100 data-splits. In the three linear models, OTUs that had the largest median ranks and drove the detection of SRNs belonged to families *Lachnospiraceae*,  and *Ruminococcaceae* (OTU01239, OTU00659, OTU00742, OTU00012, OTU00015, OTU00768, OTU00822, OTU00609), genera *Gamella*  (OTU00426) and genera *Peptostreptococcus* (OTU00367) [Figure 3]. Some of the OTUs with the highest ranks were shared among the linear models. 

We explained the feature importances in non-linear models; SVM with radial basis kernel, decision tree, random forest and XGBoost, using a method called permutation importance on the held-out test data.  Permutation importance analysis is a posthoc explanation of the model where we randomly permute non-correlated features individually and groups of highly correlated features together. We then calculate how much the predictive performance of the model (i.e AUROC values) decrease when each OTU or group of OTUs is permuted randomly. We ranked the OTUs based on how much they decreased the median testing AUROC; the OTU with the largest decrease ranking highest. The top 5 OTUs with the largest negative impact on testing AUROC overlapped in tree-based models [Figure 4]. Specifically, permuting *Peptostreptococcus* (OTU00367) abundances randomly, dropped the predictive performances the most in all tree-based methods [Figure 4]. Decision tree, random forest and XGBoost models' predictive performance dropped from `r paste(round(dt_median, 2))` median AUROC to `r paste(round(median((dt_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, from `r paste(round(rf_median, 2))` to `r paste(round(median((rf_imp %>% filter(names=="Otu00367"))$new_auc), 2))` and from `r paste(round(xgboost_median, 2))` to `r paste(round(median((xgboost_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, respectively [Figure 4]. 

To highlight the differences between the two interpretation methods, we used permutation importance to interpret linear models as well [Figure S3]. L1-regularized SVM with linear kernel picked out some of the same OTUs (OTU00822, OTU01239, OTU00609) as important in feature rankings based on weights [Figure 3] and permutation importance [Figure S3]. Similarly, L2-regularized SVM and L2-regularized logistic regression picked out some of the same OTUs in both interpretation methods, OTU00659 and OTU00012, respectively. However, for all the linear models, the rankings of these features were different due to the collinearity in microbial communities. 


__The computational efficiency of each ML model.__

We compared the training times of the seven ML models. As the complexity of a ML model [Table S1] and the number of tuned hyperparameter settings increased [Figures S1-S2], its training times increased as well [Figure 5]. Linear models trained faster than non-linear models. L1 and L2 SVM with linear kernel and L2-regularized logistic regression had the shortest training times with `r paste(round(traintime_df[traintime_index[1],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[1],3], 2))`), `r paste(round(traintime_df[traintime_index[2],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[2],3],2))`), and `r paste(round(traintime_df[traintime_index[3],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[3],3], 2))`), respectively. Whereas, a decision tree, SVM with radial basis function kernel, random forest and XGBoost had training times of `r paste(round(traintime_df[traintime_index[4],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[4],3], 1))`), `r paste(round(traintime_df[traintime_index[5],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[5],3], 1))`), `r paste(round(traintime_df[traintime_index[6],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[6],3], 1))`) and `r paste(round(traintime_df[traintime_index[7],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[7],3], 1))`), respectively [Figure 5]. 

## Discussion

In this study we established a rigorous ML pipeline to use 16S rRNA sequence counts to predict a binary health outcome. We built on others' work in the microbiome field to set-up standards for developing and evaluating ML models for microbiome data [@statnikov_comprehensive_2013; @knights_supervised_2011; @wirbel_meta-analysis_2019]. First, we used a held-out test set to illustrate the difference between cross-validation and testing AUROC values. When the difference between cross-validation and test performance is low, this suggest the models are not overfit and that they will perform similar with similar data. In all seven models, the difference between median cross-validation and testing AUROC values did not exceed `r paste(round(abs(difference_model[difference_index[7],1]), 3))` which suggests that these models are generalizable and can be used to test similar new data. Second, we performed the initial 80%-20% random datasplit 100 times in our ML pipeline. The randomization of the initial data-split to create a held-out test set is a crucial step in the ML pipeline to develop robust ML models and to report reliable performance metrics. Depending on how the data is split, there is the chance of being overoptimistic about the predictive performance of a model. In our study, we showed that there was variability in AUROC values between different random data-splits in each of the models we tested. Our results showed that the testing AUROC values varied `r paste(round(mean((all %>% group_by(model) %>% summarise(difference=max(test_aucs)-min(test_aucs)))$difference), 2))` on average between different data-splits. Third, we used the AUROC metric in our study to evaluate the predictive performance of the ML models. AUROC is always random at the value 0.5 and is a robust metric when a dataset is imbalanced. We also performed a full grid search for hyperparameter settings when building a ML model. Default hyperparameter settings in previously developed ML packages in R, Python, and Matlab programming languages are inadequate for effective application of classification algorithms and need to be optimized for each new dataset used to generate a model. In the example of L1-regularized SVM with linear kernel [Figure S1], the model showed large variability between different regularization coefficients (C) and was susceptible to performing poorly if the wrong regularization coefficient was assigned to the model by default. 
 
In this study, we benchmarked seven ML models with different classification algorithms to show that we should use ML models based on the goal of the study and our expectations of predictive performance, interpretability and computational burden. Microbiome studies use ML models with a classification task to learn the training data to assign labels, such as healthy or not to new data, but also to learn which features are important to discriminate between labels [@zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018]. Our results show that if the goal of a study is to learn the ecology behind a disease and to identify microbial biomarkers, we could create ML models that are inherently interpretable without losing predictive power. In terms of predictive performance, random forest model had the best testing AUROC values compared to the other 6 models. However, the second best model was L2-regularized logistic regression with a median AUROC difference of only 0.015 compared to random forest. In terms of interpretability, random forest was a complex ML model and could only be explained using methods such as permutation importance. On the other hand, L2-regularized logistic regression was easier to interpret (i.e. ranking absolute regression coefficients of the trained model).

Even with interpretable models such as L2-regularized logistic regression, we need to be careful with how we treat the information we get from the models. In this study we used two different methods to interpret our linear models; ranking each OTU by (1) their absolute weights in the trained models and (2) their impact on the predictive performance based on permutation importance. We observed differences in the OTU rankings between the two interpretation methods due to collinearity in the dataset. Collinearity in a microbial dataset is when one OTU is highly correlated with another OTU which causes feature weights to not be unique. The feature weights of correlated OTUs are influenced by one another which makes it difficult to interpret the ML model. To avoid misinterpreting the models we should use the highly ranked correlated OTUs to generate hypotheses about the ecology of the disease and test them with follow-up experiments.

There are other criteria when choosing ML models such as the computational burden of developing it and the sample size. In terms of computational burden, random forest model trained each data-split in 83.2 hours whereas L2-regularized logistic regression trained in 12 minutes. The generalization performance of ML models depends on sample size. The more complex the model, the more data it will need. The dataset we used for our study had 490 samples, however microbiome studies that have smaller sample sizes would benefit from using less complex models such as L2-regularized logistic regression.

This study highlights the need to make educated choices at every step of developing a ML model with microbiome data. Model selection should be done with a solid understanding of model complexity and interpretability, rigorous ML pipelines should be built with cross-validation for hyperparameter tuning and with a held-out test set for evaluating predictive performance and models should be interpreted while considering collinearity in datasets. The right methods will help us achieve the level of validity and accountability we want from models built for patient health. 

## Materials and Methods

**Data collection and study population.**
  The data used for this analysis are stool bacterial abundances and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic health was labeled by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

**Data definitions and pre-processing.**

  The colorectal health of the patient was defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. The study had 261 normal and 229 SRN samples. The bacterial abundances are the features used to predict colonic health of the patients. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. OTU counts were set to the size of our smallest sample and were subsampled at the same distances. They were then transformed by scaling to a [0-1] range. 
  
**Model training and evaluation.**

Models were trained using the machine learning wrapper caret package (v.6.0.81) in R (v.3.5.0). Within the caret package, we have made modifications to L2-regularized SVM with linear kernel function __svmLinear3__ and developed a L1-regularized SVM with linear kernel function __svmLinear4__ to calculate decision values instead of predicted probabilities. These changes are available at https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/. 

For L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For XGBoost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners.For hyperparameter selection, we started with a granular grid search. Then we narrowed and fine-tuned the range of each hyperparameter. The range of the grid depends on the ML task and ML model. A full grid search needs to be performed to avoid variability in testing performance. We can use hyper-band to help us with our hyperparameter selection [@li_hyperband:_2016]. 

The computational burden during model training due to model complexity was reduced by parallelizing segments of the ML pipeline. In this study we have parallelized each data-split which allowed 100 data-splits to be processed through the ML pipeline at the same time for each model. We can further parallelize the cross-validation step for each hyperparameter setting. 


**Permutation importance workflow.**
We created a Spearman's rank-order correlation matrix, corrected for multiple pairwise comparisons. We then defined correlated OTUs as having perfect correlation (correlation coef=1 and p<0.01). Non-correlated OTUs were permuted individually whereas correlated ones were grouped together and permuted at the same time.
 
**Statistical analysis workflow.** Data summaries, statistical analysis, and data visualizations were performed using R (v.3.5.0) with the tidyverse package (v.1.2.1). We compared the AUROC values of the seven ML models by Wilcoxon rank sum tests to determine the best predictive performance. 

**Code availability.** The code for all sequence curation and analysis steps including an Rmarkdown version of this manuscript is available at https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/.

\newpage
\includegraphics{Figure_1}
**Figure 1. Machine learning pipeline showing predictive model training and evaluation flowchart.  **  We split the data 80%/20% stratified to maintain the overall label distribution, performed five-fold cross-validation on the training data to select the best hyperparameter setting and then using these hyperparameters to train all of the training data. The model was evaluated on a held-out set of data (not used in selecting the model).
Abbreviations: cvAUROC, cross-validation area under the receiver operating characteristic curve
\newpage
\includegraphics{Figure_2.png}

**Figure 2. Generalization and classification performance of ML models using AUROC values of all cross validation and testing performances. **   The median AUROC for diagnosing individuals with SRN using bacterial abundances was higher than chance (depicted by horizontal line at 0.50) for all the ML models. Predictive performance of random forest model was higher than other ML models. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: SRN, screen-relevant neoplasias; AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting
\newpage
\includegraphics[height=17cm, width=12cm]{Figure_3.png}

**Figure 3. Interpretation of the linear ML models.** The absolute feature weights of (A) L2 logistic regression coefficients (B) L1 SVM with linear kernel (C) L2 SVM with linear kernel were ranked from highest rank 1 to 100 for each data-split. The feature ranks of the highest ranked five OTUs based on their median ranks are shown here. Similar OTUs had the largest impact on the predictive performance of L2 logistic regression and L2 SVM with linear kernel. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_4.png}

**Figure 4. Interpretation of the non-linear ML models.** (A) SVM with radial basis kernel (B) decision tree (C) random forest (D) XGBoost feature importances were explained using permutation importance using held-out test set. The gray rectangle and the dashed line show the IQR range and median of the base testing AUROC without any permutation performed. The colors of the box plots stand for the unique OTUs that are shared among the different models; pink for OTU0008, salmon for OTU0050, yellow for OTU00367, blue for OTU00110, green for OTU00361 and red for OTU00882. For all the tree-based models, a *Peptostreptococcus* species (OTU00367) had the largest impact on predictive performance of the model. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit; RBF, radial basis kernel; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_5.png}

**Figure 5. Computational efficiency of seven ML models.** The training times for of each data-split showed the differences in computational efficiency of the seven models. The median training time in hours was the highest for XGBoost and shortest for L1-regularized SVM with linear kernel. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting.  
\newpage
\includegraphics{Figure_S1.png}

**Figure S1. Hyperparameter setting performances for linear models.** (A) L2 logistic regression (B) L1 SVM with linear kernel (C) L2 SVM with linear kernel mean cross-validation AUROC values when different hyperparameters are used in training the model. The differences in AUROC values when hyperparameters change show that hyperparameter tuning is a crucial step in building a ML model. 

\newpage
\includegraphics[height=30cm, width=15cm]{Figure_S2.png}

**Figure S2. Hyperparameter setting performances for non-linear models.** (A) Decision tree (B) Random forest (C) SVM with radial basis kernel (D) XGBoost mean cross-validation AUROC values when different hyperparameters are used in training the model. The differences in AUROC values when hyperparameters change show that hyperparameter tuning is a crucial step in building a ML model.
\newpage
\includegraphics[height=17.5cm, width=13cm]{Figure_S3.png}

**Figure S3. Interpretation of the linear ML models with permutation importance.** (A) L1-regularized SVM with linear kernel (B) L2-regularized SVM with linear kernel and (C) L2-regularized logistic regression were interpreted using permutation importance using held-out test set. The gray rectangle and the dashed line show the IQR range and median of the base testing AUROC without any permutation performed. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit; RBF, radial basis kernel; OTU, Operational Taxonomic Unit.

\newpage
\includegraphics{TableS1.pdf}
**Table S1. ML model complexity and hyperparameter range.** 
\newpage


## References

<div id="refs"></div>


