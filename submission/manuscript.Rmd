---
title: "**Evaluation of machine learning methods for 16S rRNA gene data**"
bibliography: references.bib
output:
  pdf_document: 
    keep_tex: true
    includes:
      in_header: header.tex
csl: mbio.csl #Get themes at https://github.com/citation-style-language/styles
fontsize: 11pt
geometry: margin=1.0in
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in important functions -----------------#
######################################################################
source("../code/learning/functions.R")

#####################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################
source("../code/learning/load_data.R")

######################################################################
#--------- Load .csv data to get mean test AUC for 7 models--------#
######################################################################

# Read in the cvAUCs, testAUCs for 100 splits.
best_files <- list.files(path= '../data/process', pattern='combined_best.*', full.names = TRUE)
all <- map_df(best_files, read_files) 

# Get the The unpaired two-samples Wilcoxon test to see if models differ from one another signigicantly

rf_xgboost <- wilcoxon_test(all, "Random_Forest", "XGBoost")
rf_logit <- wilcoxon_test(all, "Random_Forest", "L2_Logistic_Regression")
rf_L2svm <- wilcoxon_test(all, "Random_Forest", "L2_Linear_SVM")
xgboost_L2svm <- wilcoxon_test(all, "XGBoost", "L2_Linear_SVM")
logit_L2svm <- wilcoxon_test(all, "L2_Logistic_Regression", "L2_Linear_SVM")

# Bind them together and summarise mean testAUC by model
test_all <- all %>%
  melt_data() %>% 
  select(-variable) %>% 
  group_by(model, Performance) %>% 
  summarise(mean_AUC = mean(AUC), sd_AUC = sd(AUC)) %>% 
  filter(Performance=="testing") 

# Get the order index from small to large meanAUC
performance_index <- order(test_all$mean_AUC)

# Bind them together and summarise mean cvAUC by model
cv_all <- all %>%
  melt_data() %>% 
  select(-variable) %>% 
  group_by(model, Performance) %>% 
  summarise(mean_AUC = mean(AUC), sd_AUC = sd(AUC)) %>% 
  filter(Performance=="cross-validation") 

# Get the difference between mean cvAUC and testAUC for each model
difference <- cv_all$mean_AUC - test_all$mean_AUC[match(cv_all$model, test_all$model)]
difference_model <- data.frame(difference, test_all$model)
# Get the order index from small to large of the differences
difference_index <- order(abs(difference_model$difference))

######################################################################
#---------- Load .csv datato get mean walltime for 7 models-------#
######################################################################

# Read in the walltime for each split.
walltime_files <- list.files(path= '../data/process', pattern='walltime*', full.names = TRUE) 

# ------- 1. In a loop read the files as delim
# --------2. get the model name for each with get_model_name() 
#---------3. Add a column with model name to each delim
result <- list()

for(file in walltime_files){
  model_walltime <- read_files(file) %>%  
  #summarise_walltime() %>% 
  mutate(model=get_model_name(file))
  result[[length(result)+1]] <- model_walltime
}


# ------- 1. If models are L1 or L2 Linear SVM or L2 Logit then the walltime is in minutes
# --------2. If models are Random Forest or XGBoost then the walltime is in days
#---------3. If other models, the walltime is in hours.
#---------4. This loop converts all to hours.
min_fixed_result <- list() 

for(i in result){
  i$x <- i$x/3600 # The walltimes were saved as seconds we covert to hours
  min_fixed_result[[length(min_fixed_result)+1]] <- i
}
# Bind all delims with model name and walltime together
# Summarise mean walltime
walltime_df <- bind_rows(min_fixed_result) %>% 
  group_by(model) %>% 
  summarise(mean_walltime=mean(x), sd_walltime=sd(x))

# Get the sorted mean walltime for each model small to large
walltime_index <- order(walltime_df$mean_walltime) 
  

```

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Jenna Wiens${^2}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109


\newpage
\linenumbers


## Abstract


\newpage

## Introduction

Advances in sequencing technology and decreasing costs of generating 16S rRNA gene sequences have allowed rapid exploration of human associated microbiome and its health implications. Currently, the human microbiome field is growing at an unprecedented rate and as a result, there is an increasing demand for methods that identify associations between members of the microbiome and human health. However, this is difficult as human associated microbial communities are remarkably complex and uneven. It is unlikely that a single species can explain a disease. Instead, subsets of those communities, in relation to one another and to their host, account for the differences in health outcomes. Machine learning (ML) methods are effective at recognizing and highlighting patterns in complex microbial datasets. Therefore, researchers have started to explore the utility of ML models that use microbiota associated biomarkers to predict human health and to understand the microbial ecology of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases (IBD), obesity, and type 2 diabetes [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018].  However, currently the field's use of ML lacks clarity and consistency on which methods are used and how these methods are implemented. Most notably, there are misleading practices of using "leaky" ML pipelines where models are tested on training data or reporting the best outcome of different iterations of cross-validation. Moreover, there is a lack of discussion on why a particular ML model is utilized. Recently, there is a trend towards using more complex ML models such as random forest, extreme gradient boosting and neural networks without a discussion on if and how much model interpretibility is necessary for the study [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017; @ @geman_deep_2018]. The lack of transparency on modeling methodology and model selection negatively impact model reproducibility and reliability. We need to strive toward better machine learning practices by (1) implementing consistent and reliable machine learning pipelines; (2) selecting ML models that reflect the goal of the study as it will inform our expectations of model accuracy, complexity, interpretibility and computational efficiency.

To showcase reliable ML pipeline and to shed light on how much ML model selection can affect modeling results, we performed an empirical analysis comparing several different ML models using the same dataset and with a robust ML pipeline. We used a previously published colorectal cancer (CRC) study [@baxter_dna_2016] which had fecal 16S rRNA gene sequences from `r paste(nrow(shared))` patients. We built ML models using fecal 16S rRNA gene sequences to predict patients with normal colons or patients with colonic tumors which are called screen relevant neoplasias (SRN). The study had `r paste(sum(sum(meta$dx=="normal")+sum(meta$Dx_Bin=="Adenoma")))` normal and `r paste(sum(sum(meta$dx=="cancer")+sum(meta$Dx_Bin=="adv Adenoma")))` SRN samples. We established modeling pipelines for L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost. Our ML pipeline utilized held-out test data to evaluate generalization and prediction performance of each ML model. The mean test AUROC varied from `r paste(round(test_all[performance_index[1],3], 3))` (std ± `r paste(round(test_all[performance_index[1],4], 3))`) to `r paste(round(test_all[performance_index[7],3], 3))` (std ± `r paste(round(test_all[performance_index[7],4], 3))`). Random forest had the highest mean AUROC for detecting SRN. Despite the simplicity, the L2-regularized logistic regression followed random forest in performance. In terms of computational efficiency, L2 logistic regression trained the fastest (`r paste(round(walltime_df[walltime_index[1],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[1],3], 3))`), while XGBoost took the longest (`r paste(round(walltime_df[walltime_index[7],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[7],3], 3))`). We found that mean cross-validation and testing AUROC varied only `r paste(round(difference[difference_index[7]], 3))`, which highlights the importance of a seperate held-out test set and consistent preprocessing of the data prior to evaluation. Aside from evaluating generalization and classification performances for each of these models, this study established standards for modeling pipelines of microbiome-associated machine learning models.

## Results 

__Model selection and construction__

The data has `r paste(ncol(data)-1)` features (`r paste(ncol(data)-1)` OTUs) and a two-class label that defines the colonic health of the patients (SRN or normal). All the cases were independently labeled through colonoscopies. We established modeling pipelines for a binary prediction task with L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost to emphasize the differences in  model accuracy, complexity, interpretibility and computational efficiency due to  model selection. We randomly split the data into training/validation and test sets 100 times with a 80/20 proportion [Figure 1]. Since the colonic health is not uniformly represented in the data, data-splits are stratified to maintain the overall label distribution on the training set. For each data-split, training/validation set consisted of 393 patients (209 SRN), while the test set was composed of 97 patients (52 SRN). The training/validation data was used for training purposes and validation of hyperparameter selection, and the test set was used for evaluation purposes. Validation of hyperparameter selection was performed using 100 randomizations of five-fold cross-validation on the training/validation set [Figure 1]. Similar to the initial data-split, five-fold cross-validation was also stratified to maintain the overall label distribution on the training and validation sets. We validated the cross-validation performances of each hyperparameter setting over the 100 randomizations and selected the best performing hyper-parameter setting to train the full training/validation dataset [Figures S1 and S2]. We then used the held-out test set to evaluate the prediction performance of each ML model. 

__The prediction and generalization performance of classifiers during cross-validation and when applied to the held-out test data.__

We evaluated the prediction performance of seven binary classification models when applied to held-out test data over 100 data-splits using the area under the receiver operating characteristic curve (AUROC) as the discriminative performance metric. Random Forest had the highest mean AUROC for detecting SRN, `r paste(round(mean((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="Random_Forest"))$test_aucs), 3))`) [Figure 2]. L2 logistic regression and XGBoost had significantly lower AUROC values, `r paste(round(mean((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))`) and `r paste(round(mean((all %>% filter(model=="XGBoost"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="XGBoost"))$test_aucs), 3))`) [Figure 2]. Random forest also had significanly higher performances than L2 linear SVM, RBF SVM and decision tree which had mean AUROC values of `r paste(round(mean((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))`),  `r paste(round(mean((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))`), and `r paste(round(mean((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))`), respectively [Figure 2]. We also evaluated the generalization performance of each classifier by comparing their mean cross-validation AUROC and mean testing AUROC. We found that mean cross-validation and testing AUROC difference for each model was below 0.01. 

__The complexity and interpretibility of each classifier.__

We interpreted the feature weights of L1 and L2 SVM with linear kernel and regression coefficients of L2 logistic regression using the training data. We calculated the mean weights of all the features fromt the 100 randomizations of five-fold cross-validation [Figure 3a]. In the three linear models, OTUs that drove the detection of SRNs belong to family *Lachnospiraceae* and *Ruminococcaceae* (OTUs x, y, z). To explain the importance of features in non-linear models, we used permutation importance on the held-out test data [Figure 3b]. Perfectly correlated OTUs were grouped and permuted together to get a better representation of their effect in the prediction performance of the model. Non-correlated OTUs were permuted individually. 


__The computational efficiency of each classifier.__

Linear models trained faster than non-linear models. L2 logistic Rregression and L1 and L2 SVM with linear kernel had training times of `r paste(round(walltime_df[walltime_index[1],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[1],3], 2))`), `r paste(round(walltime_df[walltime_index[2],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[2],3],2))`), and `r paste(round(walltime_df[walltime_index[3],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[3],3], 2))`), respectively. Whereas, SVM with radial basis function kernel, decision tree, random forest and xgboost had training times of `r paste(round(walltime_df[walltime_index[4],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[4],3], 1))`), `r paste(round(walltime_df[walltime_index[5],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[5],3], 1))`), `r paste(round(walltime_df[walltime_index[6],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[6],3], 1))`) and `r paste(round(walltime_df[walltime_index[7],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[7],3], 1))`), respectively [Figure 4].






## Conclusions



## Materials and Methods

#### Data collection and study population
  The data used for this analysis are stool bacterial abundances and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic disease status was defined by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

#### Data definitions and pre-processing

  The colonic health of the patient was defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. The bacterial abundances are the features used to predict colonic health of the patients. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. OTU counts were set to the size of our smallest sample and were subsampled at the same distances. They were then transformd by scaling to a [0-1] range. 
  
#### Model training and evaluation

For L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For xgboost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners.
Models were trained using the machine learning wrapper caret package (v.6.0.81) in R (v.3.5.0). 
 
**Statistical analysis workflow.** Data summaries, statistical analysis, and data visualizations were performed using R (v.3.5.0) with the tidyverse package (v.1.2.1). We compared the AUROC values of the seven ML models by Wilcoxon rank sum tests to determine the best discriminative performance. 

**Code availability.** The code for all sequence curation and analysis steps including an Rmarkdown version of this manuscript is available at https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/.

 
\newpage



**Figure 1. Generalization and classification performance of modeling methods ** AUC values of all cross validation and testing performances. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 

\newpage

## References

<div id="refs"></div>


