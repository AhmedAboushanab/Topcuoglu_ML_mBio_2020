---
title: '**Effective application of machine learning to bacterial 16S rRNA gene sequencing data**'
csl: mbio.csl
fontsize: 11pt
output:
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes
bibliography: references.bib
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in important functions -----------------#
######################################################################
source("../code/learning/functions.R")

#####################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################
source("../code/learning/load_data.R")

######################################################################
#----------------- Load feature importance data -----------------#
# Median of base testing AUC for each model
# 100 data-split permutation importances new testing AUC for top 5 features
######################################################################

source("../code/learning/load_imp_data.R")

######################################################################
#-------------------------- Load traintime data -----------------#
######################################################################
source("../code/learning/load_traintime.R")

######################################################################
#-------- Load AUROC performance data and Wilcoxon test ------------#

######################################################################
source("../code/learning/load_AUROC_data.R")
```

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Nicholas A. Lesniak${^1}$, Mack Ruffin${^3}$, Jenna Wiens${^2}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109

3\. Department of Family Medicine and Community Medicine, Penn State Hershey Medical Center, Hershey, PA




\newpage
\linenumbers


## Abstract

Machine learning (ML) modeling of the human microbiome has the potential to identify microbial biomarkers and aid in diagnosis of many diseases such as inflammatory bowel disease, diabetes, and colorectal cancer. Progress has been made towards developing ML models that predict health outcomes using bacterial abundances, but inconsistent adoption of training methods call the validity of these models into question. Furthermore, there appears to be a preference by many researchers to favor increased model complexity over interpretability. To overcome these challenges, we trained seven models that used fecal 16S rRNA sequence data to predict the presence of colonic screen relevant neoplasias (SRNs; n=490 patients, 261 controls and 229 cases). We developed a generalizable pipeline to train, validate and interpret the models. To show the effect of model selection, we assessed the predictive performance, interpretability, and computational efficiency of L2-regularized logistic regression, L1 and L2-regularized support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest, and extreme gradient boosting (XGBoost). The random forest model performed best at detecting SRNs with an AUROC of 0.695 but it was slow to train (83.2 h) and hard to interpret. Despite its simplicity, L2-regularized logistic regression followed random forest in predictive performance with an AUROC of 0.680, trained much faster (12 min), and was more interpretable. Our analysis showed that ML models should be chosen based on the goal of the study as the choice will inform expectations of performance and interpretability.

\newpage
## Importance 

Prediction of health outcomes using machine learning (ML) is rapidly being adopted in human microbiome studies. However, these ML models are likely overoptimistic in terms of quantifying predictive performance. Moreover, there is a trend towards using black box models such as random forest and neural networks without a discussion of the difficulty of interpreting such models when trying to identify microbial biomarkers of disease. This work represents a step towards developing better ML practices in microbiome research by implementing a rigorous pipeline and emphasizing the importance of selecting ML models that reflect the goal of the study. These concepts are not particular to the study of health outcomes but can also be applied to environmental microbiology studies.

\newpage


## Background

As the number of people represented in human microbiome datasets grow, there is an increasing desire to use microbiome data to diagnose diseases. However, the structure of the human microbiome is remarkably variable between individuals to the point where it is often difficult to identify the bacterial populations that are associated with diseases using traditional statistical models. This variation is likely due to the ability of many bacterial populations to fill the same niche such that different populations cause the same disease in different individuals. Furthermore, a growing number of studies have shown that it is rare for a single bacterial species to be associated with a disease. Instead, subsets of the microbiome account for differences in health. Traditional statistical approaches do not adequately account for the variation in the human microbiome and typically consider the protective or risk effects of each bacterial population individually. Recently, machine learning models (ML) have grown in popularity among microbiome researchers because of the large amount of data that can now be generated and because the models are effective at accounting for the interpersonal microbiome variation and the ecology of the disease.

ML models are useful for understanding the variation in the structure of existing data and to apply that knowledge to make predictions about new data. Researchers have used ML models to diagnose and understand the ecological basis of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases, obesity, and type 2 diabetes [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018; @thaiss_persistent_2016; @dadkhah_gut_2019; @flemer_oral_2018; @montassier_pretreatment_2016; @ai_systematic_2017; @dai_multi-cohort_2018; @ai_systematic_2017; @mossotto_classification_2017]. The task of diagnosing an individual with high confidence relies on a model that is built with rigorous methods. However, there are common methodological problems across many of these studies that need to be addressed as the field progresses. These include a lack of transparency in which methods are used and how these methods are implemented; developing and evaluating models without a separate held-out test data; large variation between the predictive performance on different folds of cross-validation; and large variation between cross-validation and testing performances. Nevertheless, the microbiome field is making progress to avoid some of these pitfalls including validating their models on independent datasets  [@wong_quantitation_2017; @mossotto_classification_2017; @sze_looking_2016] and introducing ways to better use ML tools  [@statnikov_comprehensive_2013; @knights_supervised_2011; @wirbel_meta-analysis_2019; @vangay_microbiome_2019]. More work is needed to further improve reproducibility and minimize over-optimism for model performance.

Among microbiome researchers, the lack of transparency in justifying a modelling approach has been due to an implicit assumption that more complex models are better because they are more complex. This has resulted in a trend towards using models such as random forest and neural networks [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017;  @geman_deep_2018; @zackular_human_2014] over simpler models such as logistic regression or other linear models [@mossotto_classification_2017; @wirbel_meta-analysis_2019; @thomas_metagenomic_2019]. Although the more complex models may be better at incorporating non-linear relationships or yield better predictions, they are considered to be black box models because they are not inherently interpretable. These models require post hoc explanations to quantify the importance of each feature in making a prediction and they do not show the structure of how the features are used. Depending on the application of the model, researchers may choose to use different modeling approaches. For example, researchers trying to identify the populations causing a disease would likely want a more interpretable model whereas clinicians may emphasize predictive performance. Although one may feel that they are sacrificing interpretability for performance, that tradeoff may be minimal [@rudin_please_2018; @rudin_optimized_2018]. Regardless, it is important for researchers to articulate why they have selected a specific modelling approach or even compare multiple approaches in the same study.

To showcase a rigorous ML pipeline and to shed light on how ML model selection can affect modeling results, we performed an empirical analysis comparing 7 modeling approaches with the same dataset and pipeline. We built three linear models with different forms of regularization: L2-regularized logistic regression and L1 and L2-regularized support vector machines (SVM) with a linear kernel. We also built four non-linear models: SVM with radial basis function kernel, a decision tree, random forest and XGBoost. We compared their predictive performance, interpretability, and computational efficiency. To demonstrate the performance of these modeling approaches and our pipeline, we used data from a previously published study that sought to classifiy individuals as having normal colons or colonic lesions based on the 16S rRNA gene sequences collected from fecal samples [@baxter_dna_2016]. This dataset was selected because it is a relatively large collection of individuals (N=490) connected to a clinically significant disease where there is ample evidence that the disease is driven by variation in the microbiome  [@baxter_microbiota-based_2016; @baxter_dna_2016; @zeller_potential_2014; @knights_human-associated_2011]. With this dataset we developed a framework that implements a ML pipeline that can be used for any modeling approach, evaluates predictive performance, and demonstrates how to interpret these models. This framework can be easily applied to other host-associated and environmental microbiome datasets.

## Results

__Model selection and pipeline construction__ We established a common ML pipeline to train and validate each of the seven models that is based on standard methods within the ML community (REFS by Jenna?)[Figure 1].

First, we randomly split the data into training and test sets so that the training set consisted of 80% of the full dataset while the test set was composed of the remaining 20% of the data [Figure 1]. To maintain the distribution of controls and cases that was found with the full dataset, we performed stratified splits. For example, our full dataset included 490 individuals. Of these, 261 had normal colons (53%) and 229 had a screen relevant neoplasia (SRN; 46.7%). A training set included 393 individuals, of which 209 had an SRN (53%), while the test set was composed of 97 individuals of which 52 had an SRN (54%). The training data was used to build the models and the test set was used for evaluating predictive performance.

Second, we trained seven different models using the training data [Table 1]. We selected models with different classification algorithms and regularization methods. Regularization is a technique that discourages overfitting by penalizing the model for learning the training data too well. For regularized logistic regression and SVM with linear kernel, we used L2-regularization to keep all potentially important features. For comparison, we also trained an L1-regularized SVM model with linear kernel. L1-regularization on microbiome data led to a sparser solution (i.e., forced many coefficients to zero). To explore the potential for non-linear relationships among features to improve classification, we trained tree-based models including decision tree, random forest, and XGBoost and we trained an SVM model with a non-linear kernel.

Third, fitting of these models required selecting appropriate hyperparameters. Hyperparameters are the rules that are learned from the training set in a classification algorithm. For example, in the linear models the regularization term (C) is a hyperparameter that indicates the penalty for overfitting. Similar to regularization term C, all hyperparameters are tuned to find the best model. We selected hyperparameters by performing 100 five-fold cross-validation (CV) repeats on the training set [Figure 1]. The five-fold CV was also stratified to maintain the overall case and control distribution. We chose the best hyperparameter values for each model based on its CV predictive performance using the area under the receiver operating characteristic curve (AUROC) metric [Figure S1 and S2]. The AUROC ranges from 1.0, where the model perfectly distinguishes between cases and controls, to 0.50, where the model’s predictions are no different from random chance. To select the best performing hyperparameter, we performed a full grid search for hyperparameter settings when training our models. Default hyperparameter settings in previously developed ML packages in R, Python, and Matlab programming languages are inadequate for effective application of classification algorithms and need to be optimized for each new ML task. In the example of L1-regularized SVM with linear kernel [Figure S1], the model showed large variability between different regularization coefficients (C) and was susceptible to performing poorly if the wrong regularization coefficient was assigned to the model by default.

Finally, we trained the full training dataset with the selected hyperparameter values and applied the model to the held-out data to evaluate the testing predictive performance of each model. The data-split, hyperparameter selection, training and testing steps were repeated 100 times to obtain a reliable and robust interpretation of model performance [Figure 1].


__Predictive performance and generalizability of the seven models.__  We evaluated the predictive performance of the seven models to classify individuals as having normal colons or SRNs [Figure 2]. The random forest model had significantly higher test AUROC values than the other models for detecting SRNs (Wilcoxon rank sum test, p < 0.01). The median AUROC of the random forest model was `r paste(round(median((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Random_Forest"))$test_aucs), 3))`). L2-regularized logistic regression, XGBoost, L2-regularized SVM with linear and radial basis function kernel AUROC values were not significantly different from one another and had median AUROC values of `r paste(format(round(median((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3), nsmall=3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="XGBoost"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="XGBoost"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))`) and  `r paste(round(median((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))`), respectively. L1-regularized SVM with linear kernel and decision tree had significantly lower AUROC values than the other ML models with median AUROC of `r paste(format(round(median((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3), nsmall=3))` (IQR `r paste(round(IQR((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))`) and `r paste(round(median((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))`), respectively [Figure 2]. Interestingly, these results demonstrate that the most complex model (XGBoost) did not have the best performance and that the most interpretable models (L2-regularized logistic regression and SVM with linear kernel) performed nearly as well as random forest.

To evaluate the generalizability of each model, we compared the median cross-validation AUROC to the median testing AUROC. If the difference between the cross-validation and testing AUROCs was large, then that would indicate that the models were overfit. The difference in median AUROCs was `r paste(round(abs(difference_model[difference_index[7],1]), 3))` in L1-regularized SVM with linear kernel, followed by SVM with radial basis function kernel and decision tree with a difference of `r paste(round(abs(difference_model[difference_index[6],1]), 3))` and `r paste(round(abs(difference_model[difference_index[5],1]), 3))`, respectively [Figure 2]. These differences are relatively small and would not indicate a problem with overfitting. 

To evaluate the risk for over-optimism of each model, we calculated the range of AUROC values for each model using 100 data-splits. The range among the testing AUROC values within each model varied by `r paste(format(round(mean((all %>% group_by(model) %>% summarise(difference=max(test_aucs)-min(test_aucs)))$difference), 3), nsmall=3))` on average across the seven models. If we had only done a single split, then there is a risk that we could gotten lucky or unlucky with the performance of the model. For instance, the lowest AUROC value of the random forest model was `r paste(round(min((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` whereas the highest was `r paste(format(round(max((all %>% filter(model=="Random_Forest"))$test_aucs), 3), nsmall=3))`. These results showed that depending on the data-split, the testing AUROC values showed great variability [Figure 2]. Therefore, it is important to employ the 100 hierarchical data splits that were included in our pipeline to minimize the risk of over-optimism.

To show the effect of sample size on model generalizability, we compared cross-validation AUROC values of L2-regularized logistic regression and random forest models when we subsetted our original study design with 490 subjects to 15, 30, 60, 120, and 245 subjects [Figure S3]. The range among the cross-validation AUROC values within both models at lower sample sizes were much larger than when the full collection of samples was used to train and validate the models. These results showed that because the microbiome data had many features (6920 OTUs), it was important to train the models using appropriate sample sizes to avoid problems with generalizability and over-optimism. Furthermore, it was encouraging that even for a small number of samples, the interquartile range included the median AUROC values for the larger subsetted datasets.

__Interpretation of each ML model.__ Interpretability is the degree to which humans can understand the reasons behind a model prediction [@miller_explanation_2017]. Because we often use ML models not just to predict a health outcome but also to identify the biomarkers for a disease, model interpretation becomes crucial for microbiome studies. ML models decrease in interpretability as they increase in complexity. In this study we used two methods to help interpret our models.

First, we interpreted the feature importance of the linear models (L1 and L2-regularized SVM with linear kernel and L2-regularized logistic regression) using the median rank of absolute feature weights for each OTU [Figure 3]. We also reviewed the signs of feature weights to determine whether an OTU was associated with classifying a subject as being healthy or having an SRN. It was encouraging that many of the highest ranked OTUs were shared across these three models, (e.g. OTU 50, 426, 609, 822, 1239). The benefit of this approach was that the results of the analysis were based on the trained model parameters and provided information regarding the sign and magnitude of the impact of each OTU. However, this approach is only possible with linear models.

Second, to analyze non-linear models we interpreted the feature importance using permutation importance. Whereas the absolute feature weights were determined from the trained models, here we measured importance using the held-out test data. Permutation importance analysis is a posthoc explanation of the model where we randomly permuted non-correlated features individually and groups of perfectly correlated features across the two groups in the held-out test data. We then calculated how much the predictive performance of the model (i.e testing AUROC values) decreased when each OTU or group of OTUs was randomly permuted. We ranked the OTUs based on how much the median testing AUROC decreased when it was permuted; the OTU with the largest decrease ranked highest [Figure 4]. Among the twenty OTUs with the largest impact, there was only one OTU (OTU 822) that was shared among all of the models; however, we found three OTUs (OTU 58, 110, 367) that were important in each of the tree-based models. Similarly, the random forest and XGBoost models, shared four of the most important OTUs (OTU 2, 12, 361, 477). Permutation analysis results also revealed that with the exception of the decision tree model, removal of any individual OTU had a minimal impact on model performance. For example, if OTU 367 was permuted across the samples in the decision tree model, the median AUROC dropped from `r paste(round(median((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` to `r paste(round(DT$imp, 3))`. In contrast, if the same OTU was permuted in the random forest model, the AUROC only dropped from `r paste(round(median((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` to `r paste(format(round(RF$imp, 3), nsmall=3))`. Effectively, the complexity of the communities was more fully represented in the better performing models [@sze_leveraging_2018; @wirbel_meta-analysis_2019]. At least in this case, it was not possible to distinguish between health and disease using a single OTU. Although permutation analysis allowed us to gauge the importance of an OTU, the analysis was post-hoc (i.e. done using the test data) and these results did not allow us to directly interrogate the models to know whether an OTU was associated with classifying a subject as being healthy or having an SRN.

To further highlight the differences between the two interpretation methods, we used permutation importance to interpret the linear models [Figure S4]. When we analyzed the L1-regularized SVM with linear kernel model using feature rankings based on weights [Figure 3] and permutation importance [Figure S4], 17 of the 20 top OTUs (e.g. OTU 609, 822, 1239) were deemed important by both interpretation methods. Similarly, for the L2-regularized SVM and L2-regularized logistic regression, 9 and 12 OTUs, respectively, were shared among the two interpretation methods. Although permutation analysis does not not allow us to determine the weight or the sign of the features, these results indicate that both methods are consistent in selecting the most important OTUs.

__The computational efficiency of each ML model.__ We compared the training times of the seven ML models. As expected, the training times increased with the complexity of the model and the number of tuned hyperparameter settings. Also, the linear models trained faster than non-linear models [Figures S1-S2; Figure 5]. When we subsetted the size of the training dataset, we observed a linear relationship between the size of the dataset and the training times for L2-regularized logistic regression and random forest models [Figure S5].


## Discussion

There is a growing awareness that many human diseases and environmental processes are not driven by a single organism but are the product of multiple bacterial populations. Traditional statistical approaches are useful for identifying those cases where a single organism is associated with a process. In contrast, ML methods offer the ability to incorporate the structure of the microbial communities as a whole to classify them into different categories such as coming from a patient who is healthy or has SRNs. If it is possible to classify communities reliably, then ML methods also offer the ability to identify those microbial populations within the communities that are responsible for the classification. However, the application of ML in microbiome studies is still in its infancy and the field needs to develop a better understanding of different ML methods, their strengths and weaknesses, and how to implement them.

To address these needs, we developed a framework to train rigorous, transparent, and reproducible models. We benchmarked seven ML models and showed that we can create models that are inherently interpretable and easily trained without losing predictive performance. In terms of predictive performance, the random forest model had the best testing AUROC values compared to the other six models. However, the second-best model was L2-regularized logistic regression with a median AUROC difference of only 0.015 compared to random forest. While random forest took 83.2 hours to train, L2-regularized logistic regression trained in 12 minutes. In terms of interpretability, random forest is a complex ML model and could only be explained using post-hoc methods such as permutation importance. On the other hand, L2-regularized logistic regression was easier to interpret by ranking the OTUs based on their feature weights and reviewing the signs of these weights. Comparing many different models showed us that the most complex model was not necessarily the best model for our ML task.

As we set out to select the best model, we established a pipeline that can be generalized to any modeling method that uses 16S rRNA sequence counts to predict a binary health outcome. We performed a random data-split to create a training set (80% of the data) and a held-out test set (20% of the data), which we used to evaluate predictive performance. We repeated this data-split 100 times to measure the possible variation in predictive performance. During the training, we tuned the model hyperparameters with a repeated five-fold cross-validation. Despite the high number of features microbiome datasets typically have, the models we built with this pipeline were generalizable as shown by the similar AUROC values from the cross-validation and testing.

We highlighted the importance of model interpretation to gain greater biological insights into microbiota-associated diseases. In this study we showcased two different interpretation methods: ranking each OTU by (i) their absolute weights in the trained models and (ii) their impact on the predictive performance based on permutation importance. Human-associated microbial communities have complex correlation structures which create collinearity in the datasets. This can hinder our ability to reliably interpret models because the feature weights of correlated OTUs are influenced by one another [@dormann_collinearity:_2013]. For example if one OTU is highly correlated with another OTU, only one of these OTUs would have a large feature weight thus hiding the importance of its correlated OTU. To capture all important features, once we identify highly ranked OTUs, we should review their relationships with other OTUs. These relationships will help us generate new hypotheses about the ecology of the disease and test them with follow-up experiments. When we used permutation importance, we took collinearity into consideration by grouping correlated OTUs to determine their impact as a group. We grouped OTUs that had a perfect correlation with each other however, we can reduce the correlation threshold to further investigate the relationships among features. It is important to know the correlation structures of the data to avoid misinterpreting the models. This is likely to be a particular problem with shotgun metagenomic datasets where collinearity will be more pronounced due to many genes being correlated with one another because they come from the same chromosome. To identify the true underlying microbial factors of a disease, it is crucial to do correlation analyses and further experimentation for biological validation.

In this study, we did not consider all possible modeling approaches. However, the principles highlighted throughout this study apply to other ML modeling tasks with microbiome data. For example, we did not evaluate multicategory classification methods to predict non-binary outcomes. We could have trained models to differentiate between people with normal colons and those with adenomas or carcinomas (k=3 categories). We did not perform this analysis because the clinically relevant diagnosis grouping was between patients with normal colons and those with SRNs. Furthermore, as number of categories to classify increase, more samples are required for each category to train a model. We also did not use regression-based analyses to predict a non-categorical outcome. We have previously used such an approach to train random forest models to predict fecal short-chain fatty acid concentrations based on microbiome data [@sze_fecal_2019]. Our analysis was also limited to shallow learning methods and did not explore deep learning methods such as neural networks. Deep learning methods hold great promise [@geman_deep_2018; @kocheturov_massive_2019; @kim_improved_2018] but microbiome datasets often suffer from having many features and small sample sizes, which makes the deep learning models prone to overfitting. These methods are even more complex than random forest and XGBoost and are considered uninterpretable. There is great potential for applying ML approaches to microbiome data and we can choose which ML approach is best for the study.

Our framework gives structure to investigators wanting to train, evaluate, and interpret their own ML models to identify OTUs that might be biologically relevant. However, deploying microbiome-based models to make clinical diagnoses or predictions is a significantly harder and distinct undertaking. For example, we currently lack standardized methods to collect patient samples, generate sequence data, and report clinical data. We are also challenged by the practical constraints of OTU-based approaches. The de novo algorithms commonly in use are slow, require considerable memory, and result in different OTU assignments as new data are added. Finally, we also need independent validation cohorts to test the performance of a diagnostic model. To realize the potential for using ML approaches with microbiome data, it is necessary that we direct our efforts to overcome these challenges.

Our study highlights the need to make educated choices at every step of developing a ML model with microbiome data. We created an aspirational rubric that researchers can use to identify potential pitfalls when using ML in microbiome studies and ways to avoid them [Table S1]. We have highlighted the trade-offs between model complexity and interpretability, the need for cross-validation to tune hyperparameters, the utility of held-out test sets for evaluating predictive performance, and the importance of considering correlation structures in datasets for reliable interpretation. Furthermore, we underscored the importance of proper experimental design and methods to help us achieve the level of validity and accountability we want from models built for patient health.


## Materials and Methods

**Data collection and study population.** The original stool samples described in our analysis were obtained from patients recruited by Great Lakes-New England Early Detection Research Network (Reference from Mack?). Stool samples were provided by adults who were undergoing a scheduled screening or surveillance colonoscopy. Participants were recruited from: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients’ colonic health was visually assessed by colonoscopy with bowel preparation and tissue histopathology of all resected lesions. We assigned patients into two classes: those with normal colons and those with screen relevant neoplasias (SRNs). The normal class included patients with normal colons or non-advanced adenomas whereas the SRN class included patients with advanced adenomas or carcinomas (reference for SRN?). Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as having advanced adenomas. There were 172 patients with normal colonoscopies, 198 with adenomas, and 120 with carcinomas. Of the 198 adenomas, 109 were identified as advanced adenomas. Together 261 patients were classified as normal and 229 patients were classified as having a SRN.

**16S rRNA gene sequencing data.** Stool samples provided by the patients were used for 16S rRNA gene sequencing to measure bacterial population abundances. The sequence data used in our analyses were originally generated by Baxter et al. (available through NCBI Sequence Read Archive [SRP062005], 2015). The OTU abundance table was generated by Sze et al [@sze_leveraging_2018], who processed the 16S rRNA sequences in mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH, and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016]; (\url{https://github.com/SchlossLab/Sze_CRCMetaAnalysis_mBio_2018/blob/master/data/process/baxter/baxter.0.03.subsample.shared}). These OTU abundances were the features we used to predict colorectal health of the patients. There were 6920 OTUs. OTU abundances were subsampled to the size of the smallest sample and normalized across samples such that the highest abundance of each OTU would be 1 and lowest would be 0.


**Model training and evaluation.** Models were trained using the caret package (v.6.0.81) in R (v.3.5.0). We modified the caret code to calculate decision values instead of predicted probabilities for models generated using L2-regularized SVM with linear kernel and L1-regularized SVM with linear kernel. These changes were necessary to calculate AUROC values for SVMs. The code for these changes on L2-regularized SVM with linear kernel and L1-regularized SVM with linear kernel models are available at \url{https://github.com/SchlossLab/Topcuoglu_ML_XXX_2019/blob/master/data/caret_models/svmLinear3.R} and at \url{https://github.com/SchlossLab/Topcuoglu_ML_XXX_2019/blob/master/data/caret_models/svmLinear4.R}, respectively.

For hyperparameter selection, we started with a granular grid search. Then we narrowed and fine-tuned the range of each hyperparameter. A full grid search was needed to avoid large variation in prediction performance. For L2-regularized logistic regression, L1 and L2-regularized SVM with linear and radial basis function kernels, we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For XGBoost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners. Recently developed tools such as Hyperband help researchers with hyperparameter selection that can be incorporated to microbiome studies [@li_hyperband:_2016] .

The computational burden during model training due to model complexity was reduced by parallelizing segments of the ML pipeline. We parallelized the training of each data-split. This allowed the 100 data-splits to be processed through the ML pipeline simultaneously at the same time for each model. It is possible to further parallelize the cross-validation step for each hyperparameter setting if limited by computational resources.

**Permutation importance workflow.** We calculated a Spearman's rank-order correlation matrix and defined correlated OTUs as having perfect correlation (correlation coefficient = 1 and p < 0.01). Non-correlated OTUs were permuted individually whereas correlated ones were grouped together and permuted at the same time.

**Statistical analysis workflow.** Data summaries, statistical analysis, and data visualizations were performed using R (v.3.5.0) with the tidyverse package (v.1.2.1). We compared the AUROC values of the seven ML models by Wilcoxon rank sum tests to determine the best predictive performance.

**Code availability.** The code for all sequence curation and analysis steps including an Rmarkdown version of this manuscript is available at \url{https://github.com/SchlossLab/Topcuoglu_ML_XXX_2019/}.

\newpage
\includegraphics{Figure_1}
**Figure 1. Machine learning pipeline.  **   We split the data to create a training (80%) and held-out test set (20%). The splits were stratified to maintain the overall label distribution. We performed five-fold cross-validation on the training data to select the best hyperparameter setting and then used these hyperparameters to train the models. The model was evaluated on the held-out data set. Abbreviations: cvAUC, cross-validation area under the receiver operating characteristic curve.
\newpage
\includegraphics{Figure_2.png}

**Figure 2. Generalization and classification performance of ML models using AUROC values of all cross validation and testing performances. ** The median AUROC for diagnosing individuals with SRN using bacterial abundances was higher than chance (depicted by horizontal line at 0.50) for all the ML models. Predictive performance of random forest model was higher than other ML models. The boxplot shows quartiles at the box ends and the median as the horizontal line in the box. The whiskers show the farthest points that were not outliers. Outliers were defined as those data points that are not within 1.5 times the interquartile ranges. 
\newpage
\includegraphics[height=17cm, width=12cm]{Figure_3.png}

**Figure 3. Interpretation of the linear ML models.** The absolute feature weights of (A) L2-regularized logistic regression, (B) L1-regularized SVM with linear kernel, and (C) L2-regularized SVM with linear kernel were ranked from highest rank, 1, to lowest rank, 100, for each data-split. The feature ranks of the 20 highest ranked OTUs based on their median ranks (median shown in black) are reported here.  OTUs that are associated with classifying a subject as being healthy had negative signs and were shown in blue. OTUs that are associated with classifying a subject having an SRN had positive signs and were shown in red. Some of the same OTUs were identified as important in all of the linear models. 
\newpage
\includegraphics{Figure_4.png}

**Figure 4. Interpretation of the non-linear ML models.** (A) SVM with radial basis kernel, (B) decision tree, (C) random forest, and (D) XGBoost feature importances were explained using permutation importance on the held-out test data set. The gray rectangle and the dashed line show the IQR range and median of the base testing AUROC without any permutation. The colors of the box plots represent the OTUs that were shared among the different models; yellow were OTUs that were shared among all the non-linear models, salmon were OTUs that were shared among the tree-based models, green were the OTUs shared among SVM with radial basis kernel, decision tree and XGBoost, pink were the OTUs shared among SVM with radial basis kernel and XGBoost only, red were the OTUs shared among random forest and XGBoost only and blue were the OTUs shared among decision tree and random forest only. For all of the tree-based models, a Peptostreptococcus species (OTU00367) had the largest impact on predictive performance.
\newpage
\includegraphics{Figure_5.png}

**Figure 5. Computational efficiency of seven ML models.** The median training time was the highest for XGBoost and shortest for L2-regularized logistic regression. 
\newpage
\includegraphics{Figure_S1.png}

**Figure S1. Hyperparameter setting performances for linear models.** (A) L2 logistic regression, (B) L1 SVM with linear kernel, and (C) L2 SVM with linear kernel mean cross-validation AUROC values when different hyperparameters were used in training the model. The differences in AUROC values when hyperparameters change show that hyperparameter tuning is a crucial step in building a ML model.

\newpage
\includegraphics[height=30cm, width=15cm]{Figure_S2.png}

**Figure S2. Hyperparameter setting performances for non-linear models.** (A) Decision tree, (B) Random forest, (C) SVM with radial basis kernel, and (D) XGBoost mean cross-validation AUROC values when different hyperparameters were used in training the model. The differences in AUROC values when hyperparameters change show that hyperparameter tuning is a crucial step in building a ML model. 
\newpage
\includegraphics[height=17.5cm, width=13cm]{Figure_S3.png}

**Figure S3. Classification performance of ML models across cross validation when dataset is subsetted .** (A) L2-regularized logistic regression and (B) Random forest models were trained using the original study design with 490 subjects and subsets of it with 15, 30, 60, 120, and 245 subjects. The range among the cross-validation AUROC values within both models at lower sample sizes were much larger than when the full collection of samples was used to train and validate the models, but included the ranges observed with the more complete datasets.

\newpage
\includegraphics[height=17.5cm, width=13cm]{Figure_S4.png}

**Figure S4. Interpretation of the linear ML models with permutation importance.** (A) L1-regularized SVM with linear kernel, (B) L2-regularized SVM with linear kernel, and (C) L2-regularized logistic regression were interpreted using permutation importance using held-out test set. 

\newpage
\includegraphics[height=17.5cm, width=13cm]{Figure_S5.png}

**Figure S5. Training times of ML models when dataset is subsetted .** (A) L2-regularized logistic regression and (B) Random forest models were trained using the original study design with 490 subjects and subsets of it with 15, 30, 60, 120, and 245 subjects. As the size of the dataset increased, the training times for L2-regularized logistic regression and random forest models increased linearly.


\newpage
\captionsetup{labelformat=empty}
\captionof{table}{\textbf{Table 1.} Characteristics of the machine learning models in our comparative study.}
\small
\begin{tabular}{|l|l|l|l|l|}
\hline

\rowcolor{lightgray}
\textbf{Model} & \textbf{Description} & \textbf{Linearity} & \textbf{Interpretability} \\ \hline

\makecell[l]{Logistic \\regression} & \makecell[l]{A predictive regression analysis when the dependent \\variable is binary.} & Linear & Interpretable \\ \hline

\makecell[l]{SVM with \\linear kernel} & \makecell[l]{A classifier that is defined by an optimal linear \\separating hyperplane that discriminates between labels.} & Linear & Interpretable \\ \hline

\makecell[l]{SVM with \\radial basis kernel} & \makecell[l]{A classifier that is defined by an optimal Gaussian \\separating hyperplane that discriminates between labels.} & Non-linear & Explainable$^*$ \\ \hline
Decision tree & \makecell[l]{A classifier that sorts samples down from the
root to the \\leaf node where an attribute is tested to discriminate \\between labels.} & Non-linear & Interpretable \\ \hline

Random forest & \makecell[l]{A classifier that is a decision tree ensemble that \\grow randomly with subsampled data.} & Non-linear & Explainable$^*$ \\ \hline

XGBoost & \makecell[l]{A classifier that is a decision tree ensemble that \\grow with additive training.} & Non-linear & Explainable$^*$ \\ \hline

\end{tabular}
\begin{tablenotes}\footnotesize
\item[1]{$^*$Explainable models are not inherently interpretable but can be explained with post-hoc analyses.}
\end{tablenotes}
\newpage




\captionof{table}{\textbf{Table S1.} An aspirational rubric for evaluating the rigor of ML practices.}
\small
\begin{tabular}{|l|l|l|l|}
\hline

\rowcolor{lightgray}
\textbf{Practice} & \textbf{Good} & \textbf{Better} & \textbf{Best} \\ \hline

\makecell[l]{Problem \\ definition} & \makecell[l]{Have we clearly stated \\ the ML task? \\ Do we have a priori hypotheses? \\Do we know the predictions \\ a domain expert would \\ make manually?} & \makecell[l]{Do we know the motivation \\ for solving the problem? \\ How much interpretability \\ does the problem need? } & \makecell[l]{Do we know our data? \\ Do we know the correlated \\ variables? } \\ \hline

\makecell[l]{Model \\ selection} & \makecell[l]{Do we know the \\ candidate algorithms \\ for the ML problem? \\} & \makecell[l]{Do we know our \\ computational resources \\ to fully train each model?} & \makecell[l]{How much interpretability \\ does the problem need? \\ How much each \\ candidate algorithm \\ can provide?} \\ \hline

\makecell[l]{ML pipeline \\ preparation} & \makecell[l]{Did we do \\ cross-validation} & \makecell[l]{Do we have an held-out \\ test dataset? \\ Have we tuned our model \\ hyperparameters in \\ cross-validation?} & \makecell[l]{Have we tested our model \\ on many different \\  datasets?} \\ \hline

\makecell[l]{Hyperparameter \\ selection} & \makecell[l]{Do we know the different \\ hyperparameters each model \\ can use and why?} & \makecell[l]{Did we use historically \\ effective hyperparameters? } & \makecell[l]{Did we search the \\ full grid space and \\optimized our model? } \\ \hline

\makecell[l]{Model \\ evaluation} & \makecell[l]{Have we chosen an appropriate \\ metric to evaluate \\ predictive performance?} & \makecell[l]{Have we reported \\ the predictive performance \\ on a held-out test data?} & \makecell[l]{Have we provided an average \\ predictive performance \\ of many model runs? } \\ \hline

\makecell[l]{Model \\ interpretation} & \makecell[l]{Do we know if \\ our model is \\ interpretable?} & \makecell[l]{If the model is not \\ interpretable, do we know \\how to  explain it? \\ Have we checked for \\ the effect of correlated \\ variables?} & \makecell[l]{Have we generated new \\ hypotheses based on \\ model interpretation \\ to test model results? } \\ \hline

\end{tabular}
\newpage


## References

<div id="refs"></div>
