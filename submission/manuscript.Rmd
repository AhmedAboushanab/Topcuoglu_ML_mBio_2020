---
title: "**Evaluation of machine learning methods for 16S rRNA gene data**"
bibliography: references.bib
output:
  pdf_document: 
    keep_tex: true
    includes:
      in_header: header.tex
csl: mbio.csl #Get themes at https://github.com/citation-style-language/styles
fontsize: 11pt
geometry: margin=1.0in
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
# Load in needed functions and libraries
melt_data <-  function(data) {
  data_melt <- data %>%
    melt(measure.vars=c('cv_aucs', 'test_aucs')) %>%
    rename(AUC=value) %>%
    mutate(Performance = case_when(variable == "cv_aucs" ~ 'cross-validation', variable == "test_aucs" ~ 'testing')) %>%
    group_by(Performance)
  return(data_melt)
}

# Read in files as delim that are saved in a list with a pattern
read_files <- function(filenames){
  for(file in filenames){
    # Read the files generated in main.R
    # These files have cvAUCs and testAUCs for 100 data-splits
    data <- read.delim(file, header=T, sep=',')
  }
  return(data)
}


# Get model name with sub from file name
get_model_name <- function(files){
  pat1 <- "../data/process/walltime_"
  name_files <- sub(pat1, "", files)
  pat2 <- ".csv"
  names <- sub(pat2, "", name_files)
  return(names)
}
######################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################

# Read in metadata and select only sample Id and diagnosis columns
meta <- read.delim('../data/metadata.tsv', header=T, sep='\t') %>%
  select(sample, dx, Dx_Bin, fit_result)


# Read in OTU table and remove label and numOtus columns
shared <- read.delim('../data/baxter.0.03.subsample.shared', header=T, sep='\t') %>%
   select(-label, -numOtus)

# Merge metadata and OTU table.
# Group advanced adenomas and cancers together as cancer and normal, high risk normal and non-advanced adenomas as normal
# Then remove the sample ID column
data <- inner_join(meta, shared, by=c("sample"="Group")) %>%
  mutate(dx = case_when(
    Dx_Bin== "Adenoma" ~ "normal",
    Dx_Bin== "Normal" ~ "normal",
    Dx_Bin== "High Risk Normal" ~ "normal",
    Dx_Bin== "adv Adenoma" ~ "cancer",
    Dx_Bin== "Cancer" ~ "cancer"
  )) %>%
  select(-sample, -Dx_Bin) %>%
  drop_na()
# We want the diagnosis column to be a factor
data$dx <- factor(data$dx)

######################################################################
#--------- Load .csv datato get mean test AUC for 7 models--------#
######################################################################

# Read in the cvAUCs, testAUCs for 100 splits.
best_files <- list.files(path= '../data/process', pattern='combined_best.*', full.names = TRUE)
logit <- read_files(best_files[4])
l2svm <- read_files(best_files[3])
l1svm <- read_files(best_files[2])
rbf <- read_files(best_files[6])
rf <- read_files(best_files[5])
dt <- read_files(best_files[1])
xgboost <- read_files(best_files[7])

# Get the The unpaired two-samples Wilcoxon test to see if models differ from one another signigicantly

rf_xgboost <- wilcox.test(rf$test_aucs, xgboost$test_aucs)
rf_l1svm <- wilcox.test(rf$test_aucs, l1svm$test_aucs)
l1svm_dt <- wilcox.test(l1svm$test_aucs, dt$test_aucs)
l1svm_rbf <- wilcox.test(l1svm$test_aucs, rbf$test_aucs)
l1svm_l2svm <- wilcox.test(l1svm$test_aucs, l2svm$test_aucs)
l1svm_logit <- wilcox.test(l1svm$test_aucs, logit$test_aucs)

# Get the unpaired two-samples Wilcoxon test to see if cv and test differ significantly
logit_diff <- wilcox.test(logit$test_aucs, logit$cv_aucs)
l2svm_diff <- wilcox.test(l2svm$test_aucs, l2svm$cv_aucs)
rbf_diff <- wilcox.test(rbf$test_aucs, rbf$cv_aucs)

# Bind them together and summarise mean testAUC by model
test_all <- bind_rows(logit, l2svm, rbf, dt, xgboost, rf, l1svm)%>%
  melt_data() %>% 
  select(-variable) %>% 
  group_by(model, Performance) %>% 
  summarise(mean_AUC = mean(AUC), sd_AUC = sd(AUC)) %>% 
  filter(Performance=="testing") 

# Get the order index from small to large meanAUC
performance_index <- order(test_all$mean_AUC)

# Bind them together and summarise mean cvAUC by model
cv_all <- bind_rows(logit, l2svm, rbf, dt, xgboost, rf, l1svm)%>%
  melt_data() %>% 
  select(-variable) %>% 
  group_by(model, Performance) %>% 
  summarise(mean_AUC = mean(AUC), sd_AUC = sd(AUC)) %>% 
  filter(Performance=="cross-validation") 

# Get the difference between mean cvAUC and testAUC for each model
difference <- cv_all$mean_AUC - test_all$mean_AUC[match(cv_all$model, test_all$model)]
difference_model <- data.frame(difference, test_all$model)
# Get the order index from small to large of the differences
difference_index <- order(difference_model$difference)

######################################################################
#---------- Load .csv datato get mean walltime for 7 models-------#
######################################################################

# Read in the walltime for each split.
walltime_files <- list.files(path= '../data/process', pattern='walltime*', full.names = TRUE) 

# ------- 1. In a loop read the files as delim
# --------2. get the model name for each with get_model_name() 
#---------3. Add a column with model name to each delim
result <- list()

for(file in walltime_files){
  model_walltime <- read_files(file) %>%  
  #summarise_walltime() %>% 
  mutate(model=get_model_name(file))
  result[[length(result)+1]] <- model_walltime
}


# ------- 1. If models are L1 or L2 Linear SVM or L2 Logit then the walltime is in minutes
# --------2. If models are Random Forest or XGBoost then the walltime is in days
#---------3. If other models, the walltime is in hours.
#---------4. This loop converts all to hours.
min_fixed_result <- list()
for(i in result){
  i$x <- i$x/3600 # The walltimes were saved as seconds we covert to hours
  min_fixed_result[[length(min_fixed_result)+1]] <- i
}
# Bind all delims with model name and walltime together
# Summarise mean walltime
walltime_df <- bind_rows(min_fixed_result) %>% 
  group_by(model) %>% 
  summarise(mean_walltime=mean(x), sd_walltime=sd(x))

# Get the sorted mean walltime for each model small to large
walltime_index <- order(walltime_df$mean_walltime) 
  

```

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Jenna Wiens${^2}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109


\newpage
\linenumbers


## Abstract


\newpage

## Introduction

Advances in sequencing technology and decreasing costs of generating 16S rRNA gene sequences have allowed rapid exploration of human associated microbiome and its health implications. Currently, the human microbiome field is growing at an unprecedented rate and as a result, there is an increasing demand for methods that identify associations between members of the microbiome and human health. However, this is an undertaking as human associated microbial communities are remarkably complex and uneven. It is unlikely that a single species can explain a disease. Instead, subsets of those communities, in relation to one another and to their host, account for the differences in health outcomes. Machine learning (ML) methods are effective at recognizing and highlighting patterns in complex microbial datasets. Therefore, researchers have started to explore the utility of ML models that use microbiota associated biomarkers to predict human health and to understand the microbial ecology of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases (IBD), obesity, and type 2 diabetes [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018].  However, currently the field's use of ML lacks clarity and consistency on which methods are used and how these methods are implemented. Moreover, there is a lack of deliberation on why a particular ML model is utilized. Recently, there is a trend towards using more complex ML models such as random forest, extreme gradient boosting and neural networks without a discussion on if and how much model interpretibility is necessary for the study [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017; @ @geman_deep_2018]. The lack of transparency on modeling methodology and model selection negatively impact model reproducibility and reliability. We need to strive toward better machine learning practices by (1) implementing consistent, accessible and transparent machine learning methods; (2) selecting ML models that reflect the goal of the study as it will inform our expectations of model accuracy, complexity, interpretibility and computational efficiency.

To showcase transparent ML methodologies and to shed light on how much ML model selection can affect modeling results, we performed an empirical analysis comparing several different ML models using the same dataset. We used a previously published colorectal cancer (CRC) study [@baxter_dna_2016] which had fecal 16S rRNA gene sequences and human hemoglobin concentrations from `r paste(nrow(shared))` patients. We built ML models using fecal 16S rRNA gene sequences and human hemoglobin concentrations to predict patients with normal colons or patients with colonic tumors which are called screen relevant neoplasias (SRN). The study had `r paste(sum(sum(meta$dx=="normal")+sum(meta$Dx_Bin=="Adenoma")))` normal and `r paste(sum(sum(meta$dx=="cancer")+sum(meta$Dx_Bin=="adv Adenoma")))` SRN samples. We established modeling pipelines for L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost which are ML models that increase in complexity respectively. Our ML pipeline utilized held-out test data to evaluate generalization and prediction performance of each ML model. The mean test AUROC varied from `r paste(round(test_all[performance_index[1],3], 2))` (std ± `r paste(round(test_all[performance_index[1],4], 2))`) to `r paste(round(test_all[performance_index[7],3], 2))` (std ± `r paste(round(test_all[performance_index[7],4], 2))`). Random Forest and XGBoost had the highest mean AUROC for detecting SRN. Despite the simplicity, the L1-regularized linear kernel SVM followed Random Forest and XGBoost in performance. In terms of computational efficiency, L2 logistic regression trained the fastest (`r paste(round(walltime_df[walltime_index[1],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[1],3], 3))`), while XGBoost took the longest (`r paste(round(walltime_df[walltime_index[7],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[6],3], 3))`). We found that mean cross-validation and testing AUROC varied only `r paste(round(difference[difference_index[7]], 3))`, which highlights the importance of a seperate held-out test set and consistent preprocessing/splitting data for evaluation. Aside from evaluating generalization and classification performances for each of these models, this study established standards for modeling pipelines of microbiome-associated machine learning models.

## Results 

__Model selection and construction__

The data has `r paste(ncol(data)-1)` features (`r paste(ncol(data)-2)` OTUs and FIT) and a two-class label that defines the colonic health of the patients (SRN or normal). We established modeling pipelines for binary classification with L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost to emphasize the differences in  model accuracy, complexity, interpretibility and computational efficiency due to  model selection. We randomly split the data into stratified training/validation and test sets 100 times with a 80/20 proportion [Figure 1]. For each data-split, training/validation set consisted of 393 patients (209 SRN), while the test set was composed of 97 patients (52 SRN). The training/validation data was used for training purposes and validation of hyperparameter selection, and the test set was used for evaluation purposes. Validation of hyperparameter selection was performed by randomly splitting the training/validation set into stratified training and validation sets 100 times with a 80/20 proportion [Figure 1]. For L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For xgboost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners. We validated the prediction performances of each hyperparameter setting (Figure S1]. 

__The prediction and generalization performance of classifiers during cross-validation and when applied to the held-out test data.__

We evaluated the prediction performance of seven binary classification models when applied to held-out test data over 100 data-splits using AUROC as classification performance metric. Random Forest and XGBoost had the highest mean AUROC for detecting SRN, `r paste(round(mean(rf$test_aucs), 3))` (std ± `r paste(round(sd(rf$test_aucs), 3))`) and `r paste(round(mean(xgboost$test_aucs), 3))` (std ± `r paste(round(sd(xgboost$test_aucs), 3))`) respectively [Figure 2]. L1 linear SVM had significantly lower AUROC values, `r paste(round(mean(l1svm$test_aucs), 3))` (std ± `r paste(round(sd(l1svm$test_aucs), 3))`) and `r paste(round(mean(dt$test_aucs), 3))` (std ± `r paste(round(sd(dt$test_aucs), 3))`)  [Figure 2]. However, it also had significanly higher performances than L2 linear SVM, RBF SVM, L2 logistic regression and decision tree which had mean AUROC values of `r paste(round(mean(l2svm$test_aucs), 3))` (std ± `r paste(round(sd(l2svm$test_aucs), 3))`),  `r paste(round(mean(rbf$test_aucs), 3))` (std ± `r paste(round(sd(rbf$test_aucs), 3))`), `r paste(round(mean(logit$test_aucs), 3))` (std ± `r paste(round(sd(logit$test_aucs), 3))`) and `r paste(round(mean(dt$test_aucs), 3))` (std ± `r paste(round(sd(dt$test_aucs), 3))`), respectively [Figure 2]. We also evaluated the generalization performance of each classifier by comparing their mean cross-validation AUROC and mean testing AUROC. We found that mean cross-validation and testing AUROC difference for each model was below 0.01. 

__The complexity and interpretibility of each classifier.__

We interpreted the feature weights of L1 and L2 SVM with linear kernel and regression coefficients of L2 logistic regression using the training data. To get a sense of the most important features in linear models, we calculated the mean weights of all the features over the 100 data-splits for each model. [Figure 4]. In the three linear models, FIT was the feature that drove the detection of SRNs, followed by eight OTUs which belong to family *Lachnospiraceae* and *Ruminococcaceae* (four with positive signs; Otu01239, Otu00742, Otu00008 Otu00659, and four with negative signs; Otu00015, Otu000150, Otu00609, Otu00629). To get the importance of features in a decision tree, we used gini importance which allows us to use the improvement in the split-criterion is the importance measure attributed to the splitting variable


__The computational efficiency of each classifier.__

Linear models trained faster than non-linear models. L2 logistic Rregression and L1 and L2 SVM with linear kernel had training times of `r paste(60*(round(walltime_df[4,2], 2)))` min, (std ± `r paste(60*(round(walltime_df[4,3], 2)))`), `r paste(60*(round(walltime_df[2,2], 2)))` min, (std ± `r paste(60*(round(walltime_df[2,3], 2)))`) and `r paste(60*(round(walltime_df[3,2], 2)))` min, (std ± `r paste(60*(round(walltime_df[3,3], 2)))`) respectively. Whereas, SVM with radial basis function kernel, decision tree, random forest and xgboost had training times of `r paste(round(walltime_df[6,2], 2))` hrs, (std ± `r paste(round(walltime_df[6,3], 2))`), `r paste((round(walltime_df[1,2], 2)))` hrs, (std ± `r paste((round(walltime_df[1,3], 2)))`), `r paste(round(walltime_df[5,2], 2))` hrs, (std ± `r paste(round(walltime_df[5,3], 2))`) and `r paste(round(walltime_df[7,2], 2))` hrs, (std ± `r paste(round(walltime_df[7,3], 2))`), respectively [Figure 4].






## Conclusions



## Materials and Methods

#### Data collection
  The data used for this analysis are stool bacterial abundances, stool hemoglobin levels and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic disease status was defined by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for Fecal Immunological Tests (FIT) which measure human hemoglobin concentrations and for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

#### Data definitions and pre-processing

  The colonic disease status is re-defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. Colonic disease status is the label predicted with each classifier.   The bacterial abundances and FIT results are the features used to predict colonic disease status. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. There are 6920 OTUs for each sample. FIT levels are continuous data present for each sample. Because the data are in different scales, features are transformd by scaling each feature to a [0-1] range (Table 1). 
  
#### Learning the Classifier

  To train and validate our model, labeled data is randomly split 80/20 into a training set and testing
set. Then, seven binary class classifiers, L2 logistic regression, L1 and L2 linear suppor vector machines (SVM), radial basis function SVM, decision tree, random forest and XGBoost, are learned. The training set is used for training purposes and validation of hyperparameter selection, and the test set is used for evaluation purposes. Hyperparameters are selected using 5-fold cross-validation with 100-repeats on the training set. Since the colonic disease status are not uniformly represented in the data, 5-fold splits are stratified to maintain the overall label distribution on the training set. 

#### Classifier Performance

  The classification performance of learned classifier is evaluated on the labeled held-out testing set. The optimal  classifier with optimal hyperparameters selected in the cross-validation step is used to produce a prediction for the testing set. The performance of this prediction is measured in terms of the sensitivity and specificity, in addition to Area Under the Curve (AUC) metrics. This process of splitting the data, learning a classifier with cross-validation, and testing the classifier is repeated on 100 different splits. In the end cross-validation AUC and testing AUC averaged over the 100 different training/test splits are reported. Hyperparameter budget and performance for each split is also reported. 


\newpage



**Figure 1. Generalization and classification performance of modeling methods ** AUC values of all cross validation and testing performances. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 

\newpage

## References

<div id="refs"></div>


