---
title: '**Evaluation of machine learning methods for 16S rRNA gene data**'
csl: mbio.csl
fontsize: 11pt
output:
  pdf_document:
    includes:
      in_header: header.tex
    keep_tex: yes
  word_document: default
geometry: margin=1.0in
bibliography: references.bib
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in important functions -----------------#
######################################################################
source("../code/learning/functions.R")

#####################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################
source("../code/learning/load_data.R")

######################################################################
#----------------- Load feature importance data -----------------#
# Median of base testing AUC for each model
# 100 datasplit permutation importances new testing AUC for top 5 features
######################################################################

source("../code/learning/load_imp_data.R")

######################################################################
#-------------------------- Load traintime data -----------------#
######################################################################
source("../code/learning/load_traintime.R")

######################################################################
#-------- Load AUROC performance data and Wilcoxon test ------------#

######################################################################
source("../code/learning/load_AUROC_data.R")
```
\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Nick Lesniak${^1}$, Jenna Wiens${^2}$, Mack Ruffin${^3}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109

3\. Department of Family Medicine and Community Medicine, Penn State Hershey Medical Center, Hershey, PA




\newpage
\linenumbers


## Abstract

Machine learning (ML) modeling of the human microbiome has the potential to identify the microbial biomarkers and aid in diagnosis of many chronic diseases such as inflammatory bowel disease, diabetes and colorectal cancer. Progress has been made towards developing ML models that predict health outcomes from bacterial abundances, but rigourous ML models are scarce in part due to the flawed modeling methods that call into question the validity of developed ML models. Furthermore, using black box ML models has hindered the validation of microbial biomarkers. To overcome these challenges, we benchmarked seven different ML models that use fecal 16S rRNA sequences to predict the presence/absence of colorectal cancer (CRC) lesions (n=490 patients, 261 controls and 229 cases). To show the effect of model selection, we assessed the accuracy, interpretability, and computational efficiency of the following models: L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest, and extreme gradient boosting (XGBoost). The random forest model was best at detecting CRC lesions with an AUROC of 0.695 but it was slow to train (101.3 h) and hard to interpret. Despite its simplicity, L2-regularized logistic regression followed random forest in predictive performance with an AUROC of 0.680, and it trained much faster (12 min). This study showed that we should choose ML models based on our expectations of performance and interpretability as well as our  computational resources. It also established standards for modeling pipelines of microbiome-associated ML models.


## Importance

Prediction of health outcomes using ML is rapidly being adopted by human-associated microbiome studies. However, the results so far are overoptimistic both in terms of prediction performance and validity of developed ML models. Without rigorous ML pipelines, we cannot trust ML models. Before we can speed up progress, we need to slow down, define and start using good ML practices.

\newpage

## Background

Advances in sequencing technology and decreasing costs of generating 16S rRNA gene sequences have allowed rapid exploration of human associated microbiome and its health implications. Currently, the human microbiome field is growing at an unprecedented rate and as a result, there is an increasing demand for methods that identify associations between members of the microbiome and human health. However, this is difficult as human associated microbial communities are remarkably complex, high-dimensional and uneven within and between individuals with the same disease. It is unlikely that a single species can explain a disease. Instead, subsets of those communities, in relation to one another and to their host, account for differences in health outcomes. 

Machine learning (ML) methods are effective at recognizing and highlighting patterns in complex microbial datasets. Therefore, researchers have started to explore the utility of ML models that use microbiota associated biomarkers to predict human health and to understand the microbial ecology of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases (IBD), obesity, type 2 diabetes and others [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018].  However, currently the field's use of ML lacks clarity and consistency on which methods are used and how these methods are implemented [@thaiss_persistent_2016; dadkhah_gut_2019]. More notably, ML practices such as using ML pipelines where there is no seperate held-out test dataset to evaluate model performance and reporting few or only the best outcomes of different randomizations of cross-validation are commonly used. When there are seperate testing sets to evaluate model performance, there are large differences between cross-validation and testing performances as well as large confidence intervals of testing performances [@flemer_oral_2018; @baxter_microbiota-based_2016; @dai_multi-cohort_2018; @montassier_pretreatment_2016; @papa_non-invasive_2012; @mossotto_classification_2017; @ai_systematic_2017; @wong_quantitation_2017]. Moreover, there is a lack of discussion on why a particular ML model is utilized. Recently, there is a trend towards using more complex ML models such as random forest, extreme gradient boosting and neural networks without a discussion on if and how much model interpretibility is necessary for the study [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017; @ @geman_deep_2018]. The lack of transparency on modeling methodology and model selection negatively impact model reproducibility and reliability. We need to strive toward better machine learning practices by (1) implementing consistent and rigourous machine learning pipelines and (2) selecting ML models that reflect the goal of the study as it will inform our expectations of model accuracy, complexity, interpretibility and computational efficiency.

To showcase a reliable ML pipeline and to shed light on how much ML model selection can affect modeling results, we performed an empirical analysis comparing several different ML models using the same dataset and with a robust ML pipeline. We used a previously published colorectal cancer (CRC) study [@baxter_dna_2016] which had fecal 16S rRNA gene sequences from `r paste(nrow(shared))` patients. CRC is the third most common form of cancer worldwide in which the human-associated human microbiome is hypothesized to directly contribute to its development. We built ML models using fecal 16S rRNA gene sequences to predict patients with normal colons or patients with colonic tumors which are called screen relevant neoplasias (SRN). The study had `r paste(sum(sum(meta$dx=="normal")+sum(meta$Dx_Bin=="Adenoma")))` normal and `r paste(sum(sum(meta$dx=="cancer")+sum(meta$Dx_Bin=="adv Adenoma")))` SRN samples. We established modeling pipelines for L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost. Our ML pipeline utilized held-out test data to evaluate generalization performance of each ML model. The median test AUROC varied from `r paste(round(test_all[performance_index[1],3], 3))`  to `r paste(round(test_all[performance_index[7],3], 3))`. Random forest had the highest median AUROC for detecting SRN. Despite its simplicity, the L2-regularized logistic regression followed random forest in performance. In terms of computational efficiency, L1 SVM with linear kernel trained the fastest (`r paste(round(traintime_df[traintime_index[1],2], 3))` hours, std ± `r paste(round(traintime_df[traintime_index[1],3], 3))`), while XGBoost took the longest (`r paste(round(traintime_df[traintime_index[7],2], 3))` hours, std ± `r paste(round(traintime_df[traintime_index[7],3], 3))`). We found that median cross-validation and testing AUROC varied `r paste(round(abs(difference[difference_index[7]]), 3))`, which highlights the importance of a seperate held-out test set and consistent preprocessing of the data prior to evaluation. Aside from evaluating generalization and classification performances for each of these models, this study established standards for modeling pipelines of microbiome-associated machine learning models.

## Results 

__Model selection and pipeline construction__

We used a cohort of 490 patients with 261 cases of SRN. For each patient, we had `r paste(ncol(data)-1)` features (fecal bacterial abundances) and a two-class label that defines their colonic health (SRN or normal). All the cases were independently labeled through colonoscopies. We established modeling pipelines for a binary prediction task with L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and extreme gradient boosted decision tree (XGBoost) to emphasize the differences in  model accuracy, complexity, interpretibility and computational efficiency due to  model selection. 

For regularized logistic regression and SVM with linear kernel we used L2 regularization to keep all potentially important features. For comparison, we also trained an L1 regularized SVM model with linear kernel. L1-regularization on microbiome data lead to a sparser solution (i.e., force many coefficients to zero). Finally, to explore the potential for non-linear relationships among features and the outcome of interest, we trained tree based models, decision tree, random forest and XGboost, as well as an SVM with non-linear kernel.

We established a robust and reliable ML pipeline where we train and validate each of the seven models [Figure 1]. We randomly split the data into training/validation and test sets so that the training/validation set consisted of 80% of the full dataset while the test set was composed of the remaining data [Figure 1]. Since the cases are not uniformly represented in the data, the initial data-split was stratified to maintain the overall label distribution in both the training/validation and test sets. Training/validation set consisted of 393 patients (209 SRN), while the test set was composed of 97 patients (52 SRN). The training/validation data was used for training purposes and validation of hyperparameter selection (i.e. model selection), and the test set was used for evaluation purposes. Validation of hyperparameter selection was performed using repeated five-fold cross-validation on the training/validation set [Figure 1]. Similar to the initial data-split, five-fold cross-validation was also stratified to maintain the overall label distribution on the training and validation sets. We validated the cross-validation performances of each hyperparameter setting over the 100 randomizations and selected the best performing hyper-parameter setting in terms of AUROC to train the full training/validation dataset [Figures S1 and S2]. We then used the held-out test set to evaluate the prediction performance of each ML model. The data-split, hyperparameter selection, training and testing steps were repeated 100 times to get a reliable and robust reading of model prediction performance [Figure 1].

__Discriminative performance of the seven models.__

We evaluated the prediction performances of seven binary classification models when applied to held-out test data using the area under the receiver operating characteristic curve (AUROC) as the discriminative performance metric [Figure 1]. Random forest had significantly higher test AUROC values than the other models for detecting SRNs when AUROC values were compared to the other six by Wilcoxon rank sum test (p = `r paste(round(rf_logit$p.value, 4))`). The median AUROC of the random forest model was `r paste(round(median((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Random_Forest"))$test_aucs), 3))`). L2-regularized logistic regression, XGBoost, L2-regularized SVM with linear and radial basis function kernel AUROC values were not significantly different from one another. They had median AUROC values of `r paste(round(median((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="XGBoost"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="XGBoost"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))`) and  `r paste(round(median((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))`) respectively. L1 SVM with linear kernel and decision tree had significantly lower AUROC values than the other ML models with median AUROC of `r paste(round(median((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))`) and `r paste(round(median((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))`), respectively [Figure 2].  

__Generalizability of the seven models.__

We compared the median cross-validation AUROC and median testing AUROC for each model. This difference should be low to suggest the model is not overfitting despite the large number of features. The largest difference between the two was `r paste(round(abs(difference_model[difference_index[7],1]), 3))` in L1 SVM with linear kernel followed by SVM with radial basis function kernel and decision tree with a difference of `r paste(round(abs(difference_model[difference_index[6],1]), 3))` and `r paste(round(abs(difference_model[difference_index[5],1]), 3))`, respectively [Figure 2].  

We reported the testing AUROC values over 100 random splits of the full dataset [Figure 1]. The testing AUROC values varied `r paste(round(mean((all %>% group_by(model) %>% summarise(difference=max(test_aucs)-min(test_aucs)))$difference), 2))` on average, depending on the data-split. For instance, the lowest AUROC value of the random forest model was `r paste(round(min((all %>% filter(model=="Random_Forest"))$test_aucs), 2))` whereas the highest was `r paste(round(max((all %>% filter(model=="Random_Forest"))$test_aucs), 2))`. 

__Interpretation of each ML model.__

The ML models we built using L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost decrease in interpretibility as they increase in complexity. We interpreted L1 and L2 SVM with linear kernel and L2 logistic regression using the feature weights of the trained models. We ranked the absolute weights of all the OTUs for each data-split. We calculated the median ranks of these features over the 100 data-splits. In the three linear models, OTUs that had the largest median ranks and drove the detection of SRNs belong to family *Lachnospiraceae*,  and *Ruminococcaceae* (OTU01239, OTU00659, OTU00742, OTU00012, OTU00015, OTU00768, OTU00822, OTU00609) and genera *Gamella*  (OTU00426) [Figure 3]. Some of the OTUs with the highest ranks were shared among the linear models. We explained the feature importances in non-linear models using permutation importance on the held-out test data where we randomly permuted non-correlated features individually and groups of correlated features together (see methods) to calculate the effect on testing AUROC. The top 5 OTUs with the largest negative impact on testing AUROC were overl in tree-based models [Figure 4]. Specifically, permuting *Peptostreptococcus* (OTU00367) abundances randomly, dropped the predictive performances the most for all tree-based methods. Decision tree, random forest and XGBoost models' predictive performance dropped from `r paste(round(dt_median, 2))` base testing AUROC median to `r paste(round(median((dt_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, from `r paste(round(rf_median, 2))` to `r paste(round(median((rf_imp %>% filter(names=="Otu00367"))$new_auc), 2))` and from `r paste(round(xgboost_median, 2))` to `r paste(round(median((xgboost_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, respectively [Figure 4]. The negative predictive impact of *Peptostreptococcus* in the decision tree model was followed by a *Lachnospiraceae* species (OTU00058) (`r paste(round(dt_median, 2))` base testing AUROC median to `r paste(round(median((dt_imp %>% filter(names=="Otu00058"))$new_auc), 2))`) [Figure 4B]. Other OTUs had none to minimal effect on the predictive performance.  


__The computational efficiency of each ML model.__

Linear models trained faster than non-linear models. L2 logistic Rregression and L1 and L2 SVM with linear kernel had training times of `r paste(round(traintime_df[traintime_index[1],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[1],3], 2))`), `r paste(round(traintime_df[traintime_index[2],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[2],3],2))`), and `r paste(round(traintime_df[traintime_index[3],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[3],3], 2))`), respectively. Whereas, SVM with radial basis function kernel, decision tree, random forest and xgboost had training times of `r paste(round(traintime_df[traintime_index[4],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[4],3], 1))`), `r paste(round(traintime_df[traintime_index[5],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[5],3], 1))`), `r paste(round(traintime_df[traintime_index[6],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[6],3], 1))`) and `r paste(round(traintime_df[traintime_index[7],2], 1))` hours, (std ± `r paste(round(traintime_df[traintime_index[7],3], 1))`), respectively [Figure 4].

As the complexity of a ML model increase, its training times will increase (Table S1). When regularization and hyperparameter optimization (i.e. model selection) steps are added to the model pipeline, training times will increase further. The more hyperparameters a model has, the longer it will take to train. The computational burden that comes from model complexity, can be reduced by parallelizing different segments of the ML pipeline. In this study we have parallelized each datasplit which allowed 100 atasplits to be run at the same time for each model. We can also parallelize cross-validation hyerp-parameter search for each hyper-parameter setting. 

## Discussion

#### Interpretation of results

- In this study we established a robust ML pipeline to use 16S rRNA sequence counts to predict a binary health outcome. We used a held-out test set to illustrate the difference between cross-validation and testing AUROC values. When the differences between cross-validation and test performance is low, this suggets the models will perform similar with similar data. In all seven models, the differences in cross-validation and testing AUROC values did not exceed `r paste(round(abs(difference_model[difference_index[7],1]), 3))` which suggests that these models will be able to test similar new data. 

- We also reported the results of 100 random dataset splits because there are differences in AUROC values of each of the seven models between different random data-splits. Our results showed that the testing AUROC values varied `r paste(round(mean((all %>% group_by(model) %>% summarise(difference=max(test_aucs)-min(test_aucs)))$difference), 2))` on average, depending on the data-split. This result shows that you can get "lucky" in terms of prediction performance depending on the random split. Therefore, it is crucial to report all the findings over many random data-splits to get reliable performance metrics. 
 
 - Our results also showed that we can choose different models for different reasons. Getting feature importances as weights and regression coefficients is the fastest and easiest interpratation. Similarly, linear models require less computational burden, fastest to train. Thinking about the applicability, and actionability of a model in clinical setting, we might want faster results. 
 
 - There is variability if we don't do a full grid search for the hyper-parameters. We can give each model 15 minutes to train and see how well they do. So an L1 linear model will give you terible results with one and not the other C value as seen in figure S1. We have done a granular search first and then limited the possible settings to a range that made the more sense. 

 
 - Suprisingly, l2 logistic regression does really well. 
 
 - Computational efficiency and interp of logistic vs random forest
 
 - decision tree and l1 why so low?  Decision tree model has no regularization which is why the AUROC values were low. 
 
 

#### Consideration of possible weaknesses
 - What happens with imbalanced data
 The metric you use as the discriminative performance becomes important. Don't use accuracy.
 AUROC is good and random at 0.5 always.
 
 - What happens with smaller datasets
 Generalization performance depends on size. 
 The more complex the model, more data you will need.
 Use linear models if you have little data.
 
 - Does the code work on any microbiome data?
 So if yu have a new sample, do you have the same OTUs? NO

  - Why did we use only one dataset? 
  
  - Why not validate on other data instead of held-out testing set? Becuase we are comparing several methods on 1 dataset.
  
  - What will the future data look like?
 
 

#### Relationship of results to previous literature and broader implications of having answered research question

#### Prospects for future progress
  Subsample dataset and refit see how method behaves for 7 methods. (500->250->50)
  

## Materials and Methods

**Data collection and study population.**
  The data used for this analysis are stool bacterial abundances and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic health was labeled by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

**Data definitions and pre-processing.**

  The colonic health of the patient was defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. The bacterial abundances are the features used to predict colonic health of the patients. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. OTU counts were set to the size of our smallest sample and were subsampled at the same distances. They were then transformd by scaling to a [0-1] range. 
  
**Model training and evaluation.**

For L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For xgboost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners.

Hyperparameter selection is very important. We start with a granular grid search by then narrow and refine. Range of the grid depends on the problem. We can use hyper-band to help us with our hyperparameter selection. If not a full grid search is done, there could be a lot of variability in testing performance.
Models were trained using the machine learning wrapper caret package (v.6.0.81) in R (v.3.5.0). 

**Permutation importance workflow.**
We created a Spearman's rank-order correlation matrix, corrected for multiple pairwise comparisons. We then defined correlated OTUs as having perfect correlation (correlation coef=1 and p<0.01).Other OTUs were permuted individually to get permutation importance but the correlated ones are grouped together and permuted at the same time. The reported OTUs that have impact on testing AUROC are reported in Figures xxx. If we want we can decrease the correlation coefficient to consider OTUs that are correlated with ecological consequences but this was out of the scope of this study but will be followed up in further analyses. 
 
**Statistical analysis workflow.** Data summaries, statistical analysis, and data visualizations were performed using R (v.3.5.0) with the tidyverse package (v.1.2.1). We compared the AUROC values of the seven ML models by Wilcoxon rank sum tests to determine the best discriminative performance. 

**Code availability.** The code for all sequence curation and analysis steps including an Rmarkdown version of this manuscript is available at https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/.

\newpage
\includegraphics{Figure_1}
**Figure 1. Machine learning pipeline showing predictive model training and evaluation flowchart.  **  We split the data 80%/20% stratified to maintain the overall label distribution, performed five-fold cross-validation on the training data to select the best hyperparameter setting and then using these hyperparameters to train all of the training data. The model was evaluated on a held-out set of data (not used in selecting the model).
Abbreviations: AUROC, area under the receiver operating characteristic curve
\newpage
\includegraphics{Figure_2.png}
**Figure 2. Generalization and classification performance of ML models using AUROC values of all cross validation and testing performances. **   The median AUROC for diagnosing individuals with SRN using bacterial abundances was higher than chance (depicted by horizontal line at 0.50) for all the ML models. Discriminative performance of random forest model was higher than other ML models. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: SRN, screen-relevant neoplasias; AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting
\newpage
\includegraphics{Figure_3.png}
**Figure 3. Interpretation of the linear ML models.** (A) L2 logistic regression coefficients  (B) L1 SVM with linear kernel feature weights (C) L2 SVM with linear kernel feature weights. The means weights and coefficients of the most important five OTUs for each model are shown here with the standard deviation over 100 data-splits. Similar OTUs had the largest impact on the predictive performance of L2 logistic regression and L2 SVM with linear kernel. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_4.png}
**Figure 4. Explanation of the non-linear ML models.** (A) SVM with radial basis kernel (B) decision tree (C) random forest (D) XGboost feaure importances were explanied using permutation importance using held-out test set. The gray rectangle and the dashed line show the IQR range and median of the base testing AUROC without any permutation performed. For all the tree-based models, a *Peptostreptococcus* species (OTU00367) had the largest impact on predictive performance of the model. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit; RBF, radial basis kernel; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_5.png}
**Figure 5. Computational efficiency of seven ML models.** The traintimes for training and testing of each data-split showed the differences in computational efficieny of the seven models. The median traintime in hours was the highest for XGBoost and shortest for L2 logistic regression. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting.  
\newpage
\includegraphics{Figure_S1.png}
**Figure S1. Hyperparameter setting performances for linear models.** 
Abbreviations: 
\newpage
\includegraphics{Figure_S2.png}
**Figure S2. Hyperparameter setting performances for non-linear models.** 
Abbreviations: 
\newpage




## References

<div id="refs"></div>


