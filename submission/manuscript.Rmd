---
title: "**Evaluation of machine learning methods for 16S rRNA gene data**"
bibliography: references.bib
output:
  pdf_document: 
    keep_tex: true
    includes:
      in_header: header.tex
csl: mbio.csl #Get themes at https://github.com/citation-style-language/styles
fontsize: 11pt
geometry: margin=1.0in
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in important functions -----------------#
######################################################################
source("../code/learning/functions.R")

#####################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################
source("../code/learning/load_data.R")

######################################################################
#----------------- Load feature importance data -----------------#
# Median of base testing AUC for each model
# 100 datasplit permutation importances new testing AUC for top 5 features
######################################################################

source("../code/learning/load_imp_data.R")

######################################################################
#-------------------------- Load walltime data -----------------#
######################################################################
source("../code/learning/load_walltime.R")

######################################################################
#---------------------- Load AUROC performance data -----------------#
######################################################################
source("../code/learning/load_AUROC_data.R")
```
\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Nick Lesniak${^1}$, Jenna Wiens${^2}$, Mack Ruffin${^3}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109

3\. Department of Family Medicine and Community Medicine, Penn State Hershey Medical Center, Hershey, PA




\newpage
\linenumbers


## Abstract

Machine learning (ML) modeling of the human microbiome is used to identify the microbial biomarkers and aid in diagnosis of many chronic diseases such as inflammatory bowel disease, diabetes and colorectal cancer. Progress has been made towards developing ML models that predict health outcomes from bacterial abundances, but reliable ML models are scarce in part to the flawed modeling methods. Furthermore, using black box ML models has hindered the validation of microbial biomarkers. To overcome these challenges, we benchmarked seven different ML models that use fecal 16S rRNA sequences to predict the presence/absence of colorectal cancer (CRC) lesions (n=490 patients, 261 controls and 229 cases). To show the effect of model selection, we assessed the accuracy, interpretability, and computational efficiency of the following models: L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and extreme gradient boosting (XGBoost). The random forest model was best at detecting CRC lesions with an AUROC of 0.695 but it was slow to train (101.3 h) and hard to interpret. Despite its simplicity, L2-regularized logistic regression followed random forest in predictive performance with an AUROC of 0.680, and it trained the fastest (12 min). This study showed that we should choose ML models based on our expectations of performance and interpretability as well as our  computational resources. It also established standards for modeling pipelines of microbiome-associated ML models.


## Importance

\newpage

## Introduction

Advances in sequencing technology and decreasing costs of generating 16S rRNA gene sequences have allowed rapid exploration of human associated microbiome and its health implications. Currently, the human microbiome field is growing at an unprecedented rate and as a result, there is an increasing demand for methods that identify associations between members of the microbiome and human health. However, this is difficult as human associated microbial communities are remarkably complex, high-dimensional and uneven within and between individuals with the same disease. It is unlikely that a single species can explain a disease. Instead, subsets of those communities, in relation to one another and to their host, account for differences in health outcomes. Machine learning (ML) methods are effective at recognizing and highlighting patterns in complex microbial datasets. Therefore, researchers have started to explore the utility of ML models that use microbiota associated biomarkers to predict human health and to understand the microbial ecology of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases (IBD), obesity, type 2 diabetes and others [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018].  However, currently the field's use of ML lacks clarity and consistency on which methods are used and how these methods are implemented [@thaiss_persistent_2016; dadkhah_gut_2019]. More notably, flawed ML practices are prevalent such as using ML pipelines where there is no seperate held-out test dataset to evaluate model performance, reporting few or only the best outcomes of different randomizations of cross-validation and showing a disregard for large differences between cross-validation and testing performances as well as large confidence intervals of testing performances [@flemer_oral_2018; @baxter_microbiota-based_2016; @dai_multi-cohort_2018; @montassier_pretreatment_2016; @papa_non-invasive_2012; @mossotto_classification_2017; @ai_systematic_2017; @wong_quantitation_2017]. Moreover, there is a lack of discussion on why a particular ML model is utilized. Recently, there is a trend towards using more complex ML models such as random forest, extreme gradient boosting and neural networks without a discussion on if and how much model interpretibility is necessary for the study [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017; @ @geman_deep_2018]. The lack of transparency on modeling methodology and model selection negatively impact model reproducibility and reliability. We need to strive toward better machine learning practices by (1) implementing consistent and reliable machine learning pipelines and (2) selecting ML models that reflect the goal of the study as it will inform our expectations of model accuracy, complexity, interpretibility and computational efficiency.

To showcase a reliable ML pipeline and to shed light on how much ML model selection can affect modeling results, we performed an empirical analysis comparing several different ML models using the same dataset and with a robust ML pipeline. We used a previously published colorectal cancer (CRC) study [@baxter_dna_2016] which had fecal 16S rRNA gene sequences from `r paste(nrow(shared))` patients. We built ML models using fecal 16S rRNA gene sequences to predict patients with normal colons or patients with colonic tumors which are called screen relevant neoplasias (SRN). The study had `r paste(sum(sum(meta$dx=="normal")+sum(meta$Dx_Bin=="Adenoma")))` normal and `r paste(sum(sum(meta$dx=="cancer")+sum(meta$Dx_Bin=="adv Adenoma")))` SRN samples. We established modeling pipelines for L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost. Our ML pipeline utilized held-out test data to evaluate generalization performance of each ML model. The median test AUROC varied from `r paste(round(test_all[performance_index[1],3], 3))`  to `r paste(round(test_all[performance_index[7],3], 3))`. Random forest had the highest median AUROC for detecting SRN. Despite the simplicity, the L2-regularized logistic regression followed random forest in performance. In terms of computational efficiency, L2 logistic regression trained the fastest (`r paste(round(walltime_df[walltime_index[1],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[1],3], 3))`), while XGBoost took the longest (`r paste(round(walltime_df[walltime_index[7],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[7],3], 3))`). We found that median cross-validation and testing AUROC varied only `r paste(round(abs(difference[difference_index[7]]), 3))`, which highlights the importance of a seperate held-out test set and consistent preprocessing of the data prior to evaluation. Aside from evaluating generalization and classification performances for each of these models, this study established standards for modeling pipelines of microbiome-associated machine learning models.

## Results 

__Model selection and pipeline construction__

We used a cohort of 490 patients with 261 cases of SRN. For each patient, we had `r paste(ncol(data)-1)` features (fecal bacterial abundances) and a two-class label that defines their colonic health (SRN or normal). All the cases were independently labeled through colonoscopies. We established modeling pipelines for a binary prediction task with L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and extreme gradient boosted decision tree (XGBoost) to emphasize the differences in  model accuracy, complexity, interpretibility and computational efficiency due to  model selection. 

For regularized logistic regression and SVM with linear kernel we used L2 regularization to keep all potentially important features. For comparison, we also trained an L1 regularized SVM model with linear kernel. L1-regularization on microbiome data lead to a sparser solution (i.e., force many coefficients to zero), removing features that could be correlated with other important features. Finally, to explore the potential for non-linear relationships among features and the outcome of interest, we trained tree based models, decision tree, random forest and XGboost, as well as an SVM with non-linear kernel.

We established a robust and reliable ML pipeline where we train and validate each of the seven models [Figure 1]. We randomly split the data into training/validation and test sets so that the training/validation set consisted of 80% of the full dataset while the test set was composed of the remaining data [Figure 1]. Since the cases are not uniformly represented in the data, the initial data-split was stratified to maintain the overall label distribution in both the training/validation and test sets. Training/validation set consisted of 393 patients (209 SRN), while the test set was composed of 97 patients (52 SRN). The training/validation data was used for training purposes and validation of hyperparameter selection (i.e. model selection), and the test set was used for evaluation purposes. Validation of hyperparameter selection was performed using repeated five-fold cross-validation on the training/validation set [Figure 1]. Similar to the initial data-split, five-fold cross-validation was also stratified to maintain the overall label distribution on the training and validation sets. We validated the cross-validation performances of each hyperparameter setting over the 100 randomizations and selected the best performing hyper-parameter setting in terms of AUROC to train the full training/validation dataset [Figures S1 and S2]. We then used the held-out test set to evaluate the prediction performance of each ML model. The data-split, hyperparameter selection, training and testing steps were repeated 100 times to get a reliable and robust reading of model prediction performance [Figure 1].

__Model performance and generalizability.__

We evaluated the prediction performances of seven binary classification models when applied to held-out test data using the area under the receiver operating characteristic curve (AUROC) as the discriminative performance metric [Figure 1]. Random forest had significantly higher test AUROC values than the other models for detecting SRNs when AUROC values were compared to the other six by Wilcoxon rank sum test (p = `r paste(round(rf_logit$p.value, 4))`). The median AUROC of random forest was `r paste(round(median((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Random_Forest"))$test_aucs), 3))`). L2 logistic regression, XGBoost, L2 SVM with linear and radial basis function kernel AUROC values were not significantly different from one another. They had median AUROC values of `r paste(round(median((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="XGBoost"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="XGBoost"))$test_aucs), 3))`), `r paste(round(median((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))`) and  `r paste(round(median((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))`) respectively. L1 SVM with linear kernel and decision tree had significantly lower AUROC values than the other ML models with median AUROC of `r paste(round(median((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="L1_Linear_SVM"))$test_aucs), 3))`) and `r paste(round(median((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` (IQR `r paste(round(IQR((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))`), respectively [Figure 2].  

We compared the median cross-validation AUROC and median testing AUROC for each model. This difference should be low to suggest the model is not overfitting despite the large number of features. The largest difference between the two was `r paste(round(abs(difference_model[difference_index[7],1]), 3))` in L1 SVM with linear kernel followed by SVM with radial basis function kernel and decision tree with a difference of `r paste(round(abs(difference_model[difference_index[6],1]), 3))` and `r paste(round(abs(difference_model[difference_index[5],1]), 3))`, respectively [Figure 2].  

We reported the testing AUROC values over 100 random splits of the full dataset [Figure 1]. The testing AUROC values varied `r paste(round(mean((all %>% group_by(model) %>% summarise(difference=max(test_aucs)-min(test_aucs)))$difference), 2))` on average, depending on the data-split. For instance, the lowest AUROC value of the random forest model was `r paste(round(min((all %>% filter(model=="Random_Forest"))$test_aucs), 2))` whereas the highest was `r paste(round(max((all %>% filter(model=="Random_Forest"))$test_aucs), 2))`. 

__Interpretation of each ML model.__

The ML models we built using L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost decrease in interpretibility as they increase in complexity. We interpreted L1 and L2 SVM with linear kernel using the feature weights and L2 logistic regression using regression coefficients of the trained models. We examined the 5 predictive (highest weight) and protective (lowest weight) features identified by the model. We calculated the mean weights and coefficients of these features over the 100 data-splits. In the three linear models, OTUs that had the largest mean weights and drove the detection of SRNs belong to family *Lachnospiraceae*,  and *Ruminococcaceae* (OTU01239, OTU00659, OTU00742, OTU00012, OTU00050, OTU00015, OTU00768, OTU00822, OTU00609, OTU01212, OTU00629) and genera *Gamella*  (OTU00426) [Figure 3]. We explained the feature importances in non-linear models using permutation importance on the held-out test data where we permuted groups of correlated features (see methods). For the tree-based models, permuting *Peptostreptococcus* (OTU00367) abundances randomly, dropped the predictive performances the most. Decision tree, random forest and XGBoost models' predictive performance dropped from `r paste(round(dt_median, 2))` base testing AUROC median to `r paste(round(median((dt_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, from `r paste(round(rf_median, 2))` to `r paste(round(median((rf_imp %>% filter(names=="Otu00367"))$new_auc), 2))` and from `r paste(round(xgboost_median, 2))` to `r paste(round(median((xgboost_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, respectively [Figure 4]. The negative predictive impact of *Peptostreptococcus* in the decision tree model was followed by a *Lachnospiraceae* species (OTU00058) (`r paste(round(dt_median, 2))` base testing AUROC median to `r paste(round(median((dt_imp %>% filter(names=="Otu00058"))$new_auc), 2))`) [Figure 4B]. Other OTUs had none to minimal effect on the predictive performance.  


__The computational efficiency of each ML model.__

Linear models trained faster than non-linear models. L2 logistic Rregression and L1 and L2 SVM with linear kernel had training times of `r paste(round(walltime_df[walltime_index[1],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[1],3], 2))`), `r paste(round(walltime_df[walltime_index[2],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[2],3],2))`), and `r paste(round(walltime_df[walltime_index[3],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[3],3], 2))`), respectively. Whereas, SVM with radial basis function kernel, decision tree, random forest and xgboost had training times of `r paste(round(walltime_df[walltime_index[4],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[4],3], 1))`), `r paste(round(walltime_df[walltime_index[5],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[5],3], 1))`), `r paste(round(walltime_df[walltime_index[6],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[6],3], 1))`) and `r paste(round(walltime_df[walltime_index[7],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[7],3], 1))`), respectively [Figure 4].

## Discussion

#### Interpretation of results
 - In this study we established a robust ML pipeline to use 16S rRNA sequence counts to predict a binary health outcome.
 
 - We showed the importance of held-out test set and reporting the results of many iterations of cross-validation and testing. Each data-split has a weight that comes from random splitting. Seeing that testing and cross-validation median differences being low show that the models are generalizable and the pipeline works.
 
 - Our results also show that we can choose different models for different reasons. Getting feature importances as weights and regression coefficients is the fastest and easiest interpratation. Similarly, linear models require less computational burden, fastest to train. Thinking about the applicability, and actionability of a model in clinical setting, we might want faster results. 
 
 -This is a huge difference which shows that you can get "lucky" or "unlucky" depending on the random split. It is crucial to report all the findings, many randomizations of these splits to get reliable performance metrics. 

 
 - Suprisingly, l2 logistic regression does really well. 
 
 - Computational efficiency and interp of logistic vs random forest
 
 - decision tree and l1 why so low? 
 
 

#### Consideration of possible weaknesses
 - What happens with imbalanced data
 
 - What happens with smaller datasets
 
 - Does the code work on any microbiome data?

  - Why did we use only one dataset? 
  
  - Why not validate on other data instead of held-out testing set? Becuase we are comparing several methods on 1 dataset.
  
  - What will the future data look like?
 
 

#### Relationship of results to previous literature and broader implications of having answered research question

#### Prospects for future progress
  Subsample dataset and refit see how method behaves for 7 methods. (500->250->50)
  

## Materials and Methods

**Data collection and study population.**
  The data used for this analysis are stool bacterial abundances and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic health was labeled by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

**Data definitions and pre-processing.**

  The colonic health of the patient was defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. The bacterial abundances are the features used to predict colonic health of the patients. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. OTU counts were set to the size of our smallest sample and were subsampled at the same distances. They were then transformd by scaling to a [0-1] range. 
  
**Model training and evaluation.**

For L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For xgboost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners.
Models were trained using the machine learning wrapper caret package (v.6.0.81) in R (v.3.5.0). 

**Permutation importance workflow.**
 
**Statistical analysis workflow.** Data summaries, statistical analysis, and data visualizations were performed using R (v.3.5.0) with the tidyverse package (v.1.2.1). We compared the AUROC values of the seven ML models by Wilcoxon rank sum tests to determine the best discriminative performance. 

**Code availability.** The code for all sequence curation and analysis steps including an Rmarkdown version of this manuscript is available at https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/.

\newpage
\includegraphics{Figure_1}
**Figure 1. Machine learning pipeline showing predictive model training and evaluation flowchart.  **  We split the data 80%/20% stratified to maintain the overall label distribution, performed five-fold cross-validation on the training data to select the best hyperparameter setting and then using these hyperparameters to train all of the training data. The model was evaluated on a held-out set of data (not used in selecting the model).
Abbreviations: AUROC, area under the receiver operating characteristic curve
\newpage
\includegraphics{Figure_2.png}
**Figure 2. Generalization and classification performance of ML models using AUROC values of all cross validation and testing performances. **   The median AUROC for diagnosing individuals with SRN using bacterial abundances was higher than chance (depicted by horizontal line at 0.50) for all the ML models. Discriminative performance of random forest model was higher than other ML models. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: SRN, screen-relevant neoplasias; AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting
\newpage
\includegraphics{Figure_3.png}
**Figure 3. Interpretation of the linear ML models.** (A) L2 logistic regression coefficients  (B) L1 SVM with linear kernel feature weights (C) L2 SVM with linear kernel feature weights. The means weights and coefficients of the most important five OTUs for each model are shown here with the standard deviation over 100 data-splits. Similar OTUs had the largest impact on the predictive performance of L2 logistic regression and L2 SVM with linear kernel. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_4.png}
**Figure 4. Explanation of the non-linear ML models.** (A) SVM with radial basis kernel (B) decision tree (C) random forest (D) XGboost feaure importances were explanied using permutation importance using held-out test set. The gray rectangle and the dashed line show the IQR range and median of the base testing AUROC without any permutation performed. For all the tree-based models, a *Peptostreptococcus* species (OTU00367) had the largest impact on predictive performance of the model. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit; RBF, radial basis kernel; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_5.png}
**Figure 5. Computational efficiency of seven ML models.** The walltimes for training and testing of each data-split showed the differences in computational efficieny of the seven models. The median walltime in hours was the highest for XGBoost and shortest for L2 logistic regression. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting.  
\newpage
\includegraphics{Figure_S1.png}
**Figure S1. Hyperparameter setting performances for linear models.** 
Abbreviations: 
\newpage
\includegraphics{Figure_S2.png}
**Figure S2. Hyperparameter setting performances for non-linear models.** 
Abbreviations: 
\newpage

## References

<div id="refs"></div>


