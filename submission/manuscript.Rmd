---
title: "**Evaluation of binary classification pipelines and methods for 16S rRNA gene data**"
bibliography: references.bib
output:
  word_document: default
  pdf_document:
    keep_tex: true
    includes:
      in_header: header.tex
csl: mbio.csl #Get themes at https://github.com/citation-style-language/styles
fontsize: 11pt
geometry: margin=1.0in
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
library(reticulate)
use_python("/Library/Frameworks/Python.framework/Versions/3.6/bin/python3", required = TRUE)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################

# Read in metadata and select only sample Id and diagnosis columns
meta <- read.delim('../data/metadata.tsv', header=T, sep='\t') %>%
  select(sample, dx, Dx_Bin, fit_result)


# Read in OTU table and remove label and numOtus columns
shared <- read.delim('../data/baxter.0.03.subsample.shared', header=T, sep='\t') %>%
   select(-label, -numOtus)

```


```{python, cache=FALSE, message=FALSE, warning=FALSE, engine.path = '/Library/Frameworks/Python.framework/Versions/3.6/bin/python3'}
import platform
python_version = platform.python_version()
import sklearn
sklearn_module = sklearn.__version__
```

\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Jenna Wiens${^2}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109


\newpage
\linenumbers


## Abstract


\newpage

## Introduction

Advances in sequencing technology and decreasing costs of generating 16S rRNA gene sequences allowed rapid exploration and taxonomic characterization of the human associated microbiome. Currently, the microbiome field is growing at an unprecedented rate and as a result, there is an ever-increasing demand for reproducible methods for identifying associations between members of the microbiome and human health. Human associated microbial communities are remarkably uneven and it is unlikely that a single species can explain a disease state comprehensively. It is more likely that subsets of those communities, in relation to one another and to their host, can reliably account for the unevenness and be used as biomarkers of a specific disease state. Thus, researchers have started to explore the utility of machine learning (ML) techniques to identify the biomarkers associated with healthy and diseased individuals. However, currently the field's use of ML lacks clarity and consistency on which methods are used, how these methods are implemented and if these methods are reproducible. Moreover, there is a lack of consideration on why a particular method is utilized. Often, there is a trade-off between the interpretibility and accuracy of ML methods. As researchers, we have to find the right balance to accomplish complex knowledge tasks such as identifying a disease state based on microbiome-associated biomarkers while also understanding how these tasks are accomplished.

One application of machine learning to microbiome data has been to classify patients as having colorectal tumors based on microbiota-associated biomarkers. Colorectal cancer (CRC) is one of the leading causes of death in the US, therefore developing a microbiome assay as a non-invasive screening tool would be a useful innovation. However, the efforts to combine ML methods with relative abundances of bacterial poplations in stool to predict CRC tumors have been afflicted by the aforementioned flawed ML practices. As a result, the predictive performance of these models varies greatly with areas under the receiver operating characteristic curve (AUROC) of 0.7-0.9. 







Patients with colorectal cancer have different stool community of microbes than adults with normal colons (10–15). This difference however cannot be explained by a single species in the gut but by many of them in relation to one another. Therefore, machine learning emerged as a tool to identify the microbial biomarkers of CRC. Previous studies that used cohorts of 90 (30 SRN, 30 adenomas, 30 normal colons) (7) and 490 (229 SRN, 89 adenoma, 172 normal colons) patients (5) suggest that machine learning models can predict SRN. However, the prediction performance of previous models varies greatly. There is a need for implementing consistent and transparent machine learning practices to generate reproducible and replicable CRC biomarker models. This study aims to determine whether a reproducible and replicable classification model that incorporate the relative abundance of fecal bacterial populations can be developed to discriminate between individuals with and without, SRN.

Here, colonic disease status is defined as Normal or Screen Relevant Neoplasias (SRN). Stool bacterial population abundances and stool hemoglobin levels of `r paste(sum(sum(meta$dx=="normal")+sum(meta$Dx_Bin=="Adenoma")))` Normal and `r paste(sum(sum(meta$dx=="cancer")+sum(meta$Dx_Bin=="adv Adenoma")))` SRN samples were used to learn binary classifiers and evaluate their performances. Modeling pipelines were established for L2-regularized Logistic Regression, L1 and L2 Linear Support Vector Machines (SVM), Radial Basis Function SVM, Decision Tree, Random Forest and XGBoost binary classification models. The mean AUCs of these models were `r paste(round(test_meanAUC,2))` ± `r paste(round(test_sdAUC,2))`,  `r paste(round(l1svm_summary[2,2],2))` ± `r paste(round(l1svm_summary[2,3],2))`,  `r paste(round(l2svm_summary[2,2],2))` ± `r paste(round(l2svm_summary[2,3],2))`, `r paste(round(svmRBF_summary[2,2],2))` ± `r paste(round(svmRBF_summary[2,3],2))`, `r paste(round(dt_summary[2,2],2))` ± `r paste(round(dt_summary[2,3],2))`, `r paste(round(rf_summary[2,2],2))` ± `r paste(round(rf_summary[2,3],2))`, and `r paste(round(xgboost_summary[2,2],2))` ± `r paste(round(xgboost_summary[2,3],2))`, respectively.

## Results and Discussion

## Conclusions



## Materials and Methods

#### Data collection
  The data used for this analysis are stool bacterial abundances, stool hemoglobin levels and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic disease status was defined by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for Fecal Immunological Tests (FIT) which measure human hemoglobin concentrations and for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

#### Data definitions and pre-processing

  The colonic disease status is re-defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. Colonic disease status is the label predicted with each classifier.   The bacterial abundances and FIT results are the features used to predict colonic disease status. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. There are 6920 OTUs for each sample. FIT levels are continuous data present for each sample. Because the data are in different scales, Python programming language v`r paste(py$python_version)`, module scikit-learn v`r paste(py$sklearn_module)` is used to transform features by scaling each feature to a [0-1] range (Table 1) [@scikit-learn].
  
#### Learning the Classifier

  To train and validate our model, labeled data is randomly split 80/20 into a training set and testing
set. Then, seven binary class classifiers, L2 logistic regression, L1 and L2 linear suppor vector machines (SVM), radial basis function SVM, decision tree, random forest and XGBoost, are learned. The training set is used for training purposes and validation of hyperparameter selection, and the test set is used for evaluation purposes. Hyperparameters are selected using 5-fold cross-validation with 100-repeats on the training set. Since the colonic disease status are not uniformly represented in the data, 5-fold splits are stratified to maintain the overall label distribution on the training set. Python programming language v`r paste(py$python_version)`, module scikit-learn v`r paste(py$sklearn_module)` functions are used to learn the seven classifiers (Table 1). 

#### Classifier Performance

  The classification performance of learned classifier is evaluated on the labeled held-out testing set. The optimal  classifier with optimal hyperparameters selected in the cross-validation step is used to produce a prediction for the testing set. The performance of this prediction is measured in terms of the sensitivity and specificity, in addition to Area Under the Curve (AUC) metrics. This process of splitting the data, learning a classifier with cross-validation, and testing the classifier is repeated on 100 different splits. In the end cross-validation AUC and testing AUC averaged over the 100 different training/test splits are reported. Hyperparameter budget and performance for each split is also reported. 


\newpage



**Figure 1. Generalization and classification performance of modeling methods ** AUC values of all cross validation and testing performances. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 

\newpage

## References

<div id="refs"></div>


