---
title: "**Evaluation of machine learning methods for 16S rRNA gene data**"
bibliography: references.bib
output:
  pdf_document: 
    keep_tex: true
    includes:
      in_header: header.tex
csl: mbio.csl #Get themes at https://github.com/citation-style-language/styles
fontsize: 11pt
geometry: margin=1.0in
---


```{r knitr_settings, eval=TRUE, echo=FALSE, cache=FALSE,  warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in necessary libraries -------------------#
######################################################################
deps = c("reshape2","knitr","rmarkdown","vegan","gtools", "tidyverse");
for (dep in deps){
  if (dep %in% installed.packages()[,"Package"] == FALSE){
    install.packages(as.character(dep), quiet=TRUE);
  }
  library(dep, verbose=FALSE, character.only=TRUE)
}
######################################################################
#-------------- Define the chunk options ----------------#
######################################################################
opts_chunk$set("tidy" = TRUE)
opts_chunk$set("echo" = FALSE)
opts_chunk$set("eval" = TRUE)
opts_chunk$set("warning" = FALSE)
opts_chunk$set("cache" = FALSE)

inline_hook <- function(x){
	print(x)

	if(is.list(x)){
		x <- unlist(x)
	}

	if(is.numeric(x)){
		if(abs(x - round(x)) < .Machine$double.eps^0.5){
			paste(format(x,big.mark=',', digits=0, scientific=FALSE))
		} else {
			paste(format(x,big.mark=',', digits=1, nsmall=1, scientific=FALSE))
		}
	} else {
    	paste(x)      
	}
}
knitr::knit_hooks$set(inline=inline_hook)
```

```{r LoadData, eval=TRUE, echo=FALSE, cache=FALSE, warning=FALSE, message=FALSE}
######################################################################
#----------------- Read in important functions -----------------#
######################################################################
source("../code/learning/functions.R")

#####################################################################

######################################################################
#----------------- Load OTU table and MetaData -----------------#
######################################################################
source("../code/learning/load_data.R")

######################################################################
#----------------- Load feature importance data -----------------#
# Median of base testing AUC for each model
# 100 datasplit permutation importances new testing AUC for top 5 features
######################################################################

source("../code/learning/load_imp_data.R")

######################################################################
#-------------------------- Load walltime data -----------------#
######################################################################
source("../code/learning/load_walltime.R")

######################################################################
#---------------------- Load AUROC performance data -----------------#
######################################################################
source("../code/learning/load_AUROC_data.R")
```
\vspace{35mm}

Running title: Machine learning methods in microbiome studies

\vspace{35mm}


Begüm D. Topçuoğlu${^1}$, Mack Ruffin${^2}$, Jenna Wiens${^3}$, Patrick D. Schloss^1$\dagger$^

\vspace{40mm}

$\dagger$ To whom correspondence should be addressed: pschloss@umich.edu

1\. Department of Microbiology and Immunology, University of Michigan, Ann Arbor, MI 48109

2\. Department of Family Medicine and Community Medicine, Penn State Hershey Medical Center, Hershey, PA

3\. Department of Computer Science and Engineering, University or Michigan, Ann Arbor, MI 49109


\newpage
\linenumbers


## Abstract


\newpage

## Introduction

Advances in sequencing technology and decreasing costs of generating 16S rRNA gene sequences have allowed rapid exploration of human associated microbiome and its health implications. Currently, the human microbiome field is growing at an unprecedented rate and as a result, there is an increasing demand for methods that identify associations between members of the microbiome and human health. However, this is difficult as human associated microbial communities are remarkably complex and uneven. It is unlikely that a single species can explain a disease. Instead, subsets of those communities, in relation to one another and to their host, account for the differences in health outcomes. Machine learning (ML) methods are effective at recognizing and highlighting patterns in complex microbial datasets. Therefore, researchers have started to explore the utility of ML models that use microbiota associated biomarkers to predict human health and to understand the microbial ecology of diseases such as liver cirrhosis, colorectal cancer, inflammatory bowel diseases (IBD), obesity, and type 2 diabetes [@zeller_potential_2014; @zackular_human_2014; @baxter_dna_2016; @baxter_microbiota-based_2016; @hale_shifts_2017; @pasolli_machine_2016; @sze_looking_2016; @walters_meta-analyses_2014; @vazquez-baeza_guiding_2018; @qin_alterations_2014; @geman_deep_2018].  However, currently the field's use of ML lacks clarity and consistency on which methods are used and how these methods are implemented. Most notably, there are misleading practices of using "leaky" ML pipelines where models are tested on training data or reporting the best outcome of different iterations of cross-validation. Moreover, there is a lack of discussion on why a particular ML model is utilized. Recently, there is a trend towards using more complex ML models such as random forest, extreme gradient boosting and neural networks without a discussion on if and how much model interpretibility is necessary for the study [@galkin_human_2018; @reiman_using_2017; @fioravanti_phylogenetic_2017; @ @geman_deep_2018]. The lack of transparency on modeling methodology and model selection negatively impact model reproducibility and reliability. We need to strive toward better machine learning practices by (1) implementing consistent and reliable machine learning pipelines; (2) selecting ML models that reflect the goal of the study as it will inform our expectations of model accuracy, complexity, interpretibility and computational efficiency.

To showcase reliable ML pipeline and to shed light on how much ML model selection can affect modeling results, we performed an empirical analysis comparing several different ML models using the same dataset and with a robust ML pipeline. We used a previously published colorectal cancer (CRC) study [@baxter_dna_2016] which had fecal 16S rRNA gene sequences from `r paste(nrow(shared))` patients. We built ML models using fecal 16S rRNA gene sequences to predict patients with normal colons or patients with colonic tumors which are called screen relevant neoplasias (SRN). The study had `r paste(sum(sum(meta$dx=="normal")+sum(meta$Dx_Bin=="Adenoma")))` normal and `r paste(sum(sum(meta$dx=="cancer")+sum(meta$Dx_Bin=="adv Adenoma")))` SRN samples. We established modeling pipelines for L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost. Our ML pipeline utilized held-out test data to evaluate generalization and prediction performance of each ML model. The mean test AUROC varied from `r paste(round(test_all[performance_index[1],3], 3))` (std ± `r paste(round(test_all[performance_index[1],4], 3))`) to `r paste(round(test_all[performance_index[7],3], 3))` (std ± `r paste(round(test_all[performance_index[7],4], 3))`). Random forest had the highest mean AUROC for detecting SRN. Despite the simplicity, the L2-regularized logistic regression followed random forest in performance. In terms of computational efficiency, L2 logistic regression trained the fastest (`r paste(round(walltime_df[walltime_index[1],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[1],3], 3))`), while XGBoost took the longest (`r paste(round(walltime_df[walltime_index[7],2], 3))` hours, std ± `r paste(round(walltime_df[walltime_index[7],3], 3))`). We found that mean cross-validation and testing AUROC varied only `r paste(round(difference[difference_index[7]], 3))`, which highlights the importance of a seperate held-out test set and consistent preprocessing of the data prior to evaluation. Aside from evaluating generalization and classification performances for each of these models, this study established standards for modeling pipelines of microbiome-associated machine learning models.

## Results 

__Model selection and pipeline construction__

We used a cohort of 490 patients with 261 cases of SRN. For each patient, we had `r paste(ncol(data)-1)` features (fecal bacterial abundances) and a two-class label that defines their colonic health (SRN or normal). All the cases were independently labeled through colonoscopies. We established modeling pipelines for a binary prediction task with L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost to emphasize the differences in  model accuracy, complexity, interpretibility and computational efficiency due to  model selection. We randomly split the data into training/validation and test sets so that the training/validation set consisted of 80% of the full dataset while the test set was composed of the remaining data [Figure 1]. Since the cases are not uniformly represented in the data, the initial data-split was stratified to maintain the overall label distribution on both the training/validation and test sets. Training/validation set consisted of 393 patients (209 SRN), while the test set was composed of 97 patients (52 SRN). The training/validation data was used for training purposes and validation of hyperparameter selection, and the test set was used for evaluation purposes. Validation of hyperparameter selection was performed using 100 randomizations of five-fold cross-validation on the training/validation set [Figure 1]. Similar to the initial data-split, five-fold cross-validation was also stratified to maintain the overall label distribution on the training and validation sets. We validated the cross-validation performances of each hyperparameter setting over the 100 randomizations and selected the best performing hyper-parameter setting to train the full training/validation dataset [Figures S1 and S2]. We then used the held-out test set to evaluate the prediction performance of each ML model. The data-split, hyperparameter selection, training and testing steps were repeated 100 times to get a reliable and robust reading of model prediction performance [Figure 1].

__The prediction and generalization performance of seven ML models during cross-validation and when applied to the held-out test data.__

We evaluated the prediction performance of seven binary classification models when applied to held-out test data over 100 data-splits using the area under the receiver operating characteristic curve (AUROC) as the discriminative performance metric. Random forest had the highest mean AUROC for detecting SRN, `r paste(round(mean((all %>% filter(model=="Random_Forest"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="Random_Forest"))$test_aucs), 3))`), followed by L2 logistic regression and XGBoost with mean AUROC values of `r paste(round(mean((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="L2_Logistic_Regression"))$test_aucs), 3))`) and `r paste(round(mean((all %>% filter(model=="XGBoost"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="XGBoost"))$test_aucs), 3))`) respectively [Figure 2]. Random forest had significanly higher predictive performances than L2 SVM with linear and radial basis function kernels and decision tree which had mean AUROC values of `r paste(round(mean((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="L2_Linear_SVM"))$test_aucs), 3))`),  `r paste(round(mean((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="RBF_SVM"))$test_aucs), 3))`), and `r paste(round(mean((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))` (std ± `r paste(round(sd((all %>% filter(model=="Decision_Tree"))$test_aucs), 3))`), respectively [Figure 2]. We also evaluated the generalization performance of each ML model by comparing their mean cross-validation AUROC and mean testing AUROC. We found that mean cross-validation and testing AUROC difference for each model was below 0.01. 

__The complexity and interpretibility of each ML model.__

The ML models we built using L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels, a decision tree, random forest and XGBoost increase in complexity as they decrase in interpretibility. We interpreted L1 and L2 SVM with linear kernel using the feature weights and L2 logistic regression using regression coefficients of the trained models. We calculated the mean weights and coefficients of all the features over the 100 data-splits. In the three linear models, OTUs that had the largest mean weights and drove the detection of SRNs belong to family *Lachnospiraceae*,  and *Ruminococcaceae* (OTU01239, OTU00659, OTU00742, OTU00012, OTU00050, OTU00015, OTU00768, OTU00822, OTU00609, OTU01212, OTU00629) and genera *Gamella*  (OTU00426) [Figure 3]. We explained the feature importances in non-linear models using permutation importance on the held-out test data. For the tree-based models, permuting *Peptostreptococcus* (OTU00367) abundances randomly, dropped the predictive performances the most. Decision tree, random forest and XGBoost models' predictive performance dropped from `r paste(round(dt_median, 2))` base testing AUROC median to `r paste(round(median((dt_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, from `r paste(round(rf_median, 2))` to `r paste(round(median((rf_imp %>% filter(names=="Otu00367"))$new_auc), 2))` and from `r paste(round(xgboost_median, 2))` to `r paste(round(median((xgboost_imp %>% filter(names=="Otu00367"))$new_auc), 2))`, respectively [Figure 4]. The negative predictive impact of *Peptostreptococcus* in the decision tree model was followed by a *Lachnospiraceae* species (OTU00058) (`r paste(round(dt_median, 2))` base testing AUROC median to `r paste(round(median((dt_imp %>% filter(names=="Otu00058"))$new_auc), 2))`) [Figure 4B]. Other OTUs had none to minimal effect on the predictive performance.  


__The computational efficiency of each ML model.__

Linear models trained faster than non-linear models. L2 logistic Rregression and L1 and L2 SVM with linear kernel had training times of `r paste(round(walltime_df[walltime_index[1],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[1],3], 2))`), `r paste(round(walltime_df[walltime_index[2],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[2],3],2))`), and `r paste(round(walltime_df[walltime_index[3],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[3],3], 2))`), respectively. Whereas, SVM with radial basis function kernel, decision tree, random forest and xgboost had training times of `r paste(round(walltime_df[walltime_index[4],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[4],3], 1))`), `r paste(round(walltime_df[walltime_index[5],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[5],3], 1))`), `r paste(round(walltime_df[walltime_index[6],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[6],3], 1))`) and `r paste(round(walltime_df[walltime_index[7],2], 1))` hours, (std ± `r paste(round(walltime_df[walltime_index[7],3], 1))`), respectively [Figure 4].

## Discussion



## Materials and Methods

#### Data collection and study population
  The data used for this analysis are stool bacterial abundances and clinical information of the patients recruited by Great Lakes-New England Early Detection Research Network study. These data were obtained from Sze et al [@sze_leveraging_2018]. 
  The stool samples were provided by recruited adult participants who were undergoing scheduled screening or surveillance colonoscopy. Colonoscopies were performed and fecal samples were collected from participants in four locations: Toronto (ON, Canada), Boston (MA, USA), Houston (TX, USA), and Ann Arbor (MI, USA). Patients' colonic health was labeled by colonoscopy with adequate preparation and tissue histopathology of all resected lesions. Patients with an adenoma greater than 1 cm, more than three adenomas of any size, or an adenoma with villous histology were classified as advanced adenoma. Study had `r paste(sum(meta$dx=="normal"))` patients with normal colonoscopies, `r paste(sum(meta$dx=="adenoma"))` with adenomas and `r paste(sum(meta$dx=="cancer"))` with carcinomas. Of the `r paste(sum(meta$dx=="adenoma"))` adenomas, `r paste(sum(meta$Dx_Bin=="adv Adenoma"))` were identified as advanced adenomas. Stool provided by the patients was used for 16S rRNA gene sequencing to measure bacterial population abundances. The bacterial abundance data was generated by Sze et al, by processing 16S rRNA sequences in Mothur (v1.39.3) using the default quality filtering methods, identifying and removing chimeric sequences using VSEARCH and assigning to OTUs at 97% similarity using the OptiClust algorithm [@schloss_introducing_2009; @westcott_opticlust_2017; @rognes_vsearch_2016].

#### Data definitions and pre-processing

  The colonic health of the patient was defined as two encompassing classes; Normal or Screen Relevant Neoplasias (SRNs). Normal class includes patients with non-advanced adenomas or normal colons whereas SRN class includes patients with advanced adenomas or carcinomas. The bacterial abundances are the features used to predict colonic health of the patients. Bacterial abundances are discrete data in the form of Operational Taxonomic Unit (OTU) counts. OTU counts were set to the size of our smallest sample and were subsampled at the same distances. They were then transformd by scaling to a [0-1] range. 
  
#### Model training and evaluation

For L2-regularized logistic regression, L1 and L2 support vector machines (SVM) with linear and radial basis function kernels we tuned the __cost__ hyperparameter which determines the regularization strength where smaller values specify stronger regularization. For SVM with radial basis function kernel we also tuned __sigma__ hyperparameter which determines the reach of a single training instance where for a high value of sigma, the SVM decision boundary will be dependent on the points that are closest to the decision boundary. For the decision tree model, we tuned the __depth of the tree__ where deeper the tree, the more splits it has. For random forest, we tuned the __number of features__ to consider when looking for the best tree split. For xgboost, we tuned for __learning rate__ and the __fraction of samples__ to be used for fitting the individual base learners.
Models were trained using the machine learning wrapper caret package (v.6.0.81) in R (v.3.5.0). 
 
**Statistical analysis workflow.** Data summaries, statistical analysis, and data visualizations were performed using R (v.3.5.0) with the tidyverse package (v.1.2.1). We compared the AUROC values of the seven ML models by Wilcoxon rank sum tests to determine the best discriminative performance. 

**Code availability.** The code for all sequence curation and analysis steps including an Rmarkdown version of this manuscript is available at https://github.com/SchlossLab/Topcuoglu_ML_XXXX_2019/.

\newpage
\includegraphics{Figure_1}
**Figure 1. Machine learning pipeline showing predictive model training and evaluation flowchart.  **  We split the data 80%/20% stratified to maintain the overall label distribution, performed five-fold cross-validation on the training data to select the best hyperparameter setting and then using these hyperparameters to train all of the training data. The model was evaluated on a held-out set of data (not used in selecting the model).
Abbreviations: AUROC, area under the receiver operating characteristic curve
\newpage
\includegraphics{Figure_2.png}
**Figure 2. Generalization and classification performance of ML models using AUROC values of all cross validation and testing performances. **   The median AUROC for diagnosing individuals with SRN using bacterial abundances was higher than chance (depicted by horizontal line at 0.50) for all the ML models. Discriminative performance of random forest model was higher than other ML models. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: SRN, screen-relevant neoplasias; AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting
\newpage
\includegraphics{Figure_3.png}
**Figure 3. Interpretation of the linear ML models.** (A) L2 logistic regression coefficients  (B) L1 SVM with linear kernel feature weights (C) L2 SVM with linear kernel feature weights. The means weights and coefficients of the most important five OTUs for each model are shown here with the standard deviation over 100 data-splits. Similar OTUs had the largest impact on the predictive performance of L2 logistic regression and L2 SVM with linear kernel. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_4.png}
**Figure 4. Explanation of the non-linear ML models.** (A) SVM with radial basis kernel (B) decision tree (C) random forest (D) XGboost feaure importances were explanied using permutation importance using held-out test set. For all the tree-based models, a *Peptostreptococcus* species (OTU00367) had the largest impact on predictive performance of the model. 
Abbreviations: SVM, support vector machine; OTU, Operational Taxonomic Unit; RBF, radial basis kernel; OTU, Operational Taxonomic Unit.
\newpage
\includegraphics{Figure_5.png}
**Figure 5. Computational efficiency of seven ML models.** The walltimes for training and testing of each data-split showed the differences in computational efficieny of the seven models. The median walltime in hours was the highest for XGBoost and shortest for L2 logistic regression. The boxplot shows quartiles at the box ends and the statistical median as the horizontal line in the box. The whiskers show the farthest points that are not outliers. Outliers are data points that are not within 3/2 times the interquartile ranges. 
Abbreviations: AUROC, area under the receiver operating characteristic curve; SVM, support vector machine; XGBoost, extreme gradient boosting.  
\newpage


## References

<div id="refs"></div>


